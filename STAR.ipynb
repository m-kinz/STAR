{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAR - Statistical Toolkit for Analysis of Radiotherapy Treatments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an iPython notebook. It consists of multiple cells, which can either be markdown/text or executable code. You will have to run these code cells in the correct order, for example, you will need to run the cells containing the functions, before you can use them. If you run a cell, the output, if there is one, will be displayed below it.\n",
    "\n",
    "This notebook is split up into three main sections, which have to be run in order:\n",
    "1. **Support Files and SQL:** Define the locations of the additional files needed to run this analysis and, if needed, set up the SQL connection.\n",
    "2. **Functions:** Defines all functions, has to be run before the analysis can be performed.\n",
    "3. **Function Calls:** The function calls needed to run the complete analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages, Support Files and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can run this script for the first time, you have to set up a Python environment with all needed packages. Best practice is creating a fresh virtual environment for this. Then run the following command, which will install all the packages from the \"requirements.txt\" file in the exact version which has been tested by us.\n",
    "\n",
    "*pip install -r requirements.txt*\n",
    "\n",
    "Alternatively, you can try to install the latest versions using \n",
    "\n",
    "*pip install plotly kaleido==0.1.0post1 pydicom numpy pandas scipy nbformat openpyxl scikit-learn python-gdcm pyodbc statsmodels trimesh scikit-image*\n",
    "\n",
    "Afterwards, restart the kernel and import the packages below. The import is always the first thing to run before the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linprog\n",
    "from scipy.ndimage import binary_dilation, binary_erosion, binary_fill_holes, zoom, distance_transform_edt\n",
    "from scipy.interpolate import interpn\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "import logging\n",
    "import shutil\n",
    "import gzip\n",
    "import pyodbc\n",
    "import json\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables/Supporting Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three sheets need to be supplied by you for the analysis to run:\n",
    "- **\"DICOMLocations\"** contains the locations of the DICOM files for which the analysis should be performed. For our SQL database, we need the identifiers 'PMRN', 'PlanSetupSer','StructureSet_UID','RTPlan_UID' to find the correct locations. This will likely be different for your center. In the end, you will need to have a \"Filename_Updated.pickle\" with the columns 'PMRN', 'PlanSetupSer', 'StructureSet_UID', 'RTPlan_UID', 'Dose_Path', 'StructureSet_Path', 'Plan_Path', which is generated from our SQL database using the \"updateLocs\" function.\n",
    "- **\"PTVInfoSheet\"** is an Excel sheet which links the Plan ID and PTV Structure name to other information like site. In our case, it contains the following columns, but in general it could be any information. Important is, that the identifiers, PlanSetupSer, match in all files. The PTV names will also be used to only analyze these PTVs and not any additional ones in the plan, if wanted. It can be way longer than the DICOMLocations list.\n",
    "PlanSetupSer\tPMRN\tPatient\tCourse\tPrescriptionSer\tPrescription\tRx site\tRx technique\tPlan\tNum Fractions\tDose/Frc (cGy)\tDVH site\tDVH modality\tContent Date\tPTV name\tPTV volume (cc)\tMin dose (cGy)\tMax dose (cGy)\tMean dose (cGy)\n",
    "- **\"CGTVInfoSheet\"** is similar to above for CTV and GTV. Contains Columns:\n",
    "#PlanSetupSer\tPMRN\tPatient\tCourse\tPrescriptionSer\tPrescription\tRx site\tRx technique\tPlan\tNum Fractions\tDose/Frc (cGy)\tDVH site\tDVH modality\tContent Date\tTarget name\tTarget volume (cc)\tMin dose (cGy)\tMax dose (cGy)\tMean dose (cGy)\t\n",
    "\n",
    "The Timmerman files are provided by us, but might need to be adapted to fit your OAR naming conventions:\n",
    "- **\"TimmermanSheet\"** is a CSV sheet containing simplified constraints from the Timmerman tables\n",
    "- **\"TimmermanDict\"** is a dictionary supplying search terms to match OAR structure names to constraints\n",
    "\n",
    "The program generates a coding dictionary when it anonymizes plan identifiers on the first run. If you already have one generated, you need to load it by specifying the location in the  **\"CodingDict\"** variable.\n",
    "\n",
    "Last but not least, **\"SaveFolder\"** defines the directory, where all results, namely pickles and plots, will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICOMLocations=\"//Cifs2/imrt_qa$/EIT/Marvin/STAR/DicomStructures_Joe_10kto100.xlsx\"\n",
    "PTVInfoSheet=\"//Cifs2/imrt_qa$/EIT/Marvin/STAR/PTV_stats_Verifier_2.xlsx\"\n",
    "CGTVInfoSheet=\"//Cifs2/imrt_qa$/EIT/Marvin/STAR/CTVGTV_stats_Verifier.xlsx\"\n",
    "TimmermanSheet=\"//Cifs2/imrt_qa$/EIT/Marvin/STAR/Timmerman Tables/Timmerman.csv\"\n",
    "TimmermanDict=\"//Cifs2/imrt_qa$/EIT/Marvin/STAR/Timmerman Tables/TimmermanSearchTerms.json\"\n",
    "with open(TimmermanDict, 'r') as fp:\n",
    "    TimmermanDict = json.load(fp)\n",
    "TimmermanDict = {k: v for k, v in TimmermanDict.items()}\n",
    "# CodingDict='//Cifs2/imrt_qa$/EIT/Marvin/STAR/AnonymizationDictPlan,Struc.json'\n",
    "SaveFolder='//Cifshd/homedir$/Evaluation/FinalTest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our center has a DICOM storage and a SQL database, which keeps track of the locations of the DICOM files in that storage.\n",
    "Thus, this connection is only useful, if your center has a SQL database with DICOM locations. You will likely need to adapt the \"*updateLocs*\" function below to perform correct queries for your database. If there is no SQL database that you can use, you will need generate an Excel sheet with locations of the DICOM files that you want to analyze yourself. It should contain the following columns:\n",
    "\n",
    "'PMRN', 'PlanSetupSer', 'StructureSet_UID', 'RTPlan_UID', 'Dose_Path', 'StructureSet_Path', 'Plan_Path'\n",
    "\n",
    "You will have to install Microsoft SQL driver first for this connection to work reliably! \n",
    "https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server\n",
    "\n",
    "SQLConnectString.txt should look like this, with ..... filled in with your data:\n",
    "\n",
    "Driver={SQL Server};  \n",
    "Server=.....;  \n",
    "Database=.....;  \n",
    "uid=.....;pwd=.....\n",
    "\n",
    "You need to establish this connection once everytime you start the kernel, and afterwards the functions will make use of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('//Cifs2/imrt_qa$/EIT/Marvin/XLSX Files/SQLConnectString.txt','r') as fp:\n",
    "    cnxn = pyodbc.connect(fp.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions (Run this section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines all the functions used to run the analysis. You can just run it completely and move on to the function calls, or, if you need to change something, dive in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateLocs(locsheet=DICOMLocations,debug=False):\n",
    "    \"\"\"\n",
    "    Connect to SQL database and query dose, structure and plan DICOM file locations using UIDs. Connection cnxn has to be established beforehand, otherwise it tries to load old locations.\n",
    "    This obviously needs to be adapted to your center. \n",
    "\n",
    "    Output location pickle contains colums:\n",
    "    'PMRN', 'PlanSetupSer', 'StructureSet_UID', 'RTPlan_UID', 'Dose_Path', 'StructureSet_Path', 'Plan_Path', 'ExportedDateTime'\n",
    "    \"\"\"\n",
    "    if debug or not 'cnxn' in globals():\n",
    "        print('Read stored locations.')\n",
    "        locs=pd.read_pickle(f\"{locsheet}_Updated.pickle\")\n",
    "    else:\n",
    "        locs=pd.read_excel(locsheet)[['PMRN', 'PlanSetupSer','StructureSet_UID','RTPlan_UID']]\n",
    "        dfSQL = pd.read_sql_query(\"SELECT CONCAT(ShareLocation, FilePath) AS Dose_Path, rtdosefile.RefRTPlanUID AS RTPlan_UID FROM dbo.rtdosefile JOIN dbo.pathroot ON rtdosefile.PathRootIDX = pathroot.PathRootIDX WHERE DoseSumType = 'PLAN'\", cnxn)\n",
    "        dfSQL2 = pd.read_sql_query(\"SELECT CONCAT(ShareLocation, FilePath) AS StructureSet_Path, structuresetfile.InstanceUID AS StructureSet_UID FROM dbo.structuresetfile JOIN dbo.pathroot ON structuresetfile.PathRootIDX = pathroot.PathRootIDX\", cnxn)\n",
    "        dfSQL3 = pd.read_sql_query(\"SELECT CONCAT(ShareLocation, FilePath) AS Plan_Path, rtplanfile.InstanceUID AS RTPlan_UID, rtplanfile.ExportedDateTime as ExportedDateTime FROM dbo.rtplanfile JOIN dbo.pathroot ON rtplanfile.PathRootIDX = pathroot.PathRootIDX\", cnxn)\n",
    "        locs=pd.merge(locs,dfSQL,on='RTPlan_UID',how='left')\n",
    "        locs=pd.merge(locs,dfSQL2,on='StructureSet_UID',how='left')\n",
    "        locs=pd.merge(locs,dfSQL3,on='RTPlan_UID',how='left')\n",
    "        pd.to_pickle(locs,f\"{locsheet}_Updated.pickle\")\n",
    "    return locs\n",
    "\n",
    "def get_points(structs,calccntrs = False, CT = False):\n",
    "    \"\"\"\n",
    "    Helper function to convert DICOM structures to numpy arrays. \n",
    "    Can also return the matching CT ids as a dictionary and the centers of the contours, if needed.\n",
    "    \"\"\"\n",
    "    #Initialize return arrays, if needed.\n",
    "    pts = np.array([])\n",
    "    if calccntrs: \n",
    "            cntrs = np.array([])\n",
    "            cntr = np.array([])\n",
    "    if CT:\n",
    "        CTids = dict()\n",
    "    #Go through all contour layers\n",
    "    for struc in structs:\n",
    "        if calccntrs:\n",
    "            cntrsstruct = np.array([])\n",
    "        #and concatenate them into one array\n",
    "        for cc in struc.ContourSequence:\n",
    "            pts= np.concatenate((pts,cc.ContourData))\n",
    "            if calccntrs:\n",
    "                #print(cc.ContourImageSequence[0].ReferencedSOPInstanceUID)\n",
    "                cntrsstruct = np.concatenate((cntrsstruct,np.mean(np.array(cc.ContourData).reshape(-1,3)[:,:2],axis=0),[pts[-1]]))\n",
    "            if CT:\n",
    "                CTids[pts[-1]] = cc.ContourImageSequence[0].ReferencedSOPInstanceUID #RS21.ROIContourSequence[0].ContourSequence[0].ContourImageSequence[0].ReferencedSOPInstanceUID\n",
    "        if calccntrs: \n",
    "            cntr = np.concatenate((cntr,np.mean(cntrsstruct.reshape(-1,3),axis=0)))\n",
    "            cntrs = np.concatenate((cntrs,cntrsstruct))\n",
    "    if calccntrs: \n",
    "        if len(cntr)>3:\n",
    "            return pts.reshape(-1,3), cntrs.reshape(-1,3), cntr.reshape(-1,3)\n",
    "        else:\n",
    "            return pts.reshape(-1,3), cntrs.reshape(-1,3), cntr[np.newaxis,:]\n",
    "    else: \n",
    "        if CT:\n",
    "            return pts.reshape(-1,3), CTids\n",
    "        else:    \n",
    "            return pts.reshape(-1,3), _\n",
    "\n",
    "def subsample(PTVslice):\n",
    "    \"\"\"\n",
    "    Helper function to linearly insert new contour points between existing ones. Useful, when the voxel grid is tighter than the point spacing of the contours.\n",
    "    \"\"\"\n",
    "    half=(PTVslice+np.roll(PTVslice,1,0))/2\n",
    "    quarter1=(PTVslice+half)/2\n",
    "    quarter2 = (half+np.roll(PTVslice,1,0))/2\n",
    "    # Adding an additional level of points\n",
    "    eighth1 = (PTVslice + quarter1) / 2\n",
    "    eighth2 = (quarter1 + half) / 2\n",
    "    eighth3 = (half + quarter2) / 2\n",
    "    eighth4 = (quarter2 + np.roll(PTVslice, 1, 0)) / 2\n",
    "    st1 = (PTVslice + eighth1) / 2\n",
    "    st2 = (eighth1 + quarter1) / 2\n",
    "    st3 = (quarter1 + eighth2) / 2\n",
    "    st4 = (eighth2 + half) / 2\n",
    "    st5 = (half + eighth3) / 2\n",
    "    st6 = (eighth3 + quarter2) / 2\n",
    "    st7 = (quarter2 + eighth4) / 2\n",
    "    st8 = (eighth4 + np.roll(PTVslice, 1, 0)) / 2\n",
    "    PTVslice=np.concatenate((half,quarter1,quarter2, eighth1, eighth2, eighth3, eighth4, st1, st2, st3, st4, st5, st6, st7, st8))\n",
    "    return PTVslice\n",
    "       \n",
    "\n",
    "def interpolate_contour(TVpts,z,debug=False):\n",
    "    \"\"\"\n",
    "    Sample contour tighter with points. Useful, when the voxel grid is tighter than the point spacing of the contours.\n",
    "    \"\"\"\n",
    "    #Chose points belonging to slice\n",
    "    mask = np.isclose(TVpts[:,2],z)\n",
    "    if np.any(mask):\n",
    "        PTVslice=TVpts[mask][:,:2]#+spacing/2\n",
    "        if debug:\n",
    "            fig0=px.scatter(x=PTVslice[:,1],y=PTVslice[:,0])\n",
    "            fig0.show()\n",
    "        #Fill gaps, get better sampling of contour\n",
    "        half=(PTVslice+np.roll(PTVslice,1,0))/2\n",
    "        #Filter for disconnected contours\n",
    "        valid=(np.linalg.norm(half-PTVslice,axis=1))<2*np.median(np.linalg.norm(PTVslice-np.roll(PTVslice,1,0),axis=1))\n",
    "        if np.all(valid):\n",
    "            PTVslice=np.concatenate((PTVslice,subsample(PTVslice)))\n",
    "        else:\n",
    "            PTVsliceall=PTVslice.copy()\n",
    "            splits=np.arange(0,len(valid))[~valid]\n",
    "            for i in range(len(splits)):\n",
    "                if i==len(splits)-1:\n",
    "                    upper=len(valid)+1\n",
    "                else:\n",
    "                    upper=splits[i+1]\n",
    "                PTVslice=np.concatenate((PTVslice,subsample(PTVsliceall[splits[i]:upper])))\n",
    "        if debug:\n",
    "            print(np.all(valid))\n",
    "            fig1=px.scatter(x=PTVslice[:,1],y=PTVslice[:,0])\n",
    "            fig1.show()\n",
    "            print(np.max(PTVslice),np.min(PTVslice))\n",
    "        return PTVslice\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def to_grid(TVpts,z,origin,spacing,size,debug=False):\n",
    "    \"\"\"\n",
    "    Turn contour into a 2d boolean grid using specified dimensions and coordinates.\n",
    "    \"\"\"\n",
    "    PTVgrid=np.full(size, False)\n",
    "    PTVslice= interpolate_contour(TVpts,z,debug)\n",
    "    if np.any(PTVslice):\n",
    "        PTVslice = coordinate_space(PTVslice,origin,spacing)\n",
    "        PTVgrid[PTVslice[:,1],PTVslice[:,0]] = True #y is row, x is column\n",
    "        if debug:\n",
    "            print(np.max(PTVslice),np.min(PTVslice))\n",
    "            print(PTVslice.shape,PTVgrid.shape)\n",
    "            fig2=px.scatter(x=PTVslice[:,0],y=PTVslice[:,1])\n",
    "            fig2.show()\n",
    "            fig=px.imshow(PTVgrid)\n",
    "            fig.show()\n",
    "            input()\n",
    "    return PTVgrid\n",
    "\n",
    "def coordinate_space(pt,origin,spacing,start=False):\n",
    "    \"\"\"\n",
    "    Convert contour point coordinates in DICOM space into array indices \n",
    "    \"\"\"\n",
    "    if start:\n",
    "        r= np.floor((pt-origin[:len(spacing)])/(spacing*np.array([2,2,1])[:len(spacing)])).astype(int)*np.array([2,2,1])[:len(spacing)]\n",
    "    else:\n",
    "        if len(pt.shape)>2:\n",
    "            r= np.floor((pt-origin[np.newaxis,:len(spacing),np.newaxis,np.newaxis])/spacing[np.newaxis,:,np.newaxis,np.newaxis]).astype(int)\n",
    "        else:\n",
    "            r= np.floor((pt-origin[:len(spacing)])/(spacing*np.array([1,1,1])[:len(spacing)])).astype(int)*np.array([1,1,1])[:len(spacing)]\n",
    "    return r \n",
    "\n",
    "def real_space(coord,origin,spacing):\n",
    "    \"\"\"\n",
    "    Convert array indices to DICOM space coordinates\n",
    "    \"\"\"\n",
    "    pts=coord[:,[1,0,2]]*spacing+origin[:len(spacing)]\n",
    "    if len(spacing)==2:\n",
    "        pts+=spacing/2\n",
    "    elif len(spacing)==3:\n",
    "        pts+=np.array([spacing[0],spacing[1],0])/2\n",
    "    return pts\n",
    "\n",
    "def getoss(XYZ):\n",
    "    \"\"\"Get approximate spacing without CT.\n",
    "    The idea is to find points that share the same x or y coordinate and then take the most common spacing between them as spacing.\n",
    "    \"\"\"\n",
    "    Spts=np.round(XYZ,2)\n",
    "    mask0=Spts[:,0]==np.roll(Spts[:,0],1)\n",
    "    mask2=Spts[:,2]==np.roll(Spts[:,2],1) #CAN NOT BE CALCULATED IF STRUC HAS ONLY ONE SLICE, WOULD NEED TO STORE SLICE SPACING\n",
    "    mask=mask0*mask2\n",
    "    mask1=Spts[:,1]==np.roll(Spts[:,1],1)\n",
    "    mask=mask1*mask2\n",
    "    difs0=np.abs(Spts[mask][:,0]-np.roll(Spts[:,0],1)[mask])    \n",
    "    difs1=np.abs(Spts[mask][:,1]-np.roll(Spts[:,1],1)[mask])\n",
    "    difs=np.concatenate([difs0,difs1])\n",
    "    unique, counts = np.unique(difs[difs>0.5], return_counts=True)\n",
    "    spacin=np.round(unique[np.argmax(counts)],2)\n",
    "    spacing=np.array([spacin,spacin])   \n",
    "    size=np.roll(np.ceil(np.abs((np.max(XYZ[:,:2],0)-np.min(XYZ[:,:2],0)))/spacin).astype(int),1)+np.array([2,2])\n",
    "    minxyz=np.min(XYZ,axis=0)\n",
    "    return minxyz, spacing, size\n",
    "\n",
    "def correct_matches(PTVmatch,spacing):\n",
    "    PTVmatch=np.array(PTVmatch)\n",
    "    if len(PTVmatch.shape)>2:\n",
    "        difs=np.linalg.norm(PTVmatch-np.roll(PTVmatch,1,2),axis=1)\n",
    "        jumps1=difs>2*spacing[0]\n",
    "        jumps2=np.isfinite(PTVmatch[:,0,:,:]) \n",
    "        jumps=np.all([jumps1,jumps2],0)\n",
    "        # Assuming 'a' is your multi-dimensional boolean array and 'axis' is your specified axis\n",
    "        a = jumps\n",
    "        axis = 1\n",
    "\n",
    "        # Find the indices of the last True values along the specified axis\n",
    "        # indices = a.shape[axis] - np.argmax(a[:,::-1,:], axis=axis) -1\n",
    "        indices = np.argmax(a, axis=axis)\n",
    "\n",
    "        # Create a new boolean array of the same shape as 'a'\n",
    "        b = np.zeros_like(a)\n",
    "\n",
    "        # Set only the elements at the found indices to True\n",
    "        np.put_along_axis(b, np.expand_dims(indices, axis=axis), True, axis=axis)\n",
    "        x,y,z = np.meshgrid(np.arange(PTVmatch.shape[0]),np.arange(PTVmatch.shape[1]),np.arange(PTVmatch.shape[3]))\n",
    "        return PTVmatch[x,y,indices,z].transpose((1,0,2)),b\n",
    "\n",
    "def flatten(xss):\n",
    "    \"\"\"Get rid of higher dimension in lists.\"\"\"\n",
    "    return np.array([x for xs in xss for x in xs])\n",
    "\n",
    "def getfolder(base, kind):\n",
    "    \"\"\"Helper function which searches sibling folders for CT files\"\"\"\n",
    "    CTfiles=glob.glob(f\"{base}/*/{kind}*\")\n",
    "    if len(CTfiles)>0:\n",
    "        CTfolder=np.unique([f.split('\\\\')[-2] for f in CTfiles])\n",
    "    else: CTfolder = []\n",
    "    return CTfolder\n",
    "\n",
    "def mergeIntervalResults(folder=SaveFolder):\n",
    "    \"\"\"Combine all the different interval files into one.\"\"\"\n",
    "    df1=pd.concat([pd.read_pickle(i) for i in glob.glob(f\"{folder}/PTV[*\")],ignore_index=True)\n",
    "    df1.to_pickle(f\"{folder}/PTV.pickle\")\n",
    "    df2=pd.concat([pd.read_pickle(i) for i in glob.glob(f\"{folder}/Folders[*\")],ignore_index=True)\n",
    "    df2.to_pickle(f\"{folder}/Folders.pickle\")\n",
    "    df4=pd.concat([pd.read_pickle(i) for i in glob.glob(f\"{folder}/MU[*\")],ignore_index=True)\n",
    "    df4.to_pickle(f\"{folder}/MU.pickle\")\n",
    "    df5=pd.concat([pd.read_pickle(i) for i in glob.glob(f\"{folder}/PCAMesh[*\")],ignore_index=True)\n",
    "    df5.to_pickle(f\"{folder}/PCAMesh.pickle\")\n",
    "    df6=pd.concat([pd.read_pickle(i) for i in glob.glob(f\"{folder}/Treatable[*\")],ignore_index=True)\n",
    "    df6.to_pickle(f\"{folder}/Treatable.pickle\")\n",
    "    df7=pd.concat([pd.read_pickle(i) for i in glob.glob(f\"{folder}/Dose[*\")],ignore_index=True)\n",
    "    df7.to_pickle(f\"{folder}/Dose.pickle\")\n",
    "    df8=pd.concat([pd.read_pickle(i) for i in glob.glob(f\"{folder}/DistsPTVOAR[*\")],ignore_index=True)\n",
    "    df8.to_pickle(f\"{folder}/DistsPTVOAR.pickle\")\n",
    "    df9=pd.concat([pd.read_pickle(i) for i in glob.glob(f\"{folder}/HU[*\")],ignore_index=True)\n",
    "    df9.to_pickle(f\"{folder}/HU.pickle\")\n",
    "    return df1,df2,df4,df5,df6,df7,df8,df9 \n",
    "\n",
    "def collectStructureNames(folder='//Cifs2/imrt_qa$/EIT/Marvin/DicomStructures_Joe_2.xlsx',save=SaveFolder):\n",
    "    \"\"\"Check structure names to find what to search for, in this case whether Lungs is enough\"\"\"\n",
    "    ds = pd.read_excel(folder)\n",
    "    ds2= pd.read_excel(PTVInfoSheet)\n",
    "    countermoved=0\n",
    "    result={'PlanSetupSer' : [], 'Structures' : []}\n",
    "    counterlung=0\n",
    "    for i,RSf in enumerate(ds['StructureSet_DicomFIle_Path']):\n",
    "        site = ds2['Rx site'][ds2['PlanSetupSer']==ds['PlanSetupSer'][i]].iloc[0]\n",
    "        if np.any([s in site for s in [\"Lung\", \"LUNG\", \"lung\", \"LUL\", \"RUL\", \"LLL\", \"RLL\", \"Thoracic-NSCLC\", \"Thoracic-SCLC\"]]):\n",
    "            try:\n",
    "                RS =  pydicom.dcmread(RSf)\n",
    "            except:\n",
    "                countermoved+=1\n",
    "                continue\n",
    "            names=[]\n",
    "            for s in RS.StructureSetROISequence:\n",
    "                names.append(s.ROIName)\n",
    "            result['PlanSetupSer'].append(ds['PlanSetupSer'][i])\n",
    "            result['Structures'].append(names)\n",
    "            counterlung+=1\n",
    "            if counterlung == 50:\n",
    "                df = pd.DataFrame.from_dict(result)\n",
    "                df.to_pickle(f\"{save}/LungStructuresNames.pickle\")\n",
    "                print(f'Unavailable files: {countermoved}')\n",
    "                return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth & HU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeStructuresCT(RS=None,RP=None,save=None,folder=None,LoadCT=False,PTVnames=None,debug=0,mode=0,TimmermanDict=TimmermanDict):\n",
    "    \"\"\"\n",
    "    Main function to evaluate a structure file in connection to CT images, if wanted.\n",
    "\n",
    "    Input:\n",
    "    - RS : DICOM StructureSet\n",
    "    - RP : DICOM Plan (optional, then tries to load structure set specified in plan)\n",
    "    - save : str ; Folder where the result should be stored.\n",
    "    - folder : list ; folder locations of different DICOM files\n",
    "    - LoadCT : bool ; Whether to use CT images or not\n",
    "    - PTVnames : list ; List of PTV names that should be used for the analysis instead of searching for 'ptv' in structure names\n",
    "    - debug : bool ; changes behaviour of code to supply some more information while running\n",
    "    - mode : str ; Set to 'HU' to only calculate HU of structures.\n",
    "\n",
    "    Output:\n",
    "    - Dist.pickle : Pandas dataframe ; contains the depth analysis results. Columns: \"Name\", \"z\", \"q0\", \"d2\", \"d4\", \"d4_att\", \"q0_att\", \"q0_attLung\"\n",
    "    - results2: Pandas dataframe ; contains the TV structures as point clouds. Colmumns: \"Name\",'TV Type',\"CTOriginSpacingSize\", 'XYZ'\n",
    "    - HU.pickle : Pandas dataframe; contains HU results. Columns: \"PlanSetupSer\",\"Name\", 'TV Type', 'HU Counts', 'HU Values', 'HU Mean', 'HU First Quartile', 'HU Median', 'HU Third Quartile', 'In Lung', 'In Left Lung', 'In Right Lung'\n",
    "    \"\"\"\n",
    "    #Initialize result variables\n",
    "    results = {\"Name\" : [], \"z\" : [], \"q0\":[], \"d2\" : [], \"d4\" : [], \"d4_att\" : [], \"q0_att\" : [], \"q0_attLung\" : []}\n",
    "    results2 = {\"Name\" : [],'TV Type':[],\"CTOriginSpacingSize\" : [], 'XYZ' :[]}\n",
    "    results3 = {\"Name\" : [], 'Type' : [], 'HU Counts' : [], 'HU Values' : [], 'HU Mean' : [], 'HU First Quartile' : [], 'HU Median' : [], 'HU Third Quartile' : [], 'In Lung' : [], 'In Left Lung' : [], 'In Right Lung' : []}\n",
    "    ROITV = []\n",
    "    TVNameD = {}\n",
    "    GCPTVD = {}\n",
    "    ROIOAR= []\n",
    "    ROIbody = None\n",
    "    ROISkin = None\n",
    "    ROILung = None\n",
    "    ROILungL = None\n",
    "    ROILungR = None\n",
    "    if RP:\n",
    "        RS=pydicom.dcmread(f\"{folder[0]}/RS.{RP.ReferencedStructureSetSequence[0].ReferencedSOPInstanceUID}.dcm\")\n",
    "    #Go through all Structures and try to match them to given categories\n",
    "    for s in RS.StructureSetROISequence:\n",
    "        if debug: print(s.ROIName)\n",
    "        cfname=s.ROIName.casefold()\n",
    "        if cfname[0] in 'zx^':\n",
    "            continue\n",
    "        elif cfname == 'body' or cfname == 'outer contour':\n",
    "            if debug: print(s.ROIName,'is body')\n",
    "            ROIbody = s.ROINumber\n",
    "            continue\n",
    "        elif cfname == 'skin':\n",
    "            if debug: print(s.ROIName,'is Skin')\n",
    "            ROISkin = s.ROINumber\n",
    "            continue\n",
    "        elif \"ctv\" == cfname[:3]:\n",
    "            if debug: print(s.ROIName,'is CTV')\n",
    "            TVNameD[s.ROINumber]=(s.ROIName)\n",
    "            ROITV.append(s.ROINumber)\n",
    "            GCPTVD[s.ROINumber]='CTV'\n",
    "            continue\n",
    "        elif \"gtv\" in cfname[:4]:\n",
    "            if debug: print(s.ROIName,'is GTV')\n",
    "            TVNameD[s.ROINumber]=(s.ROIName)\n",
    "            ROITV.append(s.ROINumber)\n",
    "            GCPTVD[s.ROINumber]='GTV'\n",
    "            continue\n",
    "        if PTVnames:\n",
    "            if s.ROIName in PTVnames:\n",
    "                if debug: print(s.ROIName,'is PTV')\n",
    "                TVNameD[s.ROINumber]=(s.ROIName)\n",
    "                ROITV.append(s.ROINumber)\n",
    "                GCPTVD[s.ROINumber]='PTV'\n",
    "                continue\n",
    "        else:\n",
    "            if \"ptv\" in cfname: #in general want PTV and then blank,_,^,number\n",
    "                if not \"avoid\" in cfname and not \"overlap\" in cfname:\n",
    "                    if debug: print(s.ROIName,'is PTV')\n",
    "                    TVNameD[s.ROINumber]=(s.ROIName)\n",
    "                    ROITV.append(s.ROINumber)\n",
    "                    GCPTVD[s.ROINumber]='PTV'\n",
    "                    continue\n",
    "        if \"lungs\" in cfname: \n",
    "            if debug: print(s.ROIName, 'is Lung')\n",
    "            ROILung = s.ROINumber\n",
    "        elif cfname==\"lung_l\":\n",
    "            ROILungL=s.ROINumber\n",
    "        elif cfname==\"lung_r\":\n",
    "            ROILungR=s.ROINumber\n",
    "        #Not updated to match Timmerman\n",
    "        elif np.any([oar in cfname for oar in TimmermanDict.keys()]):\n",
    "            ROIOAR.append(s.ROINumber)\n",
    "            if debug: print(s.ROIName, 'is OAR')\n",
    "            \n",
    "    if not ROIbody:\n",
    "        ROIbody = ROISkin\n",
    "    if len(ROITV)==0 or not ROIbody:\n",
    "        logging.warning(f'No PTV or no body in RS UID {RS.SOPInstanceUID}')\n",
    "        if PTVnames: print(\"Searching for:\",PTVnames)\n",
    "        for s in RS.StructureSetROISequence:\n",
    "            print(s.ROIName)\n",
    "        return [],[]\n",
    "    ctrs = RS.ROIContourSequence\n",
    "    TVs = []\n",
    "    OARs = []\n",
    "    TVName=[]\n",
    "    GCPTV=[]\n",
    "    #Match contours to structures\n",
    "    for c in ctrs:\n",
    "        try:\n",
    "            _=c.ContourSequence\n",
    "        except:\n",
    "                continue\n",
    "        if c.ReferencedROINumber in ROITV:\n",
    "            TVs.append(c)\n",
    "            TVName.append(TVNameD[c.ReferencedROINumber])\n",
    "            GCPTV.append(GCPTVD[c.ReferencedROINumber])\n",
    "        elif c.ReferencedROINumber == ROIbody: \n",
    "            body = [c]\n",
    "        elif c.ReferencedROINumber == ROILung: \n",
    "            Lung = [c] \n",
    "        elif c.ReferencedROINumber == ROILungL: \n",
    "            LungL = [c] \n",
    "        elif c.ReferencedROINumber == ROILungR: \n",
    "            LungR = [c] \n",
    "        elif c.ReferencedROINumber in ROIOAR:\n",
    "            OARs.append(c)\n",
    "    #Load contour points\n",
    "    Spts,CTids  = get_points(body,CT=LoadCT)\n",
    "    if ROILung:\n",
    "        try:\n",
    "            Lpts,_ = get_points(Lung)\n",
    "        except:\n",
    "            logging.warning(f\"Lung Points could not be loaded in RS UID {RS.SOPInstanceUID}\")\n",
    "            ROILung=None\n",
    "    if ROILungL:\n",
    "        try:\n",
    "            LptsL,_ = get_points(LungL)\n",
    "        except:\n",
    "            logging.warning(f\"Lung Points could not be loaded in RS UID {RS.SOPInstanceUID}\")\n",
    "            ROILungL=None\n",
    "    if ROILungR:\n",
    "        try:\n",
    "            LptsR,_ = get_points(LungR)\n",
    "        except:\n",
    "            logging.warning(f\"Lung Points could not be loaded in RS UID {RS.SOPInstanceUID}\")\n",
    "            ROILungR=None\n",
    "    if mode != 'HU':\n",
    "        OARpts=[get_points([oar])[0] for oar in OARs]\n",
    "    TVptss=[get_points([PTV],True) for PTV in TVs]\n",
    "    #Try to load CT files from folders. Assumes that all of them are in one folder.\n",
    "    LoadCTmain = LoadCT\n",
    "    if LoadCT:\n",
    "        found=False\n",
    "        for f in folder:\n",
    "            try:\n",
    "                CT = pydicom.dcmread(f\"{f}/CT.{CTids[Spts[0,2]]}\")\n",
    "                found=True\n",
    "                folder=[f]\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        if found == False:\n",
    "            try:\n",
    "                CT = pydicom.dcmread(f\"{folder[0]}/CT.{CTids[Spts[0,2]]}.dcm\")\n",
    "            except:\n",
    "                logging.warning(f'CT file not encountered for {RS.SOPInstanceUID}, {Spts[0,2]}')\n",
    "                LoadCT=False\n",
    "        if LoadCT:\n",
    "            origin=np.array(CT.ImagePositionPatient)\n",
    "            spacing=np.array(CT.PixelSpacing)\n",
    "            origin-=np.array([spacing[0],spacing[1],0])/2\n",
    "            if spacing[0]!=spacing[1]:\n",
    "                warnings.warn(f\"Spacing not symmetric, change code\")\n",
    "            size=np.array([CT.Columns,CT.Rows])\n",
    "            for i,o in enumerate(origin[:2]): #origin should be negative, so switch corner \n",
    "                if o>0:\n",
    "                    origin[i]=o-size[i]*spacing[i]\n",
    "    if not LoadCT:\n",
    "        origin,spacing,size=getoss(Spts)\n",
    "        try:\n",
    "            _,spacing1,_=getoss(TVptss[0][0]) #might fail if ptv too small\n",
    "            size=np.ceil(size*(spacing/spacing1)).astype(int)\n",
    "            spacing=spacing1\n",
    "        except:\n",
    "            logging.warning(f'PTV spacing failed for {RS.SOPInstanceUID}, {TVName[0]}')   \n",
    "    #Determine Slice Spacing\n",
    "    unz = np.unique(Spts[:,2])\n",
    "    spac, counts= np.unique(np.abs(unz-np.roll(unz,1)), return_counts=True)\n",
    "    slicespacing=spac[np.argmax(counts)]\n",
    "    #Set up dictionaries to save boolean masks of contours for different structures and slices\n",
    "    PD_dict={}\n",
    "    if mode != 'HU':\n",
    "        MaskOAR_dict={}\n",
    "        Sgrid_dict={}\n",
    "    if ROILung: \n",
    "        MaskLung_dict={}\n",
    "        if LoadCT:\n",
    "            LungHU_dict={}\n",
    "    if ROILungL: \n",
    "        MaskLungL_dict={}\n",
    "        if LoadCT:\n",
    "            LungLHU_dict={}\n",
    "    if ROILungR: \n",
    "        MaskLungR_dict={}\n",
    "        if LoadCT:\n",
    "            LungRHU_dict={}\n",
    "    sampling=0.8 #Sampling parameter for the ray tracing algortihm. Gets multiplied with the spacing to determine the distance of ray sample points.\n",
    "    #Loop through all TVs \n",
    "    for p,_ in enumerate(TVs):\n",
    "        TVpts, TVcntrs, TVcntr = TVptss[p]\n",
    "        if mode != 'HU':\n",
    "            results2[\"Name\"].append(TVName[p])\n",
    "            results2[\"TV Type\"].append(GCPTV[p])\n",
    "            results2['XYZ'].append(TVpts)\n",
    "            results2[\"CTOriginSpacingSize\"].append([origin,np.array([spacing[0],spacing[1],slicespacing]),size])\n",
    "        if LoadCT:\n",
    "            results3[\"Name\"].append(TVName[p])\n",
    "            results3[\"Type\"].append(GCPTV[p])\n",
    "            TVHU_dict={}\n",
    "        if RP: \n",
    "            IsoDev = np.array(TVcntr[0]-RP.BeamSequence[0].ControlPointSequence[0].IsocenterPosition)\n",
    "            print(f\"StrucIso-PlanIso={IsoDev}\")\n",
    "        #Determine slices that need to be loaded for TV and loop through them\n",
    "        zs=np.unique(TVcntrs[:,2])\n",
    "        IsoinPTV=0\n",
    "        for z in zs:\n",
    "            #Try to find CT for slice and then convert it to HU\n",
    "            if LoadCT: \n",
    "                PD = PD_dict.get(z,False)\n",
    "                if not np.any(PD):\n",
    "                    try:\n",
    "                        CTids[z]\n",
    "                    except:\n",
    "                        continue\n",
    "                    try:\n",
    "                        CT = pydicom.dcmread(f\"{folder[0]}/CT.{CTids[z]}\")\n",
    "                    except:\n",
    "                        try:\n",
    "                            CT = pydicom.dcmread(f\"{folder[0]}/CT.{CTids[z]}.dcm\")\n",
    "                        except:\n",
    "                            logging.warning(f'CT file not encountered for {RS.SOPInstanceUID}')\n",
    "                            LoadCT=False\n",
    "                    if LoadCT: \n",
    "                        PD=(CT.RescaleSlope*CT.pixel_array+CT.RescaleIntercept)/1000#(-CT.RescaleIntercept)#1000 #use intercept instead of 1000, because min pixel is 0 and we do not want values <-1, as that would lead to a negative distance\n",
    "                        # PD[PD<0]=0\n",
    "                        PD_dict[z]=PD\n",
    "            #Turn Structures into binary masks\n",
    "            iso= TVcntrs[TVcntrs[:,2]==z][0][:2]\n",
    "            isoc = coordinate_space(iso,origin,spacing)\n",
    "            PTVgrid = binary_dilation(to_grid(TVpts,z,origin,spacing,size))\n",
    "            if mode != 'HU':\n",
    "                Sgrid = Sgrid_dict.get(z,False)\n",
    "                if not np.any(Sgrid):\n",
    "                    Sgrid = binary_dilation(to_grid(Spts,z,origin,spacing,size))#,iterations=2)\n",
    "                    Sgrid_dict[z]=Sgrid\n",
    "                if debug==2:\n",
    "                    if 'PTV' in TVName[p]:\n",
    "                        print(isoc,TVName[p])\n",
    "                        fig = px.imshow(Sgrid+PTVgrid, color_continuous_scale='gray',title=z)\n",
    "                        fig.show()\n",
    "                        # pxy=TVpts[TVpts[:,2]==z]\n",
    "                        # fig2=px.scatter(x=pxy[:,0],y=pxy[:,1])\n",
    "                        # fig2.show()\n",
    "                        input()\n",
    "                MaskOAR = MaskOAR_dict.get(z,False)\n",
    "                if not np.any(MaskOAR):\n",
    "                    MaskOAR=np.full(size, False)\n",
    "                    for oar in OARpts:\n",
    "                        MaskOAR+=binary_erosion(binary_fill_holes(binary_dilation(to_grid(oar,z,origin,spacing,size))))\n",
    "                    MaskOAR_dict[z]=MaskOAR\n",
    "            if ROILung: \n",
    "                MaskLung = MaskLung_dict.get(z,False)\n",
    "                if not np.any(MaskLung):\n",
    "                    Lgrid = binary_dilation(to_grid(Lpts,z,origin,spacing,size))#,iterations=2)\n",
    "                    MaskLung = binary_erosion(binary_fill_holes(Lgrid))#,iterations=2)\n",
    "                    if mode != 'HU':\n",
    "                        MaskOAR+=MaskLung\n",
    "                    MaskLung_dict[z]=MaskLung\n",
    "                    if LoadCT:\n",
    "                        LungHU_dict[z]=(PD*1000+1e4)*MaskLung\n",
    "            if ROILungL: \n",
    "                MaskLungL = MaskLungL_dict.get(z,False)\n",
    "                if not np.any(MaskLungL):\n",
    "                    LgridL = binary_dilation(to_grid(LptsL,z,origin,spacing,size))#,iterations=2)\n",
    "                    MaskLungL = binary_erosion(binary_fill_holes(LgridL))#,iterations=2)\n",
    "                    MaskLungL_dict[z]=MaskLungL\n",
    "                    if LoadCT:\n",
    "                        LungLHU_dict[z]=(PD*1000+1e4)*MaskLungL\n",
    "            if ROILungR: \n",
    "                MaskLungR = MaskLungR_dict.get(z,False)\n",
    "                if not np.any(MaskLungR):\n",
    "                    LgridR = binary_dilation(to_grid(LptsR,z,origin,spacing,size))#,iterations=2)\n",
    "                    MaskLungR = binary_erosion(binary_fill_holes(LgridR))#,iterations=2)\n",
    "                    MaskLungR_dict[z]=MaskLungR\n",
    "                    if LoadCT:\n",
    "                        LungRHU_dict[z]=(PD*1000+1e4)*MaskLungR\n",
    "            MaskPTV = binary_erosion(binary_fill_holes(PTVgrid))\n",
    "            #Perform Raytracing to find out distance from all points of TV contour for all angles to body contour\n",
    "            if mode != 'HU':\n",
    "                Smask = Spts[:,2]==z\n",
    "                mask = TVpts[:,2]==z\n",
    "                PTVptsxy=TVpts[mask][:,:2]\n",
    "                md=np.mean(np.linalg.norm(PTVptsxy-np.roll(PTVptsxy,1,0),axis=1))\n",
    "                skip=np.max([int(10/md),1])\n",
    "                PTVptsxy=PTVptsxy[::skip]\n",
    "                lengthray=(np.max(np.linalg.norm(Spts[Smask][:,np.newaxis,:2]-PTVptsxy[np.newaxis,:,:],axis=2))/np.min(spacing*sampling)+1).astype(int)\n",
    "                if MaskPTV[isoc[1],isoc[0]]:\n",
    "                    IsoinPTV+=1\n",
    "                    targets=np.concatenate((iso[np.newaxis,:],PTVptsxy))\n",
    "                else:\n",
    "                    targets=PTVptsxy\n",
    "                rays = targets[:,:,np.newaxis,np.newaxis] - np.arange(lengthray)[np.newaxis,np.newaxis,:,np.newaxis] * spacing[np.newaxis,:,np.newaxis,np.newaxis]*sampling * np.array([np.sin(-np.arange(0,360,2)* np.pi / 180.),np.cos(-np.arange(0,360,2)* np.pi / 180.)])[np.newaxis,:,np.newaxis,:]\n",
    "                mask=np.all([np.all([rays[:,0,:,:]<=np.max(Spts[Smask][:,0]), rays[:,0,:,:]>=np.min(Spts[Smask][:,0])],0),np.all([rays[:,1,:,:]<=np.max(Spts[Smask][:,1]), rays[:,1,:,:]>=np.min(Spts[Smask][:,1])],0)],0)\n",
    "                raycoordinates = coordinate_space(rays,origin,spacing)\n",
    "                inOAR=np.full(mask.shape,False)\n",
    "                inOAR[mask]=MaskOAR[raycoordinates[:,1,:,:][mask],raycoordinates[:,0,:,:][mask]]\n",
    "                inSkin=np.full(mask.shape,False)\n",
    "                inSkin[mask]=Sgrid[raycoordinates[:,1,:,:][mask],raycoordinates[:,0,:,:][mask]]\n",
    "                Smatchb=rays.copy()\n",
    "                Smatchb[~np.stack([inSkin,inSkin],1)]=np.inf\n",
    "                Smatch,Sbool = correct_matches(Smatchb,spacing)\n",
    "                PTVtoSkin=np.cumsum(np.roll(Sbool,1,1)[:,::-1,:],1)[:,::-1,:]\n",
    "                q0=np.sum(spacing[0]*sampling*PTVtoSkin,1)+spacing[0]\n",
    "                PTVtoSkininOAR=np.all([PTVtoSkin,inOAR],0)\n",
    "                d4=np.sum(spacing[0]*sampling*PTVtoSkininOAR,1)\n",
    "                if ROILung: \n",
    "                    inLung=np.full(mask.shape,False)\n",
    "                    inLung[mask]=MaskLung[raycoordinates[:,1,:,:][mask],raycoordinates[:,0,:,:][mask]]\n",
    "                    PTVtoSkininLung=np.all([PTVtoSkin,inLung],0)\n",
    "                    d2=np.sum(spacing[0]*sampling*PTVtoSkininLung,1)\n",
    "                    q0_attLung=q0+d2*((-650)/1000)\n",
    "            #Calculate Attenuation Corrected Water Equivalent Distances using HU from CT\n",
    "            if LoadCT:  \n",
    "                if mode != 'HU':\n",
    "                    inc=np.zeros(mask.shape)\n",
    "                    inc[mask]=spacing[0]*sampling+spacing[0]*sampling*PD[raycoordinates[:,1,:,:][mask],raycoordinates[:,0,:,:][mask]]\n",
    "                    q0_att=np.sum(inc*PTVtoSkin,1)+spacing[0]\n",
    "                    # q0_att_max=np.max(np.min(q0_att,1))\n",
    "                    d4_att=np.sum(inc*PTVtoSkininOAR,1)\n",
    "                TVHU_dict[z]=(PD*1000+1e4)*MaskPTV\n",
    "            else:\n",
    "                if mode != 'HU':\n",
    "                    d4_att=np.nan\n",
    "                    q0_att=np.nan\n",
    "            if mode != 'HU':\n",
    "                results[\"Name\"].append(TVName[p])\n",
    "                results[\"z\"].append(z)\n",
    "                results[\"d4\"].append(d4.T/10) #Distance between PTV and skin inside of OARs\n",
    "                results[\"q0\"].append(q0.T/10) #Distance between PTV and skin\n",
    "                try:\n",
    "                    q0_att=q0_att.T\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    d4_att=d4_att.T\n",
    "                except:\n",
    "                    pass\n",
    "                results[\"q0_att\"].append(q0_att/10) #Distance between PTV and skin attenuation corrected\n",
    "                results[\"d4_att\"].append(d4_att/10) #Distance between PTV and skin inside of OARs attenuation corrected\n",
    "                if ROILung:  \n",
    "                    results[\"d2\"].append(d2.T/10) #Distance between PTV and skin inside of lung\n",
    "                    results[\"q0_attLung\"].append(q0_attLung.T/10) #Distance between PTV and skin attenuation corrected only in lung\n",
    "                else:                \n",
    "                    results[\"d2\"].append(np.zeros(d4.T.shape))\n",
    "                    results[\"q0_attLung\"].append(q0.T/10)\n",
    "            if LoadCTmain: LoadCT=True\n",
    "        #Store HU of structures and check if in lungs\n",
    "        if LoadCT:\n",
    "            TVHU_acc=[]\n",
    "            if ROILung or ROILungL or ROILungR:      \n",
    "                inLung=True           \n",
    "                inLungL=True              \n",
    "                inLungR=True       \n",
    "            else:       \n",
    "                inLung=False           \n",
    "                inLungL=False              \n",
    "                inLungR=False \n",
    "            for key, value in TVHU_dict.items():\n",
    "                TVHU_acc.append((value[value>0]-1e4).astype(int))\n",
    "                if ROILung or ROILungL or ROILungR:\n",
    "                    inner=binary_dilation(value.astype(bool),iterations=1)\n",
    "                    circ=binary_dilation(inner,iterations=1)^inner\n",
    "                    if ROILung:\n",
    "                        L_value=LungHU_dict[key]\n",
    "                        intsec=circ & L_value.astype(bool)\n",
    "                        # fig = px.imshow(inner+(circ*10)+(intsec*30))\n",
    "                        # fig.show()\n",
    "                        # input()\n",
    "                        if np.sum(intsec)/np.sum(circ)<0.8:\n",
    "                            inLung=False\n",
    "                        if GCPTV[p]=='CTV' or GCPTV[p]=='GTV':\n",
    "                            L_value[value.astype(bool)]=0\n",
    "                            LungHU_dict[key]=L_value\n",
    "                    else:\n",
    "                        inLung=False\n",
    "                    if ROILungL:\n",
    "                        L_value=LungLHU_dict[key]\n",
    "                        intsec=circ & L_value.astype(bool)\n",
    "                        if np.sum(intsec)/np.sum(circ)<0.8:\n",
    "                            inLungL=False\n",
    "                        if GCPTV[p]=='CTV' or GCPTV[p]=='GTV':\n",
    "                            L_value[value.astype(bool)]=0\n",
    "                            LungLHU_dict[key]=L_value\n",
    "                    else:\n",
    "                        inLungL=False\n",
    "                    if ROILungR:\n",
    "                        L_value=LungRHU_dict[key]\n",
    "                        intsec=circ & L_value.astype(bool)\n",
    "                        if np.sum(intsec)/np.sum(circ)<0.8:\n",
    "                            inLungR=False\n",
    "                        if GCPTV[p]=='CTV' or GCPTV[p]=='GTV':\n",
    "                            L_value[value.astype(bool)]=0\n",
    "                            LungRHU_dict[key]=L_value\n",
    "                    else:\n",
    "                        inLungR=False\n",
    "            TVHU_acc=flatten(TVHU_acc)\n",
    "            value,count=np.unique(TVHU_acc,return_counts=True)\n",
    "            results3[\"HU Counts\"].append(count)\n",
    "            results3[\"HU Values\"].append(value)\n",
    "            results3[\"HU Mean\"].append(np.mean(TVHU_acc))\n",
    "            one,two,three=np.quantile(TVHU_acc,[0.25,0.50,0.75])\n",
    "            results3[\"HU First Quartile\"].append(one)\n",
    "            results3[\"HU Median\"].append(two)\n",
    "            results3[\"HU Third Quartile\"].append(three)\n",
    "            results3['In Lung'].append(inLung)\n",
    "            results3['In Left Lung'].append(inLungL)\n",
    "            results3['In Right Lung'].append(inLungR)\n",
    "    if LoadCT:\n",
    "        if ROILung:\n",
    "            results3[\"Name\"].append('Lung')\n",
    "            results3[\"Type\"].append('Lung')\n",
    "            LungHU_acc=[]  \n",
    "            for key, value in LungHU_dict.items():\n",
    "                LungHU_acc.append((value[value!=0]-1e4).astype(int))\n",
    "            LungHU_acc=flatten(LungHU_acc)\n",
    "            value,count=np.unique(LungHU_acc,return_counts=True)\n",
    "            results3[\"HU Counts\"].append(count)\n",
    "            results3[\"HU Values\"].append(value)\n",
    "            results3[\"HU Mean\"].append(np.mean(LungHU_acc))\n",
    "            one,two,three=np.quantile(LungHU_acc,[0.25,0.50,0.75])\n",
    "            results3[\"HU First Quartile\"].append(one)\n",
    "            results3[\"HU Median\"].append(two)\n",
    "            results3[\"HU Third Quartile\"].append(three)\n",
    "            results3['In Lung'].append(True)\n",
    "            results3['In Left Lung'].append(False)\n",
    "            results3['In Right Lung'].append(False)\n",
    "        if ROILungL:\n",
    "            results3[\"Name\"].append('Left Lung')\n",
    "            results3[\"Type\"].append('Left Lung')\n",
    "            LungHU_acc=[]  \n",
    "            for key, value in LungLHU_dict.items():\n",
    "                LungHU_acc.append((value[value!=0]-1e4).astype(int))\n",
    "            LungHU_acc=flatten(LungHU_acc)\n",
    "            value,count=np.unique(LungHU_acc,return_counts=True)\n",
    "            results3[\"HU Counts\"].append(count)\n",
    "            results3[\"HU Values\"].append(value)\n",
    "            results3[\"HU Mean\"].append(np.mean(LungHU_acc))\n",
    "            one,two,three=np.quantile(LungHU_acc,[0.25,0.50,0.75])\n",
    "            results3[\"HU First Quartile\"].append(one)\n",
    "            results3[\"HU Median\"].append(two)\n",
    "            results3[\"HU Third Quartile\"].append(three)\n",
    "            results3['In Lung'].append(True)\n",
    "            results3['In Left Lung'].append(True)\n",
    "            results3['In Right Lung'].append(False)\n",
    "        if ROILungR:\n",
    "            results3[\"Name\"].append('Right Lung')\n",
    "            results3[\"Type\"].append('Right Lung')\n",
    "            LungHU_acc=[]  \n",
    "            for key, value in LungRHU_dict.items():\n",
    "                LungHU_acc.append((value[value!=0]-1e4).astype(int))\n",
    "            LungHU_acc=flatten(LungHU_acc)\n",
    "            value,count=np.unique(LungHU_acc,return_counts=True)\n",
    "            results3[\"HU Counts\"].append(count)\n",
    "            results3[\"HU Values\"].append(value)\n",
    "            results3[\"HU Mean\"].append(np.mean(LungHU_acc))\n",
    "            one,two,three=np.quantile(LungHU_acc,[0.25,0.50,0.75])\n",
    "            results3[\"HU First Quartile\"].append(one)\n",
    "            results3[\"HU Median\"].append(two)\n",
    "            results3[\"HU Third Quartile\"].append(three)\n",
    "            results3['In Lung'].append(True)\n",
    "            results3['In Left Lung'].append(False)\n",
    "            results3['In Right Lung'].append(True)\n",
    "    df=pd.DataFrame.from_dict(results)\n",
    "    if save: \n",
    "        if not os.path.exists(save):\n",
    "            os.makedirs(save)\n",
    "        #Contains the calculated distances from raytracing\n",
    "        if mode != 'HU':\n",
    "            if debug: df.to_csv(f\"{save}/Dist.csv\", index=False)\n",
    "            df.to_pickle(f\"{save}/Dist.pickle\")\n",
    "        #contains HU values\n",
    "        if LoadCT:\n",
    "            df3=pd.DataFrame.from_dict(results3)\n",
    "            df3.to_pickle(f\"{save}/HU.pickle\")\n",
    "    return df,results2\n",
    "\n",
    "def batchAnalyzeStructuresCT_sheet(loc_sheet='//Cifs2/imrt_qa$/EIT/Marvin/DicomStructures_Joe_3.xlsx',PTV_sheet=PTVInfoSheet,interval=[],debug=False,mode=0, save=SaveFolder):\n",
    "    \"\"\"\n",
    "    Given an excel sheet of RT DICOM file locations and another with PTV names and Plan IDs, this function runs the main analysis function in batch mode.\n",
    "    \n",
    "    Input:\n",
    "    loc_sheet : excel sheet of RT DICOM file locations\n",
    "    PTV_sheet : excel sheet of PTV names and Plan IDs\n",
    "    interval : list of int ; Only load locations in this interval from sheet\n",
    "    debug : bool ; changes behaviour of code to supply some more information while running\n",
    "    mode : str ; Set to 'HU' to only calculate HU of structures.\n",
    "\n",
    "    Output:\n",
    "    PTV.pickle: Pandas dataframe ; contains the TV structures as point clouds, columns: 'PlanSetupSer',\"Name\",'TV Type',\"CTOriginSpacingSize\",'XYZ'\n",
    "    Files.pickle : Pandas dataframe; contains the CT folder location. Assumes that CT images are stored in a different folder from the structures, but in the same base folder. Like base/structures and base/CT. Could be solved more elegant by providing CT folder location in the location sheet. Columns: 'PlanSetupSer', 'Root Folder', 'CT Folder'\n",
    "    \"\"\"\n",
    "    results2 = {'PlanSetupSer':[],\"Name\" : [],'TV Type':[],\"CTOriginSpacingSize\" : [],'XYZ' :[]}\n",
    "    # results3 = {'PlanSetupSer':[], 'Root Folder' : [],'RP Folder' : [], 'CT Folder' : [], 'RD Folder' : [], 'RI Folder' : []}\n",
    "    results3 = {'PlanSetupSer':[], 'Root Folder' : [], 'CT Folder' : []}\n",
    "    ds=updateLocs(loc_sheet,debug)\n",
    "    if interval: ds=ds.iloc[interval[0]:interval[1]]\n",
    "    ds2= pd.read_excel(PTV_sheet)\n",
    "    countermoved=0\n",
    "    if not os.path.exists(save):\n",
    "        os.makedirs(save)\n",
    "    logging.basicConfig(filename=f'{save}/LogsPTV.log', encoding='utf-8', level=logging.WARNING)\n",
    "    for i,RSf in enumerate(ds['StructureSet_Path']):\n",
    "        save2 = f\"{save}/{ds['PlanSetupSer'].iloc[i]}\"\n",
    "        if not os.path.exists(save2) or mode == 'HU':\n",
    "            try:\n",
    "                RS =  pydicom.dcmread(RSf)\n",
    "                base='/'.join(RSf.split('\\\\')[:-2])\n",
    "                # RPfolder = getfolder(base, \"RP\")\n",
    "                CTfolder = getfolder(base, \"CT\")    #assumes that CT images are stored in a different folder from the structures, but in the same base folder. Like base/structures and base/CT. Could be solved more elegant by providing CT folder location in the location sheet.\n",
    "                # RDfolder = getfolder(base, \"RD\")\n",
    "                # RIfolder = getfolder(base, \"RI\")\n",
    "                results3['PlanSetupSer'].append(ds['PlanSetupSer'].iloc[i])\n",
    "                results3['Root Folder'].append(base)\n",
    "                # results3['RP Folder'].append(RPfolder)\n",
    "                results3['CT Folder'].append(CTfolder)\n",
    "                # results3[\"RD Folder\"].append(RDfolder)\n",
    "                # results3['RI Folder'].append(RIfolder)\n",
    "            except:\n",
    "                countermoved+=1\n",
    "                logging.exception(f\"Could not find PlanSetupSer {ds['PlanSetupSer'].iloc[i]}.\")\n",
    "                continue\n",
    "            print(i)\n",
    "            TVs=ds2['PTV name'][ds2['PlanSetupSer']==ds['PlanSetupSer'].iloc[i]].tolist()\n",
    "            try:\n",
    "                if len(CTfolder)>0:\n",
    "                    _,rs = analyzeStructuresCT(RS=RS,save=save2,PTVnames=TVs,folder=[f\"{base}/{CTf}\" for CTf in CTfolder],LoadCT=True,debug=debug,mode=mode)\n",
    "                else:\n",
    "                    _,rs = analyzeStructuresCT(RS=RS,save=save2,PTVnames=TVs,debug=debug,mode=mode) #Search for TVs mentioned in excel only instead of general search, because some TVs are not called PTV, and can also get wrong structures when just searching for PTV\n",
    "            except BaseException:\n",
    "                logging.exception(f\"analyzeStructuresCT failed for PlanSetupSer {ds['PlanSetupSer'].iloc[i]}.\")\n",
    "                continue\n",
    "            if mode != 'HU':\n",
    "                if len(rs)!=0: \n",
    "                    results2[\"Name\"].extend(rs[\"Name\"])\n",
    "                    results2[\"CTOriginSpacingSize\"].extend(rs[\"CTOriginSpacingSize\"])\n",
    "                    results2[\"XYZ\"].extend(rs[\"XYZ\"])\n",
    "                    results2[\"TV Type\"].extend(rs[\"TV Type\"])\n",
    "                    for _ in range(len(rs[\"Name\"])):\n",
    "                        results2['PlanSetupSer'].append(ds['PlanSetupSer'].iloc[i])\n",
    "    if mode != 'HU':        \n",
    "        df = pd.DataFrame.from_dict(results2)\n",
    "        df3 = pd.DataFrame.from_dict(results3)\n",
    "        if interval:\n",
    "            df.to_pickle(f\"{save}/PTV{interval}.pickle\")\n",
    "            df3.to_pickle(f\"{save}/Folders{interval}.pickle\")\n",
    "        else:    \n",
    "            try:\n",
    "                shutil.copyfile(f\"{save}/PTV.pickle\",f\"{save}/PTV.pickle.old\")\n",
    "                shutil.copyfile(f\"{save}/Folders.pickle\",f\"{save}/Folders.pickle.old\")\n",
    "                PTV=pd.read_pickle(f\"{save}/PTV.pickle\")\n",
    "                Folders=pd.read_pickle(f\"{save}/Folders.pickle\")\n",
    "                df=pd.concat([PTV,df])\n",
    "                df3=pd.concat([Folders,df3])\n",
    "                df.to_pickle(f\"{save}/PTV.pickle\")\n",
    "                df3.to_pickle(f\"{save}/Folders.pickle\")\n",
    "            except:\n",
    "                if not os.path.exists(f\"{save}/PTV.pickle\"):\n",
    "                    df.to_pickle(f\"{save}/PTV.pickle\")\n",
    "                    df3.to_pickle(f\"{save}/Folders.pickle\")\n",
    "                else:\n",
    "                    logging.exception(f\"PTV.pickle exists, but appending failed. Create PTVnew.\")\n",
    "                    df.to_pickle(f\"{save}/PTVnew.pickle\")\n",
    "                    df3.to_pickle(f\"{save}/Foldersnew.pickle\")\n",
    "        print(f'Unavailable files: {countermoved}')\n",
    "        logging.warning(f'Unavailable files: {countermoved}')\n",
    "        return df,df3\n",
    "    \n",
    "def batchAnalyzeStructuresCTPlan_folder(folder='//Cifshd/homedir$/Data/Mobius1_4_patient_HD',save=f\"//Cifshd/homedir$/Evaluation/Mobius1_4_patient_HD\",debug=False,analyzePlan=True):\n",
    "    \"\"\"Analyzes Plan and Structure files in one folder\n",
    "    \n",
    "    Input:\n",
    "    folder : folder with DICOM files\n",
    "    save : folder where results should be stored\n",
    "    debug : bool ; changes behaviour of code to supply some more information while running\n",
    "    analyzePlan : bool ; Whether to analyze plans as well or only structures and CT\n",
    "\n",
    "    Output:\n",
    "    results2: Pandas dataframe ; contains the TV structures as point clouds\n",
    "    resultsMU : Pandas dataframe; contains the plan analysis results\n",
    "    \n",
    "    Also stores output specified in analyzeStructuresCT function, namely:\n",
    "    results : Pandas dataframe ; contains the depth analysis results\n",
    "    results3 : Pandas dataframe; contains HU results\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    results2 = {'PlanSetupSer':[],\"Name\" : [],'TV Type':[],\"CTOriginSpacingSize\" : [],'XYZ' :[]}\n",
    "    resultsMU = {'PlanSetupSer' : [], 'Beam Name' : [], 'Meterset [MU]' : [], 'Gantry Angle' : [], 'MU per Angle' : [], 'Beam Delivery Duration Limit [s]':[],'Table Top Roll Angle' :[],'Table Top Pitch Angle':[],\n",
    "                 'Patient Support Angle' : [],'Table Top Eccentric Angle':[],'Beam Limiting Device Position Sequence' : [],'Beam Limiting Device Angle' : [],'SSD [cm]' : [],'Dose Rate Set [MU/min]' : [],\n",
    "                   'Dose [Gy]':[],'High Dose Technique Type':[],'Radiation Type':[],'Beam Type':[], 'Isocenter [mm]':[]}\n",
    "    for i,RSf in enumerate(glob.glob(f\"{folder}/RS*\")):\n",
    "        RS =  pydicom.dcmread(RSf)\n",
    "        save2 = f\"{save}/{RS.SOPInstanceUID}\"\n",
    "        print(save)\n",
    "        _,rs = analyzeStructuresCT(RS=RS,save=save2,folder=[folder],LoadCT=True,debug=debug)\n",
    "        if len(rs)!=0: \n",
    "            results2[\"Name\"].extend(rs[\"Name\"])\n",
    "            results2[\"CTOriginSpacingSize\"].extend(rs[\"CTOriginSpacingSize\"])\n",
    "            results2[\"XYZ\"].extend(rs[\"XYZ\"])\n",
    "            results2[\"TV Type\"].extend(rs[\"TV Type\"])\n",
    "            for _ in range(len(rs[\"Name\"])):\n",
    "                results2['PlanSetupSer'].append(RS.SOPInstanceUID)\n",
    "    df = pd.DataFrame.from_dict(results2)\n",
    "    df.to_pickle(f\"{save}/PTV.pickle\")\n",
    "    if analyzePlan:\n",
    "        for RPfileloc in glob.glob(f\"{folder}/RP*\"):\n",
    "            RPfile=pydicom.read_file(RPfileloc)\n",
    "            angle, mu, beam, Roll, Pitch, Yaw, TableTopEccentricAngle,BeamLimitingDevicePositionSequence,BeamDeliveryDurationLimit, iso, Meterset,Dose,BeamDeliveryDurationLimit,BeamLimitingDeviceAngle,SSD,DoseRateSet,HighDoseTechniqueType,RadiationType,BeamType=analyzePlan(RPfile)\n",
    "            if len(angle)!=0: \n",
    "                resultsMU[\"Gantry Angle\"].append(angle)\n",
    "                resultsMU[\"MU per Angle\"].append(mu)\n",
    "                resultsMU[\"Beam Name\"].append(beam)\n",
    "                resultsMU[\"Table Top Roll Angle\"].append(Roll)\n",
    "                resultsMU[\"Table Top Pitch Angle\"].append(Pitch)\n",
    "                resultsMU[\"Patient Support Angle\"].append(Yaw)\n",
    "                resultsMU[\"Table Top Eccentric Angle\"].append(TableTopEccentricAngle)\n",
    "                resultsMU[\"Beam Limiting Device Position Sequence\"].append(BeamLimitingDevicePositionSequence)\n",
    "                resultsMU[\"Beam Delivery Duration Limit [s]\"].append(BeamDeliveryDurationLimit)\n",
    "                resultsMU[\"Isocenter [mm]\"].append(iso)\n",
    "                resultsMU[\"Meterset [MU]\"].append(Meterset)\n",
    "                resultsMU[\"Dose [Gy]\"].append(Dose)\n",
    "                resultsMU[\"Beam Limiting Device Angle\"].append(BeamLimitingDeviceAngle)\n",
    "                resultsMU[\"SSD [cm]\"].append(SSD)\n",
    "                resultsMU[\"Dose Rate Set [MU/min]\"].append(DoseRateSet)\n",
    "                resultsMU[\"High Dose Technique Type\"].append(HighDoseTechniqueType)\n",
    "                resultsMU[\"Radiation Type\"].append(RadiationType)\n",
    "                resultsMU[\"Beam Type\"].append(BeamType)\n",
    "                resultsMU['PlanSetupSer'].append(RPfile.ReferencedStructureSetSequence[0].ReferencedSOPInstanceUID)\n",
    "        dfMU = pd.DataFrame.from_dict(resultsMU)\n",
    "        dfMU.to_pickle(f\"{save}/MU.pickle\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze HU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileHU(folder,interval=None):\n",
    "    \"\"\"\n",
    "    Go through all HU results in folders\n",
    "    \n",
    "    \n",
    "    Input:\n",
    "    - folder : folder where analyzeStructuresCT stored the HU dataframes\n",
    "    - interval : list of int ; Only load locations in this interval from folder\n",
    "\n",
    "    Output:\n",
    "    - HU.pickle : Pandas dataframe; contains HU results for all plans. Columns: \"PlanSetupSer\",\"Name\", 'TV Type', 'HU Counts', 'HU Values', 'HU Mean', 'HU First Quartile', 'HU Median', 'HU Third Quartile', 'In Lung', 'In Left Lung', 'In Right Lung'\n",
    "    \"\"\"\n",
    "\n",
    "    results = {\"PlanSetupSer\" : [],\"Name\" : [], 'TV Type' : [], 'HU Counts' : [], 'HU Values' : [], 'HU Mean' : [], 'HU First Quartile' : [], 'HU Median' : [], 'HU Third Quartile' : [], 'In Lung' : [], 'In Left Lung' : [], 'In Right Lung' : []}\n",
    "    try:\n",
    "        done=pd.read_pickle(f\"{folder}/HU.pickle\")\n",
    "    except:\n",
    "        pass\n",
    "    PSSs=glob.glob(f\"{folder}/[0-9]*\")\n",
    "    if interval:\n",
    "        PSSs=PSSs[interval[0]:interval[1]]\n",
    "    for j,f1 in enumerate(PSSs):\n",
    "        if 'done' in locals():\n",
    "            if np.any(int(f1.split('\\\\')[-1]) == done['PlanSetupSer']):\n",
    "                continue\n",
    "        print(j, f1)\n",
    "        try:\n",
    "            df = pd.read_pickle(f\"{f1}/HU.pickle\")\n",
    "        except:\n",
    "            continue\n",
    "        for i in range(len(df['Name'])):\n",
    "            try:\n",
    "                results['PlanSetupSer'].append(int(f1.split('\\\\')[-1]))\n",
    "            except:\n",
    "                results['PlanSetupSer'].append(f1.split('\\\\')[-1])\n",
    "            results[\"Name\"].append(df['Name'][i])\n",
    "            results[\"TV Type\"].append(df['Type'][i])\n",
    "            results[\"HU Counts\"].append(df['HU Counts'][i])\n",
    "            results[\"HU Values\"].append(df['HU Values'][i])\n",
    "            results[\"HU Mean\"].append(df['HU Mean'][i])\n",
    "            results[\"HU First Quartile\"].append(df['HU First Quartile'][i])\n",
    "            results[\"HU Median\"].append(df['HU Median'][i])\n",
    "            results[\"HU Third Quartile\"].append(df['HU Third Quartile'][i])\n",
    "            results[\"In Lung\"].append(df['In Lung'][i])\n",
    "            results[\"In Left Lung\"].append(df['In Left Lung'][i])\n",
    "            results[\"In Right Lung\"].append(df['In Right Lung'][i])\n",
    "    df=pd.DataFrame.from_dict(results)\n",
    "    if 'done' in locals():\n",
    "        df=pd.concat((done,df))\n",
    "    df.to_pickle(f\"{folder}/HU{interval if np.any(interval) else ''}.pickle\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineMinMaxDepths(ds, dist):\n",
    "    \"\"\"\n",
    "    Determine Max and Min Depths of contours\n",
    "    \n",
    "    Input:\n",
    "    ds : Pandas dataframe; Contains the depths results generated for one structure by analyzeStructuresCT\n",
    "    dist : str ; Name of the column of the depth of interest\n",
    "\n",
    "    Output:\n",
    "    viableSlicesPercentage : float ; percentage of slices where max min depth is smaller than 8\n",
    "    maxmindepth : float ; distance needed at minimum to treat whole structure\n",
    "    minmindepth : float ; distance needed at minimum to reach structure\n",
    "    dang : float ; Mean of depth across points and slices per angle. ATTENTION: Angle for one point is not the same as for another, given that each point is iso. But maybe okay if source is far? Otherwise just use first point, which is iso in the center\n",
    "    \"\"\"\n",
    "    depth=np.array([np.max(np.min(z,axis=0)) for z in ds[dist]]) #min of angles, max of points gives one value per slice\n",
    "    viableslices=depth<8\n",
    "    try:\n",
    "        depthmin=np.array([np.min(z[np.all([(z-np.roll(z,1,0))<0.5,(z-np.roll(z,1,0))>0.1],0)]) for z in ds[dist]]) #Filter out min outliers. Black magic happening. Basically, check that distance jump to next angle is not too big, but also not very small as for multiple outliers next to each other\n",
    "        minmindepth=np.min(depthmin[~np.isinf(depthmin)])\n",
    "    except:\n",
    "        minmindepth=np.nan    \n",
    "    maxmindepth=np.max(depth[~np.isinf(depth)]) #take max dist as depth. More accurate than mean, as that is what really has to be treated at most\n",
    "    dang = np.mean([np.mean(z,axis=1) for z in ds[dist]],axis=0) #Mean of depth across points and slices per angle. ATTENTION: Angle for one point is not the same as for another, given that each point is iso. But maybe okay if source is far? Otherwise just use first point, which is iso in the center\n",
    "    viableSlicesPercentage=np.sum(viableslices)/len(viableslices)\n",
    "    return viableSlicesPercentage, maxmindepth, minmindepth, dang\n",
    "\n",
    "def planDetermineMinMaxDepths(df,cols = [\"q0\",\"q0_att\",\"q0_attLung\"]):\n",
    "    \"\"\"Wrapper for depths function to use it in batch mode.\"\"\"\n",
    "    Names = df['Name'].unique()\n",
    "    treatableq2CT=[]\n",
    "    treatableq2L=[]\n",
    "    treatableq2=[]\n",
    "    maxMinDepth_q0_att=[]\n",
    "    maxMinDepth_q0=[]\n",
    "    maxMinDepth_q0_attLung=[]\n",
    "    minMinDepth_q0_att=[]\n",
    "    minMinDepth_q0=[]\n",
    "    minMinDepth_q0_attLung=[]\n",
    "    dang2CT=[]\n",
    "    dang2=[]\n",
    "    dang2L=[]\n",
    "    for Name in Names:   \n",
    "        dfPTV = df[df['Name']==Name]\n",
    "        for d in cols:\n",
    "            if d == \"q0_att\": \n",
    "                if np.any(np.isnan(dfPTV[d].iloc[0])):\n",
    "                    treatableq2CT.append(np.nan)\n",
    "                    maxMinDepth_q0_att.append(np.nan)\n",
    "                    minMinDepth_q0_att.append(np.nan)\n",
    "                    dang2CT.append(np.nan)\n",
    "                else:\n",
    "                    tr,mmd, minmindepth,dang=determineMinMaxDepths(dfPTV,d)\n",
    "                    treatableq2CT.append(tr)    \n",
    "                    maxMinDepth_q0_att.append(mmd)\n",
    "                    minMinDepth_q0_att.append(minmindepth)\n",
    "                    dang2CT.append(dang)\n",
    "            elif d == \"q0_attLung\": \n",
    "                    tr,mmd, minmindepth,dang=determineMinMaxDepths(dfPTV,d)\n",
    "                    treatableq2L.append(tr)    \n",
    "                    maxMinDepth_q0_attLung.append(mmd)\n",
    "                    minMinDepth_q0_attLung.append(minmindepth)\n",
    "                    dang2L.append(dang)\n",
    "            elif d == \"q0\": \n",
    "                    tr,mmd, minmindepth,dang=determineMinMaxDepths(dfPTV,d)\n",
    "                    treatableq2.append(tr)    \n",
    "                    maxMinDepth_q0.append(mmd)\n",
    "                    minMinDepth_q0.append(minmindepth)\n",
    "                    dang2.append(dang)\n",
    "    return Names, treatableq2CT, treatableq2L, treatableq2,maxMinDepth_q0_att,maxMinDepth_q0_attLung,maxMinDepth_q0,minMinDepth_q0_att,minMinDepth_q0_attLung,minMinDepth_q0, dang2CT, dang2L, dang2\n",
    "\n",
    "def batchDetermineMinMaxDepths(folder,plot=False,interval=None):\n",
    "    \"\"\"\n",
    "    Wrapper for planDetermineMinMaxDepths to go through all results in folders\n",
    "    \n",
    "    Input:\n",
    "    - folder : folder where analyzeStructuresCT stored the HU dataframes\n",
    "    - interval : list of int ; Only load locations in this interval from folder\n",
    "\n",
    "    Output:\n",
    "    - Treatable.pickle : Pandas dataframe; contains Distance results for all plans. Columns: \"PlanSetupSer\", \"Name\",\"Treatable q0\",\"Treatable q0_att\", \"Treatable q0_attLung\", \"Max Depth q0\", \"Max Depth q0_att\", \"Max Depth q0_attLung\", \"Min Depth q0\", \"Min Depth q0_att\", \"Min Depth q0_attLung\", \"Mean Depth q0 per Angle\", \"Mean Depth q0_att per Angle\",\"Mean Depth q0_attLung per Angle\"\n",
    "    \"\"\"\n",
    "    results = {\"PlanSetupSer\" : [], \"Name\" : [],\"Treatable q0\" : [],\"Treatable q0_att\" : [], \"Treatable q0_attLung\" : [], \"Max Depth q0\" : [], \"Max Depth q0_att\" : [], \"Max Depth q0_attLung\" : [], \"Min Depth q0\" : [], \"Min Depth q0_att\" : [], \"Min Depth q0_attLung\" : [], \"Mean Depth q0 per Angle\" : [], \"Mean Depth q0_att per Angle\" : [],\"Mean Depth q0_attLung per Angle\" : []}\n",
    "    try:\n",
    "        done=pd.read_pickle(f\"{folder}/Treatable.pickle\")\n",
    "    except:\n",
    "        pass\n",
    "    PSSs=glob.glob(f\"{folder}/[0-9]*\")\n",
    "    if interval:\n",
    "        PSSs=PSSs[interval[0]:interval[1]]\n",
    "    for j,f1 in enumerate(PSSs):\n",
    "        if 'done' in locals():\n",
    "            if np.any(int(f1.split('\\\\')[-1]) == done['PlanSetupSer']):\n",
    "                continue\n",
    "        print(j, f1)\n",
    "        df = pd.read_pickle(f\"{f1}/Dist.pickle\")\n",
    "        if plot:\n",
    "            TVs, mdptvbc, meptvbc, treatableq2CT,  treatableq2, maxMinDepth_q0_att, maxMinDepth_q0, dang2CT, dang2= plothmbatch(df,save=f\"{f1}/\")\n",
    "        else:\n",
    "            TVs, treatableq2CT,  treatableq2L,  treatableq2, maxMinDepth_q0_att, maxMinDepth_q0_attLung, maxMinDepth_q0,minMinDepth_q0_att,minMinDepth_q0_attLung,minMinDepth_q0, dang2CT, dang2L, dang2= planDetermineMinMaxDepths(df)\n",
    "        for i in range(len(TVs)):\n",
    "            try:\n",
    "                results['PlanSetupSer'].append(int(f1.split('\\\\')[-1]))\n",
    "            except:\n",
    "                results['PlanSetupSer'].append(f1.split('\\\\')[-1])\n",
    "            results[\"Name\"].append(TVs[i])\n",
    "            results[\"Treatable q0_att\"].append(treatableq2CT[i])  \n",
    "            results[\"Treatable q0_attLung\"].append(treatableq2L[i]) \n",
    "            results[\"Treatable q0\"].append(treatableq2[i])\n",
    "            results[\"Max Depth q0_att\"].append(maxMinDepth_q0_att[i])  \n",
    "            results[\"Max Depth q0_attLung\"].append(maxMinDepth_q0_attLung[i]) \n",
    "            results[\"Max Depth q0\"].append(maxMinDepth_q0[i])\n",
    "            results[\"Min Depth q0_att\"].append(minMinDepth_q0_att[i])  \n",
    "            results[\"Min Depth q0_attLung\"].append(minMinDepth_q0_attLung[i]) \n",
    "            results[\"Min Depth q0\"].append(minMinDepth_q0[i])\n",
    "            results[\"Mean Depth q0_att per Angle\"].append(dang2CT[i])\n",
    "            results[\"Mean Depth q0_attLung per Angle\"].append(dang2L[i])\n",
    "            results[\"Mean Depth q0 per Angle\"].append(dang2[i])\n",
    "    df=pd.DataFrame.from_dict(results)\n",
    "    if 'done' in locals():\n",
    "        df=pd.concat((done,df))\n",
    "    df.to_pickle(f\"{folder}/Treatable{interval if np.any(interval) else ''}.pickle\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TV Mesh/Shape Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh as tm\n",
    "from scipy.stats import skew\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "def chamfer_distance(x, y, metric='l2', direction='bi'):\n",
    "    \"\"\"Chamfer distance between two point clouds\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: numpy array [n_points_x, n_dims]\n",
    "        first point cloud\n",
    "    y: numpy array [n_points_y, n_dims]\n",
    "        second point cloud\n",
    "    metric: string or callable, default 'l2'\n",
    "        metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used.\n",
    "    direction: str\n",
    "        direction of Chamfer distance.\n",
    "            'y_to_x':  computes average minimal distance from every point in y to x\n",
    "            'x_to_y':  computes average minimal distance from every point in x to y\n",
    "            'bi': compute both\n",
    "    Returns\n",
    "    -------\n",
    "    chamfer_dist: float\n",
    "        computed bidirectional Chamfer distance:\n",
    "            sum_{x_i \\\\in x}{\\\\min_{y_j \\\\in y}{||x_i-y_j||**2}} + sum_{y_j \\\\in y}{\\\\min_{x_i \\\\in x}{||x_i-y_j||**2}}\n",
    "\n",
    "    https://gist.github.com/sergeyprokudin/c4bf4059230da8db8256e36524993367\n",
    "    \"\"\"\n",
    "    \n",
    "    if direction=='y_to_x':\n",
    "        x_nn = NearestNeighbors(n_neighbors=1, leaf_size=1, algorithm='kd_tree', metric=metric).fit(x)\n",
    "        min_y_to_x = x_nn.kneighbors(y)[0]\n",
    "        chamfer_dist = np.mean(min_y_to_x)\n",
    "    elif direction=='x_to_y':\n",
    "        y_nn = NearestNeighbors(n_neighbors=1, leaf_size=1, algorithm='kd_tree', metric=metric).fit(y)\n",
    "        min_x_to_y = y_nn.kneighbors(x)[0]\n",
    "        chamfer_dist = np.mean(min_x_to_y)\n",
    "    elif direction=='bi':\n",
    "        x_nn = NearestNeighbors(n_neighbors=1, leaf_size=1, algorithm='kd_tree', metric=metric).fit(x)\n",
    "        min_y_to_x = x_nn.kneighbors(y)[0]\n",
    "        y_nn = NearestNeighbors(n_neighbors=1, leaf_size=1, algorithm='kd_tree', metric=metric).fit(y)\n",
    "        min_x_to_y = y_nn.kneighbors(x)[0]\n",
    "        chamfer_dist = np.mean(min_y_to_x) + np.mean(min_x_to_y)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid direction type. Supported types: \\'y_x\\', \\'x_y\\', \\'bi\\'\")\n",
    "        \n",
    "    return chamfer_dist\n",
    "\n",
    "def genMeshfromVox(vox,spacing,slicespacing,debug):\n",
    "    mesh = tm.voxel.ops.matrix_to_marching_cubes(vox.transpose((1,0,2)))\n",
    "\n",
    "    #rescale binary to real dimensions of PTV\n",
    "    mesh.vertices=mesh.vertices*[spacing[0],spacing[1],slicespacing]#+[minxyz[0],minxyz[1],unz[0]]\n",
    "    mc=mesh.center_mass\n",
    "    mesh.vertices-=mc\n",
    "    \n",
    "    pca = PCA(3)\n",
    "    pca.fit(mesh.vertices)\n",
    "    if not debug: mesh.vertices = pca.transform(mesh.vertices)\n",
    "    \n",
    "    # compactness (Ratio of the volume of the object to the volume of a sphere with the same surface area. More compact shapes have values closer to 1.)\n",
    "    r2=np.sqrt(mesh.area/(4*np.pi))\n",
    "    cs=np.abs(mesh.volume)/(mesh.area*r2/3)  \n",
    "\n",
    "    #solidity\n",
    "    s=np.abs(mesh.volume/mesh.convex_hull.volume )\n",
    "\n",
    "    return  mesh, mc, pca, cs, s\n",
    "def genMesh(df,XYZ, PSS, debug=0,spacing=None):\n",
    "\n",
    "    maxxyz=np.max(XYZ,axis=0)\n",
    "    unz = np.unique(XYZ[:,2])\n",
    "\n",
    "    if not np.any(spacing):\n",
    "        oss = df[df[\"PlanSetupSer\"]==PSS].iloc[0]['CTOriginSpacingSize']\n",
    "        slicespacing=oss[1][2]\n",
    "        try:\n",
    "            minxyz,spacing,size=getoss(XYZ)\n",
    "            spacing/=2 #?? depends on the interpolation in to_grid\n",
    "            size*=2 #??\n",
    "        except:\n",
    "            minxyz=np.min(XYZ,axis=0)\n",
    "            spacing=np.abs(oss[1][:2])/2\n",
    "            size=np.roll(np.ceil(np.abs((np.max(XYZ[:,:2],0)-np.min(XYZ[:,:2],0)))/spacing[0]).astype(int),1)+np.array([4,4])\n",
    "    \n",
    "    else:\n",
    "        minxyz=np.min(XYZ,axis=0)\n",
    "        size=np.roll(np.ceil(np.abs((np.max(XYZ[:,:2],0)-np.min(XYZ[:,:2],0))/spacing[0])).astype(int),1)+np.array([4,4])\n",
    "        slicespacing=spacing[2]\n",
    "        spacing=spacing[:2]\n",
    "    nslices=np.ceil(np.abs((maxxyz[2]-minxyz[2])/slicespacing)+1).astype(int) \n",
    "    vox = np.full((size[0],size[1],nslices),False)\n",
    "    for _,z in enumerate(unz): \n",
    "        vox[:,:,((z-minxyz[2])/slicespacing).astype(int)]=binary_erosion(binary_fill_holes(binary_dilation(to_grid(XYZ,z,minxyz,spacing,size)))) #problem with double walls\n",
    "    \n",
    "    if debug: \n",
    "        print(f\"{maxxyz=},{minxyz=},{vox.shape}\")\n",
    "        fig = px.imshow(vox[:,:,j], color_continuous_scale='gray',title=z)\n",
    "        fig.show()\n",
    "        fig = px.imshow(to_grid(XYZ,z,minxyz,spacing,size), color_continuous_scale='gray',title=z)\n",
    "        fig.show()\n",
    "\n",
    "  \n",
    "    mesh, mc, pca, cs, s = genMeshfromVox(vox,spacing,slicespacing,debug)\n",
    "    return mesh, mc, vox, minxyz, maxxyz, spacing, slicespacing, size, pca, cs, s\n",
    "\n",
    "def compileTVShapes(dfmain,save=None,debug=False,interval=None):\n",
    "    \"\"\"\n",
    "    This is a wrapper for the genMesh function.\n",
    "\n",
    "    In:\n",
    "    - dfmain : Pandas Dataframe, PTV.pickle generated by batchAnalyzeStructuresCT_sheet\n",
    "    - save : Bool, save pcikle or only return dataframe\n",
    "    - debug : Bool, show some additional info\n",
    "    - interval : int array [Start, End], only use specified rows of dfmain\n",
    "    \n",
    "    Out:\n",
    "    - PCAMesh.pickle : Pandas dataframe, containing the columns:\n",
    "    'PlanSetupSer',\"Name\",'TV Type', 'CTOriginSpacingSize','PTVOriginSpacingSize', 'Center', 'max' ,'min', 'PC', 'PC_Var','PC_Skew', 'PC_min', 'PC_max','Size', 'Solidity', 'Compactness', \"Volume\"}\n",
    "    \n",
    "    \"\"\"\n",
    "    PSSs=dfmain['PlanSetupSer'].unique()\n",
    "    if not np.any(interval==None):\n",
    "        PSSs=PSSs[interval[0]:interval[1]]\n",
    "    if save: logging.basicConfig(filename=f'{save}/LogsPCA.log', encoding='utf-8', level=logging.WARNING)\n",
    "    results2 = {'PlanSetupSer':[],\"Name\" : [],'TV Type':[], 'CTOriginSpacingSize':[],'PTVOriginSpacingSize':[], 'Center' : [], 'max' : [],'min' : [], 'PC' : [], 'PC_Var' : [],'PC_Skew' : [], 'PC_min' : [], 'PC_max' : [],'Size' : [], 'Solidity' : [], 'Compactness' : [], \"Volume\" : []}\n",
    "    for j,PSS in enumerate(PSSs):\n",
    "        dfpss=dfmain[dfmain['PlanSetupSer']==PSS]\n",
    "        print(j, PSS)\n",
    "        for TVType in ['PTV','CTV','GTV']:\n",
    "            df=dfpss[dfpss[\"TV Type\"]==TVType]\n",
    "            if len(df['Name']) > 1:\n",
    "                cumvox=None\n",
    "                cumvox_minxyz=None\n",
    "                cumvox_maxxyz=None\n",
    "                cumvox_spacing=None\n",
    "            for i in range(len(df)):\n",
    "                try:\n",
    "                    XYZ=df['XYZ'].iloc[i]\n",
    "                    if debug: print(df['Name'].iloc[i])\n",
    "                    if i > 0:\n",
    "                        mesh, mc, vox, minxyz, maxxyz, spacing, slicespacing, size,pca,cs,s = genMesh(df,XYZ,df['PlanSetupSer'].iloc[i],debug,spacing=cumvox_spacing)\n",
    "                    else: \n",
    "                        mesh, mc, vox, minxyz, maxxyz, spacing, slicespacing, size,pca,cs,s = genMesh(df,XYZ,df['PlanSetupSer'].iloc[i],debug)\n",
    "                    if len(df['Name']) > 1:\n",
    "                        if not np.any(cumvox):\n",
    "                            cumvox=vox\n",
    "                            cumvox_minxyz=minxyz\n",
    "                            cumvox_maxxyz=maxxyz\n",
    "                            cumvox_spacing=np.array([spacing[0],spacing[1],slicespacing])\n",
    "                        else:\n",
    "                            cumvox_minxyz_temp=np.min([cumvox_minxyz,minxyz],axis=0)\n",
    "                            cumvox_maxxyz=np.max([cumvox_maxxyz,maxxyz],axis=0)\n",
    "                            cumvox_size = np.ceil(np.abs((cumvox_maxxyz-cumvox_minxyz_temp))/(cumvox_spacing*np.array([2,2,1]))).astype(int)*np.array([2,2,1])+np.array([4,4,2])\n",
    "                            cumvox_temp = np.full((cumvox_size[1],cumvox_size[0],cumvox_size[2]),False)\n",
    "                            start=coordinate_space(cumvox_minxyz,cumvox_minxyz_temp,cumvox_spacing,True)[[1,0,2]]\n",
    "                            cumvox_temp[start[0]:start[0]+cumvox.shape[0],start[1]:start[1]+cumvox.shape[1],start[2]:start[2]+cumvox.shape[2]]=cumvox\n",
    "                            start=coordinate_space(minxyz,cumvox_minxyz_temp,cumvox_spacing,True)[[1,0,2]]\n",
    "                            cumvox_temp[start[0]:start[0]+vox.shape[0],start[1]:start[1]+vox.shape[1],start[2]:start[2]+vox.shape[2]] = cumvox_temp[start[0]:start[0]+vox.shape[0],start[1]:start[1]+vox.shape[1],start[2]:start[2]+vox.shape[2]] | vox\n",
    "                            cumvox=cumvox_temp\n",
    "                            cumvox_minxyz=cumvox_minxyz_temp\n",
    "                        results2[\"TV Type\"].append(df['TV Type'].iloc[i])\n",
    "                    else:\n",
    "                        results2[\"TV Type\"].append(f\"{df['TV Type'].iloc[i]} Solo\")\n",
    "                    results2[\"Name\"].append(df['Name'].iloc[i])\n",
    "                    results2[\"PlanSetupSer\"].append(PSS)\n",
    "                    results2[\"CTOriginSpacingSize\"].append(df['CTOriginSpacingSize'].iloc[i])\n",
    "                    results2['PTVOriginSpacingSize'].append([minxyz,spacing,size])\n",
    "                    results2[\"Center\"].append(np.mean(XYZ,axis=0))\n",
    "                    results2[\"min\"].append(minxyz)\n",
    "                    results2[\"max\"].append(maxxyz)\n",
    "                    results2[\"PC\"].append(pca.components_)\n",
    "                    results2[\"PC_Var\"].append(pca.explained_variance_)\n",
    "                    results2[\"PC_Skew\"].append(skew(mesh.vertices))\n",
    "                    results2[\"PC_min\"].append(np.min(mesh.vertices,axis=0))\n",
    "                    results2[\"PC_max\"].append(np.max(mesh.vertices,axis=0))\n",
    "                    results2[\"Size\"].append(4*np.mean(np.abs(mesh.vertices),axis=0))\n",
    "                    results2[\"Compactness\"].append(cs)\n",
    "                    results2[\"Solidity\"].append(s)\n",
    "                    results2[\"Volume\"].append(np.sum(vox)*np.prod(spacing)*slicespacing/1000)\n",
    "                except BaseException:\n",
    "                    logging.exception(f\"PCA failed for PTV {df['Name'].iloc[i], PSS}.\")\n",
    "                    continue\n",
    "\n",
    "                if debug:\n",
    "                    data=[]\n",
    "                    XYZ-=np.mean([maxxyz,minxyz],0)\n",
    "                    mesh.vertices-=np.mean([ results2[\"PC_max\"][-1], results2[\"PC_min\"][-1]],0)\n",
    "                    color=np.zeros(XYZ.shape)\n",
    "                    color[::2]=1\n",
    "                    data.append(go.Scatter3d(x=XYZ[:,0],y=XYZ[:,1],z=XYZ[:,2],mode='markers',marker=dict(size=2,color=color,opacity=0.8),name=df['PTV'].iloc[i]))\n",
    "                    data.append(go.Scatter3d(x=mesh.vertices[:,0],y=mesh.vertices[:,1],z=mesh.vertices[:,2],mode='markers',marker=dict(size=2,opacity=0.8),name=f\"Vertices {df['PTV'].iloc[i]}\"))\n",
    "                    fig=go.Figure(data=data)\n",
    "                    fig.show()\n",
    "                    input()\n",
    "            \n",
    "            if len(df['Name']) > 1 and np.any(cumvox):\n",
    "                mesh, mc, pca, cs, s = genMeshfromVox(cumvox,cumvox_spacing[:2],cumvox_spacing[2],debug)\n",
    "                coords=np.transpose(np.nonzero(cumvox))\n",
    "                cumpts=real_space(coords,cumvox_minxyz,cumvox_spacing)\n",
    "                results2[\"Name\"].append(f\"{df['TV Type'].iloc[i]} All Python\")\n",
    "                results2[\"TV Type\"].append(f\"{df['TV Type'].iloc[i]} All\")\n",
    "                results2[\"PlanSetupSer\"].append(PSS)\n",
    "                results2[\"CTOriginSpacingSize\"].append(df['CTOriginSpacingSize'].iloc[i])\n",
    "                results2['PTVOriginSpacingSize'].append([cumvox_minxyz,cumvox_spacing,cumvox_size])\n",
    "                results2[\"Center\"].append(np.mean(cumpts,axis=0))\n",
    "                results2[\"min\"].append(cumvox_minxyz)\n",
    "                results2[\"max\"].append(cumvox_maxxyz)\n",
    "                results2[\"PC\"].append(pca.components_)\n",
    "                results2[\"PC_Var\"].append(pca.explained_variance_)\n",
    "                results2[\"PC_Skew\"].append(skew(mesh.vertices))\n",
    "                results2[\"PC_min\"].append(np.min(mesh.vertices,axis=0))\n",
    "                results2[\"PC_max\"].append(np.max(mesh.vertices,axis=0))\n",
    "                results2[\"Size\"].append(4*np.mean(np.abs(mesh.vertices),axis=0))\n",
    "                results2[\"Compactness\"].append(cs)\n",
    "                results2[\"Solidity\"].append(s)\n",
    "                results2[\"Volume\"].append(np.sum(cumvox)*np.prod(cumvox_spacing)/1000)\n",
    "    if not debug:\n",
    "        df = pd.DataFrame.from_dict(results2)\n",
    "        if save:\n",
    "            df.to_pickle(f\"{save}/PCAMesh{interval if interval else ''}.pickle\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzePlan(RP):\n",
    "    \"\"\"Function to read metrics from RT Plan DICOM files\"\"\"\n",
    "    try:\n",
    "        PMRN=int(RP.PatientID)\n",
    "    except:\n",
    "        PMRN=np.nan \n",
    "    try:\n",
    "        SUID=RP.ReferencedStructureSetSequence[0].ReferencedSOPInstanceUID\n",
    "    except:\n",
    "        SUID=''\n",
    "    try:\n",
    "        PlanLabel=RP.RTPlanLabel\n",
    "    except:\n",
    "        PlanLabel=''\n",
    "    angle=[]\n",
    "    mu=[]\n",
    "    beam=[]\n",
    "    iso=[]\n",
    "    Meterset=[]\n",
    "    Dose=[]\n",
    "    Roll=[]\n",
    "    Pitch=[]\n",
    "    Yaw=[]\n",
    "    TableTopEccentricAngle=[]\n",
    "    BeamLimitingDevicePositionSequence=[]\n",
    "    BeamDeliveryDurationLimit=[]\n",
    "    BeamType=[]\n",
    "    RadiationType=[]\n",
    "    HighDoseTechniqueType=[]\n",
    "    DoseRateSet=[]\n",
    "    BeamLimitingDeviceAngle=[]\n",
    "    SSD=[]\n",
    "    BeamDict = {}\n",
    "    # print(RP.BeamSequence)\n",
    "    # print(RP.FractionGroupSequence[0])\n",
    "    for rbs in RP.FractionGroupSequence[0].ReferencedBeamSequence:\n",
    "        try:\n",
    "            bms=float(rbs.BeamMeterset) \n",
    "        except:\n",
    "            bms=np.nan\n",
    "        try:\n",
    "            BDDL=float(rbs.BeamDeliveryDurationLimit)\n",
    "        except:\n",
    "            BDDL=np.nan\n",
    "        try:\n",
    "            BD=float(rbs.BeamDose) \n",
    "        except:\n",
    "            BD=np.nan\n",
    "        BeamDict[rbs.ReferencedBeamNumber]=[bms,BDDL,BD]\n",
    "    for bs in RP.BeamSequence:\n",
    "        if bs.TreatmentDeliveryType == 'TREATMENT' and bs.PrimaryDosimeterUnit == 'MU':\n",
    "            cmw_temp=0\n",
    "            angles=[]\n",
    "            BLDPSs=[]\n",
    "            MUs=[]\n",
    "            GA_temp=np.nan\n",
    "            BLDPS_temp=None\n",
    "            for cp in bs.ControlPointSequence:\n",
    "                try:\n",
    "                    mu1=BeamDict[bs.BeamNumber][0]*(-cmw_temp+cp.CumulativeMetersetWeight)#ReferencedDoseReferenceSequence[0].CumulativeDoseReferenceCoefficient\n",
    "                    cmw_temp=cp.CumulativeMetersetWeight\n",
    "                    try: #Needed for static treatments\n",
    "                        GA_temp=cp.GantryAngle\n",
    "                        BLDPS_temp=cp.BeamLimitingDevicePositionSequence\n",
    "                    except:\n",
    "                        pass\n",
    "                    if mu1>0:\n",
    "                        MUs.append(mu1)\n",
    "                        angles.append(GA_temp)\n",
    "                        BLDPSs.append(BLDPS_temp)\n",
    "                except:\n",
    "                    logging.exception(f\"MU dose failed for patient {RP.PatientID}\")\n",
    "                    continue\n",
    "                    # logging.exception(f\"MU angle failed for patient {RP.PatientID}\")\n",
    "            try:\n",
    "                iso.append(np.array(bs.ControlPointSequence[0].IsocenterPosition)) #CONTINUE: Try except nan\n",
    "            except:\n",
    "                iso.append(np.nan)\n",
    "            try:    \n",
    "                Roll.append(float(bs.ControlPointSequence[0].TableTopRollAngle))\n",
    "            except:\n",
    "                Roll.append(np.nan)\n",
    "            try:\n",
    "                Pitch.append(float(bs.ControlPointSequence[0].TableTopPitchAngle))\n",
    "            except:\n",
    "                Pitch.append(np.nan)\n",
    "            try:\n",
    "                Yaw.append(float(bs.ControlPointSequence[0].PatientSupportAngle))\n",
    "            except:\n",
    "                Yaw.append(np.nan)\n",
    "            try:\n",
    "                BeamType.append(bs.BeamType)\n",
    "            except:\n",
    "                BeamType.append(np.nan)\n",
    "            try:\n",
    "                RadiationType.append(bs.RadiationType)\n",
    "            except:\n",
    "                RadiationType.append(np.nan)\n",
    "            try:\n",
    "                HighDoseTechniqueType.append(bs[0x300a, 0x00c7].value)\n",
    "            except:\n",
    "                HighDoseTechniqueType.append(np.nan)\n",
    "            try:\n",
    "                DoseRateSet.append(float(bs.ControlPointSequence[0].DoseRateSet))\n",
    "            except:\n",
    "                DoseRateSet.append(np.nan)\n",
    "            try:\n",
    "                SSD.append(float(bs.ControlPointSequence[0][0x300a, 0x0130].value)/10)\n",
    "            except:\n",
    "                SSD.append(np.nan)\n",
    "            try:\n",
    "                BeamLimitingDeviceAngle.append(float(bs.ControlPointSequence[0].BeamLimitingDeviceAngle))\n",
    "            except:\n",
    "                BeamLimitingDeviceAngle.append(np.nan)\n",
    "            try:\n",
    "                TableTopEccentricAngle.append(float(bs.ControlPointSequence[0].TableTopEccentricAngle))\n",
    "            except:\n",
    "                TableTopEccentricAngle.append(np.nan)\n",
    "            beam.append(bs.BeamName)\n",
    "            Meterset.append(BeamDict[bs.BeamNumber][0])\n",
    "            Dose.append(BeamDict[bs.BeamNumber][2])\n",
    "            BeamDeliveryDurationLimit.append(BeamDict[bs.BeamNumber][1])\n",
    "            angle.append(angles)\n",
    "            mu.append(MUs)\n",
    "            BeamLimitingDevicePositionSequence.append(BLDPSs)\n",
    "            \n",
    "    return PMRN, SUID, PlanLabel, angle, mu, beam, Roll, Pitch, Yaw, TableTopEccentricAngle,BeamLimitingDevicePositionSequence,BeamDeliveryDurationLimit, iso, Meterset,Dose,BeamDeliveryDurationLimit,BeamLimitingDeviceAngle,SSD,DoseRateSet,HighDoseTechniqueType,RadiationType,BeamType\n",
    "\n",
    "\n",
    "def batchAnalyzePlan(save,locsheet=DICOMLocations,interval=None):\n",
    "    \"\"\"\n",
    "    Wrapper to read plans in batch mode.\n",
    "\n",
    "    In:\n",
    "    - save; str, save location\n",
    "    - locsheet; str, xlsx file with DICOM plan locations\n",
    "    - interval; int array [start, end], only read specified row from loc sheet\n",
    "\n",
    "    Out:\n",
    "    - MU.pickle ; Pandas dataframe, contains the plan information from DICOM plans. Columns: 'PMRN', 'StructureUID', 'Rx Name' ,'Beam Name' , 'Meterset [MU]', 'Gantry Angle', 'MU per Angle', 'Beam Delivery Duration Limit [s]','Table Top Roll Angle','Table Top Pitch Angle',\n",
    "                 'Patient Support Angle','Table Top Eccentric Angle','Beam Limiting Device Angle','SSD [cm]','Dose Rate Set [MU/min]',\n",
    "                   'Dose [Gy]','High Dose Technique Type','Radiation Type','Beam Type', 'Isocenter [mm]'\n",
    "    \"\"\"\n",
    "    resultsMU = {'PMRN' : [], 'StructureUID' : [], 'Rx Name' : [] ,'Beam Name' : [], 'Meterset [MU]' : [], 'Gantry Angle' : [], 'MU per Angle' : [], 'Beam Delivery Duration Limit [s]':[],'Table Top Roll Angle' :[],'Table Top Pitch Angle':[],\n",
    "                 'Patient Support Angle' : [],'Table Top Eccentric Angle':[],'Beam Limiting Device Angle' : [],'SSD [cm]' : [],'Dose Rate Set [MU/min]' : [],\n",
    "                   'Dose [Gy]':[],'High Dose Technique Type':[],'Radiation Type':[],'Beam Type':[], 'Isocenter [mm]':[]} #'Beam Limiting Device Position Sequence' : [],\n",
    "    if type(locsheet)==str:\n",
    "        locs=updateLocs(locsheet)\n",
    "    else:\n",
    "        locs=locsheet\n",
    "    if not np.any(interval==None):\n",
    "        locs=locs[interval[0]:interval[1]]\n",
    "    if \"PlanSetupSer\" in locs.keys():\n",
    "        resultsMU['PlanSetupSer'] = []\n",
    "        havePSS=True\n",
    "    else:\n",
    "        havePSS=False\n",
    "    for i,RPfileloc in enumerate(locs[\"Plan_Path\"]):\n",
    "        if i%100==0: print(i/len(locs))\n",
    "        if havePSS: PSS=locs[\"PlanSetupSer\"].iloc[i]\n",
    "        try:\n",
    "            RPfile=pydicom.read_file(RPfileloc)\n",
    "        except:\n",
    "            continue\n",
    "        if RPfile.SOPInstanceUID == locs[\"RTPlan_UID\"].iloc[i]:\n",
    "            PMRN, SUID, PlanLabel, angle, mu, beam, Roll, Pitch, Yaw, TableTopEccentricAngle,BeamLimitingDevicePositionSequence,BeamDeliveryDurationLimit, iso, Meterset,Dose,BeamDeliveryDurationLimit,BeamLimitingDeviceAngle,SSD,DoseRateSet,HighDoseTechniqueType,RadiationType,BeamType=analyzePlan(RPfile)\n",
    "            if len(angle)!=0: \n",
    "                if havePSS: resultsMU['PlanSetupSer'].append(PSS)\n",
    "                resultsMU['PMRN'].append(PMRN)\n",
    "                resultsMU['StructureUID'].append(SUID)\n",
    "                resultsMU['Rx Name'].append(PlanLabel)\n",
    "                resultsMU[\"Gantry Angle\"].append(angle)\n",
    "                resultsMU[\"MU per Angle\"].append(mu)\n",
    "                resultsMU[\"Beam Name\"].append(beam)\n",
    "                resultsMU[\"Table Top Roll Angle\"].append(Roll)\n",
    "                resultsMU[\"Table Top Pitch Angle\"].append(Pitch)\n",
    "                resultsMU[\"Patient Support Angle\"].append(Yaw)\n",
    "                resultsMU[\"Table Top Eccentric Angle\"].append(TableTopEccentricAngle)\n",
    "                #resultsMU[\"Beam Limiting Device Position Sequence\"].append(BeamLimitingDevicePositionSequence)\n",
    "                resultsMU[\"Beam Delivery Duration Limit [s]\"].append(BeamDeliveryDurationLimit)\n",
    "                resultsMU[\"Isocenter [mm]\"].append(iso)\n",
    "                resultsMU[\"Meterset [MU]\"].append(Meterset)\n",
    "                resultsMU[\"Dose [Gy]\"].append(Dose)\n",
    "                resultsMU[\"Beam Limiting Device Angle\"].append(BeamLimitingDeviceAngle)\n",
    "                resultsMU[\"SSD [cm]\"].append(SSD)\n",
    "                resultsMU[\"Dose Rate Set [MU/min]\"].append(DoseRateSet)\n",
    "                resultsMU[\"High Dose Technique Type\"].append(HighDoseTechniqueType)\n",
    "                resultsMU[\"Radiation Type\"].append(RadiationType)\n",
    "                resultsMU[\"Beam Type\"].append(BeamType)\n",
    "    dfMU = pd.DataFrame.from_dict(resultsMU)\n",
    "    if not os.path.exists(save):\n",
    "        os.makedirs(save)\n",
    "    if not np.any(interval==None):\n",
    "        dfMU.to_pickle(f\"{save}/MU{interval}.pickle\")\n",
    "    else:    \n",
    "        try:\n",
    "            shutil.copyfile(f\"{save}/MU.pickle\",f\"{save}/MU.pickle.old\")\n",
    "            MU=pd.read_pickle(f\"{save}/MU.pickle\")\n",
    "            dfMU=pd.concat([MU,dfMU])\n",
    "            dfMU.to_pickle(f\"{save}/MU.pickle\")\n",
    "        except:\n",
    "            if not os.path.exists(f\"{save}/MU.pickle\"):\n",
    "                dfMU.to_pickle(f\"{save}/MU.pickle\")\n",
    "            else:\n",
    "                logging.exception(f\"MU.pickle exists, but appending failed. Create PTVnew.\")\n",
    "                dfMU.to_pickle(f\"{save}/MUnew.pickle\")\n",
    "    return dfMU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signed_bwdist(im):\n",
    "    '''\n",
    "    Find perim and return masked image (signed/reversed)\n",
    "    '''    \n",
    "    im = -bwdist(bwperim(im))*np.logical_not(im) + bwdist(bwperim(im))*im\n",
    "    return im\n",
    "\n",
    "def bwdist(im):\n",
    "    '''\n",
    "    Find distance map of image\n",
    "    '''\n",
    "    dist_im = distance_transform_edt(1-im)\n",
    "    return dist_im\n",
    "\n",
    "def bwperim(im):\n",
    "    return 0 - (im ^ binary_dilation(im))\n",
    "\n",
    "def interp_shape(top, bottom, precision):\n",
    "    '''\n",
    "    Interpolate between two contours\n",
    "\n",
    "    Input: top \n",
    "            [X,Y] - Image of top contour (mask)\n",
    "           bottom\n",
    "            [X,Y] - Image of bottom contour (mask)\n",
    "           precision\n",
    "             float  - % between the images to interpolate \n",
    "                Ex: num=0.5 - Interpolate the middle image between top and bottom image\n",
    "    Output: out\n",
    "            [X,Y] - Interpolated image at num (%) between top and bottom\n",
    "\n",
    "    '''\n",
    "    if precision>2:\n",
    "        print(\"Error: Precision must be between 0 and 1 (float)\")\n",
    "\n",
    "    top = signed_bwdist(top)\n",
    "    bottom = signed_bwdist(bottom)\n",
    "\n",
    "    # row,cols definition\n",
    "    r, c = top.shape\n",
    "\n",
    "    # Reverse % indexing\n",
    "    precision = 1+precision\n",
    "\n",
    "    # rejoin top, bottom into a single array of shape (2, r, c)\n",
    "    top_and_bottom = np.stack((top, bottom))\n",
    "\n",
    "    # create ndgrids \n",
    "    points = (np.r_[0, 2], np.arange(r), np.arange(c))\n",
    "    xi = np.rollaxis(np.mgrid[:r, :c], 0, 3).reshape((r*c, 2))\n",
    "    xi = np.c_[np.full((r*c),precision), xi]\n",
    "\n",
    "    # Interpolate for new plane\n",
    "    out = interpn(points, top_and_bottom, xi)\n",
    "    out = out.reshape((r, c))\n",
    "\n",
    "    # Threshold distmap to values above 0\n",
    "    out = out > 0\n",
    "\n",
    "    return out\n",
    "\n",
    "def voxdose(struc,origin,spacing,slicepos,dose):\n",
    "    \"\"\"Interpolate between slices to fit other resolution\"\"\"\n",
    "    unz=np.unique(struc[:,2])\n",
    "    imgslices=dict()\n",
    "    for z in unz:\n",
    "        imgslices[np.round(z,3)]=binary_erosion(binary_fill_holes(binary_dilation(to_grid(struc,z,origin,spacing,[dose.shape[0],dose.shape[1]]))))\n",
    "    slicespcaingstruc=np.abs(np.median(unz-np.roll(unz,1))) \n",
    "    # print(slicepos,unz)\n",
    "    vox = np.full(dose.shape,False)\n",
    "    for i,z in enumerate(slicepos): \n",
    "        XYZint=struc\n",
    "        if not z in unz:\n",
    "            # find closest slices. Incorrect for disconnected structs, which will become connected. Introduce max distance betwen slices at 5mm?\n",
    "            pos=unz-z\n",
    "            if np.any(pos<0) and np.any(pos>0):\n",
    "                l=np.max(pos[pos<0])\n",
    "                u=np.min(pos[pos>0])\n",
    "                lu=-l+u\n",
    "                if lu<=slicespcaingstruc:\n",
    "                    # Run interpolation\n",
    "                    XYZint = interp_shape(imgslices[np.round(u+z,3)],imgslices[np.round(l+z,3)], np.abs(l)/lu)\n",
    "                    vox[:,:,i]=XYZint\n",
    "        else:\n",
    "            vox[:,:,i]=imgslices[np.round(z,3)]\n",
    "    return vox\n",
    "\n",
    "def tryexceptMinDist(pc,XYZOARi):\n",
    "    \"\"\"Reduce size of point clouds until they fit memory in a quick n dirty way\"\"\"\n",
    "    try:\n",
    "        result=np.min(np.linalg.norm(pc[:,np.newaxis]-XYZOARi,axis=2))\n",
    "    except np.core._exceptions._ArrayMemoryError:\n",
    "        result=tryexceptMinDist(pc[::2],XYZOARi[::2])\n",
    "    except:\n",
    "        result=np.nan\n",
    "    return result\n",
    "\n",
    "def analyzeDose(RDfile,XYZ,ri,RS, Names, TimmermanFrac,debug=True,calc=True,TimmermanDict=TimmermanDict):\n",
    "    \"\"\"Reads RT DICOM dose files in combination with structure files to calculate dose metrics related to Timmerman tables. Could probably be merged with depth code to save some run time.\n",
    "    In:\n",
    "    RDfile : DICOM Dose file\n",
    "    XYZ : list of PTV contours. Set to 'None' to load PTV names.\n",
    "    ri : list of ints ; Prescribed Target Doses\n",
    "    RS : RT DICOM Structure file\n",
    "    Names : list of str ; Names of PTVs. Set to 'None' to find PTVs\n",
    "    TimmermanFrac : int ; Number of Fractions in Timmerman List to compare to\n",
    "    debug : bool ; Prints some extra info\n",
    "    calc : bool ; Uses stored dose per voxel to recalculate metrics, if off only DVH are loaded\n",
    "    TimmermanDict : dict ; contains Timmermann dose constraints\n",
    "    \"\"\"\n",
    "    # Initialize result dicts\n",
    "    ctVri={}\n",
    "    cV={}\n",
    "    cmind={}\n",
    "    cmaxd={}\n",
    "    cmeand={}\n",
    "    ctotald={}\n",
    "    coverlap={}\n",
    "    coverlapborder={}\n",
    "    Vri=0\n",
    "    tVri={}\n",
    "    V={}\n",
    "    mind={}\n",
    "    maxd={}\n",
    "    meand={}\n",
    "    timvoldose={}\n",
    "    dvhdict={}\n",
    "    ROIdict={}\n",
    "    ROIdictOAR={}\n",
    "    ROIdictOARsearch={}\n",
    "    ROIdictorigname={}\n",
    "    NumberPTVs=len(XYZ)   \n",
    "    NamesN=[]     \n",
    "    Searchterms=['None' for i in Names] \n",
    "    SearchtermsN=[]\n",
    "    OrigNames=Names.copy()\n",
    "    OrigNamesN=[]\n",
    "    Names=['PTV' for i in Names]\n",
    "    #Store Structure names and match them to categories\n",
    "    f = open(\"StructureNames.txt\", \"a\")\n",
    "    for s in RS.StructureSetROISequence:\n",
    "        # print(s.ROIName)\n",
    "        cfname=s.ROIName.casefold()\n",
    "        found=False\n",
    "        if cfname[0] not in 'xz^' and 'ptv' not in cfname and 'ctv' not in cfname and 'gtv' not in cfname:  \n",
    "            f.write(cfname)   \n",
    "            f.write(\"\\n\")\n",
    "            for oar in TimmermanDict.keys():\n",
    "                if oar in cfname:\n",
    "                    found=oar\n",
    "                    break      \n",
    "        if cfname == 'body':\n",
    "            ROIdict[s.ROINumber] = 'body'\n",
    "            ROIdictOARsearch[s.ROINumber]= \"body\"\n",
    "            ROIdictorigname[s.ROINumber]='body'\n",
    "        elif found:\n",
    "            ROIdictOARsearch[s.ROINumber]=found\n",
    "            ROIdictOAR[s.ROINumber]=TimmermanDict[found]\n",
    "            ROIdictorigname[s.ROINumber]=cfname\n",
    "            if debug: print(cfname, found, TimmermanDict[found])\n",
    "        else:\n",
    "            if np.any(OrigNames==None):\n",
    "                if \"ptv\" in cfname[:4]: #in general want PTV and then blank,_,^,number\n",
    "                    if not \"avoid\" in cfname and not \"overlap\" in cfname:\n",
    "                        ROIdictorigname[s.ROINumber]=s.ROIName\n",
    "                        ROIdict[s.ROINumber] = 'PTV'\n",
    "            else:\n",
    "                if s.ROIName in OrigNames:\n",
    "                    ROIdictorigname[s.ROINumber]=s.ROIName\n",
    "                    ROIdict[s.ROINumber] = 'PTV'\n",
    "    f.close()\n",
    "    #Match structures to their contours\n",
    "    ctrs = RS.ROIContourSequence\n",
    "    OARs = []\n",
    "    TVs = []\n",
    "    for c in ctrs:\n",
    "        try:\n",
    "            _=c.ContourSequence\n",
    "        except:\n",
    "                continue\n",
    "        try:\n",
    "            Names.append(ROIdictOAR[c.ReferencedROINumber])\n",
    "            Searchterms.append(ROIdictOARsearch[c.ReferencedROINumber])\n",
    "            OrigNames.append(ROIdictorigname[c.ReferencedROINumber])\n",
    "            OARs.append(c)\n",
    "        except: \n",
    "            TV=ROIdict.get(c.ReferencedROINumber, False)\n",
    "            if TV:\n",
    "                TVs.append(TV)\n",
    "    if np.any(XYZ==None):\n",
    "        XYZ=[get_points([PTV],True) for PTV in TVs]\n",
    "    #Calculate min distance between PTV and OAR contours\n",
    "    if len(OARs)>0 and len(Names):\n",
    "        XYZOAR=[get_points([OAR],False)[0] for OAR in OARs]\n",
    "        MinDists=np.array([[tryexceptMinDist(pc,XYZOARi) for XYZOARi in XYZOAR] for pc in XYZ])\n",
    "        NamesPTVDists=np.repeat(OrigNames[:len(XYZ)],len(XYZOAR))\n",
    "        NamesOARDists=np.concatenate(np.repeat([OrigNames[len(XYZ):]],len(XYZ),axis=0))\n",
    "        XYZ.extend(XYZOAR)\n",
    "    else:\n",
    "        MinDists=np.array([])\n",
    "        NamesPTVDists=[]\n",
    "        NamesOARDists=[]\n",
    "    Centers=dict(zip(OrigNames,[np.mean(pc,axis=0) for pc in XYZ]))\n",
    "    Centers['body']=np.array([np.nan,np.nan,np.nan])\n",
    "    ROIdict = ROIdict | ROIdictOAR\n",
    "    ridict = dict(zip(OrigNames,ri)) | dict(zip(TimmermanFrac['OAR'],TimmermanFrac['Volume max [Gy]']*100))#dict(zip(OARNames,OARLimits))\n",
    "    ridict['body']=np.min(ri)\n",
    "    rivoldict = dict(zip(TimmermanFrac['OAR'],np.array(TimmermanFrac['Volume [cc]'],dtype=float)))\n",
    "    #Get Metrics from DVHs\n",
    "    for DVH in RDfile.DVHSequence:\n",
    "        try:\n",
    "            name=ROIdict[DVH.DVHReferencedROISequence[0].ReferencedROINumber]\n",
    "            searchterm=ROIdictOARsearch.get(DVH.DVHReferencedROISequence[0].ReferencedROINumber, \"None\")\n",
    "            origname=ROIdictorigname.get(DVH.DVHReferencedROISequence[0].ReferencedROINumber, \"None\")\n",
    "        except:\n",
    "            continue\n",
    "        NamesN.append(name)\n",
    "        SearchtermsN.append(searchterm)\n",
    "        OrigNamesN.append(origname)\n",
    "        DVHdose=np.cumsum(DVH.DVHData[::2])*100\n",
    "        DVHvols=np.array(DVH.DVHData[1::2])\n",
    "        try:\n",
    "            if name=='PTV':\n",
    "                rin=ridict[origname]\n",
    "            else:\n",
    "                rin=ridict[name]\n",
    "        except:\n",
    "            print(ri,name,\"Not found in Ridict\",Names)\n",
    "            rin=np.nan #ri[0]\n",
    "        if np.isnan(rin):\n",
    "            tVri[origname]=np.nan\n",
    "        else:\n",
    "            tVri[origname]=DVHvols[np.argmin(np.abs(DVHdose-rin))]\n",
    "        if name == 'body':\n",
    "            Vri=tVri[\"body\"]\n",
    "        V[origname]=float(DVHvols[0])\n",
    "        dif=(np.concatenate(([0],DVHvols))-np.roll(np.concatenate(([0],DVHvols)),len(DVHvols)))[1:]\n",
    "        mind[origname]=DVHdose[DVHvols==DVHvols[0]][-1]#float(DVH.DVHMinimumDose)*100\n",
    "        maxd[origname]=np.max(DVHdose)#float(DVH.DVHMaximumDose)*100 Sometimes completely off, whyever\n",
    "        meand[origname]=np.sum(DVHdose*dif)/np.sum(dif)#float(DVH.DVHMeanDose)*100\n",
    "        dvhdict[origname]=np.array(DVH.DVHData)\n",
    "        rivol=rivoldict.get(name,np.nan)\n",
    "        if np.isnan(rivol):\n",
    "            timvoldose[origname]=np.nan\n",
    "        elif rivol<0:\n",
    "            timvoldose[origname]=DVHdose[np.argmin(np.abs(DVHvols-DVHvols[np.argmin(np.abs(DVHdose-meand[origname]))]))]\n",
    "        else:\n",
    "            timvoldose[origname]=DVHdose[np.argmin(np.abs(DVHvols-rivol))]\n",
    "    for namei,origname in enumerate(OrigNamesN):\n",
    "        # print(name, name in NamesN)\n",
    "        if not origname in OrigNamesN:\n",
    "            NamesN.append(NamesN[namei])\n",
    "            SearchtermsN.append(Searchterms[namei])\n",
    "            OrigNamesN.append(origname)\n",
    "            V[origname]=np.nan\n",
    "            tVri[origname]=np.nan\n",
    "            maxd[origname]=np.nan\n",
    "            mind[origname]=np.nan\n",
    "            meand[origname]=np.nan\n",
    "            dvhdict[origname]=np.nan\n",
    "    try:\n",
    "        V['body']\n",
    "    except:\n",
    "        V['body']=np.nan\n",
    "        tVri['body']=np.nan\n",
    "        Vri=np.nan\n",
    "        maxd['body']=np.nan\n",
    "        mind['body']=np.nan\n",
    "        meand['body']=np.nan\n",
    "        dvhdict['body']=np.nan\n",
    "\n",
    "    if calc:\n",
    "        dose1=(RDfile.pixel_array*RDfile.DoseGridScaling).transpose(1,2,0)*100\n",
    "        zoomfactor=2\n",
    "        dose=zoom(dose1,zoomfactor,grid_mode=True,mode='grid-constant')\n",
    "        if debug:\n",
    "            print(dose1.shape)\n",
    "            fig1 = px.imshow(dose1[:,:,54])#, animation_frame=2, labels=dict(animation_frame=\"Slice\"))\n",
    "            fig1.show()\n",
    "            fig = px.imshow(dose[:,:,108])#[::3,::3,::3], animation_frame=2, labels=dict(animation_frame=\"Slice\"))\n",
    "            fig.show()\n",
    "            print(ri,np.max(dose))\n",
    "            print(dose.shape)\n",
    "            input()\n",
    "        origin=np.array(RDfile.ImagePositionPatient)\n",
    "        spacing=np.array(RDfile.PixelSpacing)/zoomfactor\n",
    "        slicepos=np.array(RDfile.GridFrameOffsetVector)\n",
    "        # print(slicepos)\n",
    "        # slicepos=zoom(slicepos,zoomfactor,grid_mode=True) #continue\n",
    "        slicespacing0=slicepos-np.roll(slicepos,1)\n",
    "        if np.any(slicespacing0[2:-2]!=slicespacing0[3]):\n",
    "            logging.exception(f\"Slicepos is not always evenly spaced. Change code{RDfile.ReferencedRTPlanSequence[0].ReferencedSOPInstanceUID}\")\n",
    "        slicespacing=np.abs(np.median(slicespacing0)) / zoomfactor\n",
    "        slicepos=np.cumsum(np.ones(dose.shape[2])*slicespacing)-slicespacing\n",
    "        # print(slicepos)\n",
    "        voxtovol=np.prod(spacing)*slicespacing/1000\n",
    "        cVri=np.sum(dose>np.min(ri))*voxtovol\n",
    "        if slicepos[0]==0:\n",
    "            slicepos+=origin[2]\n",
    "        vox_acc_dict={}\n",
    "        vals,counts=np.unique(Names,return_counts=True)\n",
    "        for val in vals[counts>1]:\n",
    "            vox_acc_dict[val]=np.full(dose.shape,False)\n",
    "        if not 'PTV' in vox_acc_dict.keys():\n",
    "            vox_acc_dict['PTV']=np.full(dose.shape,False)\n",
    "        minz=np.inf\n",
    "        maxz=-np.inf\n",
    "        #Recalculate metrics using voxel dose\n",
    "        for k,struc in enumerate(XYZ):\n",
    "            minz=np.min([minz,np.min(struc[:,2])])\n",
    "            maxz=np.max([maxz,np.max(struc[:,2])])\n",
    "            vox=voxdose(struc,origin,spacing,slicepos,dose)\n",
    "            cV[OrigNames[k]]=np.sum(vox)*voxtovol\n",
    "            tdose=dose[vox]\n",
    "            cmaxd[OrigNames[k]]=np.max(tdose)\n",
    "            cmind[OrigNames[k]]=np.min(tdose)\n",
    "            cmeand[OrigNames[k]]=np.mean(tdose)\n",
    "            ctotald[OrigNames[k]]=np.sum(tdose)\n",
    "            if Names[k]=='PTV':\n",
    "                coverlap[OrigNames[k]]=cV[OrigNames[k]]\n",
    "                coverlapborder[OrigNames[k]]=1\n",
    "                ctVri[OrigNames[k]]=np.sum((dose>ridict.get(OrigNames[k],np.nan)) & vox)*voxtovol\n",
    "            else:\n",
    "                coverlap[OrigNames[k]]=np.sum((vox & vox_acc_dict['PTV']))*voxtovol\n",
    "                inner=binary_dilation(vox_acc_dict['PTV'],iterations=1)\n",
    "                circ=binary_dilation(inner,iterations=1)^inner\n",
    "                intsec=circ & vox\n",
    "                coverlapborder[OrigNames[k]]= np.sum(intsec)/np.sum(circ)\n",
    "                ctVri[OrigNames[k]]=np.sum((dose>ridict.get(Names[k],np.nan)) & vox)*voxtovol\n",
    "            if Names[k] in vox_acc_dict.keys():\n",
    "                vox_acc_dict[Names[k]]=(vox | vox_acc_dict[Names[k]])\n",
    "        vox=np.full(dose.shape,False)\n",
    "        vox[:,:,np.max([0,int((minz-origin[2])/slicespacing)-10]):int((maxz-origin[2])/slicespacing)+10]=dose[:,:,np.max([0,int((minz-origin[2])/slicespacing)-10]):int((maxz-origin[2])/slicespacing)+10]>1e-5\n",
    "        vox=(vox ^ vox_acc_dict['PTV'])\n",
    "        ctVri['body']=np.sum(((dose>np.min(ri)) & vox))*voxtovol\n",
    "        cV['body']=np.sum(vox)*voxtovol\n",
    "        tdose=dose[vox]\n",
    "        cmaxd['body']=np.max(tdose)\n",
    "        cmind['body']=np.min(tdose)\n",
    "        cmeand['body']=np.mean(tdose)\n",
    "        ctotald['body']=np.sum(tdose)\n",
    "        coverlap['body']=0\n",
    "        coverlapborder['body']=1\n",
    "\n",
    "        #Merge structures of one type and calculate metrics\n",
    "        for key in vox_acc_dict.keys():\n",
    "            if key=='PTV' and NumberPTVs<2:\n",
    "                continue\n",
    "            NamesN.append(f'{key} All Python')\n",
    "            OrigNamesN.append(NamesN[-1])\n",
    "            SearchtermsN.append('None')\n",
    "            vox_acc=vox_acc_dict[key]\n",
    "            if key ==\"PTV\":\n",
    "                MinDists=np.concatenate((MinDists.flatten(),np.min(MinDists,axis=0)))\n",
    "                NamesPTVDists=np.concatenate((NamesPTVDists,np.repeat(NamesN[-1],len(Names)-NumberPTVs)))\n",
    "                NamesOARDists=np.concatenate((NamesOARDists,OrigNames[NumberPTVs:]))\n",
    "                rin=np.min(ri)\n",
    "            else:\n",
    "                rin=ridict.get(key,np.nan)\n",
    "            keyorignames=np.array(OrigNames)[np.array(Names)==key]\n",
    "            Centers[NamesN[-1]]=np.mean(np.array([Centers[n] for n in keyorignames]),axis=0)\n",
    "            ctVri[NamesN[-1]]=np.sum((dose*vox_acc)>rin)*voxtovol\n",
    "            cV[NamesN[-1]]=np.sum(vox_acc)*voxtovol\n",
    "            tdose=dose[vox_acc]\n",
    "            cmaxd[NamesN[-1]]=np.max(tdose)\n",
    "            cmind[NamesN[-1]]=np.min(tdose)\n",
    "            cmeand[NamesN[-1]]=np.mean(tdose)\n",
    "            ctotald[NamesN[-1]]=np.sum(tdose)\n",
    "            coverlap[NamesN[-1]]=np.sum((vox_acc & vox_acc_dict['PTV']))*voxtovol\n",
    "            inner=binary_dilation(vox_acc_dict['PTV'],iterations=1)\n",
    "            circ=binary_dilation(inner,iterations=1)^inner\n",
    "            intsec=circ & vox_acc\n",
    "            coverlapborder[NamesN[-1]]= np.sum(intsec)/np.sum(circ)\n",
    "            vtemp=np.array([V[n] for n in keyorignames])\n",
    "            V[NamesN[-1]]=np.sum(vtemp)\n",
    "            tVri[NamesN[-1]]=np.sum(np.array([tVri[n] for n in keyorignames]))\n",
    "            maxd[NamesN[-1]]=np.max(np.array([maxd[n] for n in keyorignames]))\n",
    "            mind[NamesN[-1]]=np.min(np.array([mind[n] for n in keyorignames]))\n",
    "            meand[NamesN[-1]]=np.sum(vtemp*np.array([meand[n] for n in keyorignames]))/V[NamesN[-1]]\n",
    "            dvhstemp=[dvhdict[n] for n in keyorignames]\n",
    "            dvhlengths=[len(dvh) for dvh in dvhstemp]\n",
    "            maxdvh=np.argmax(dvhlengths)\n",
    "            dvhtemp=np.copy(dvhstemp[maxdvh])\n",
    "            tosum=np.arange(len(dvhlengths))\n",
    "            tosum=tosum[tosum!=maxdvh]\n",
    "            for i in tosum:\n",
    "                dvhtemp[1:dvhlengths[i]:2]+=dvhstemp[i][1::2]\n",
    "            dvhdict[NamesN[-1]]=dvhtemp   \n",
    "            rivol=rivoldict.get(key,np.nan)\n",
    "            if np.isnan(rivol):\n",
    "                timvoldose[NamesN[-1]]=np.nan\n",
    "            else:\n",
    "                DVHdose=np.cumsum(dvhtemp[::2])*100\n",
    "                DVHvols=np.array(dvhtemp[1::2])\n",
    "                timvoldose[NamesN[-1]]=DVHdose[np.argmin(np.abs(DVHvols-rivol))]\n",
    "    else:\n",
    "        for name in OrigNamesN:\n",
    "            ctVri[name]=np.nan\n",
    "            cV[name]=np.nan\n",
    "            cmaxd[name]=np.nan\n",
    "            cmind[name]=np.nan\n",
    "            cmeand[name]=np.nan\n",
    "            ctotald[name]=np.nan\n",
    "            coverlap[name]=np.nan\n",
    "            coverlapborder[name]=np.nan\n",
    "        ctVri['body']=np.nan\n",
    "        cV['body']=np.nan\n",
    "        cmaxd['body']=np.nan\n",
    "        cmind['body']=np.nan\n",
    "        cmeand['body']=np.nan\n",
    "        ctotald['body']=np.nan\n",
    "        coverlap['body']=np.nan\n",
    "        coverlapborder['body']=np.nan\n",
    "        cVri=np.nan\n",
    "    \n",
    "    return NamesN,Vri,tVri,V,mind,maxd,meand,timvoldose,coverlapborder,cVri,ctVri,cV,cmind,cmaxd,cmeand,ctotald,coverlap,Centers,MinDists.flatten(),NamesPTVDists,NamesOARDists,SearchtermsN,OrigNamesN,dvhdict\n",
    "\n",
    "def batchAnalyzeDose(save,timmermansheet=TimmermanSheet,verifiersheet=PTVInfoSheet,locsheet=DICOMLocations,interval=None,debug=False,calc=True):\n",
    "    \"\"\"\n",
    "    Wrapper to read dose files in batch mode.\n",
    "\n",
    "    In:\n",
    "    - save; str, save location\n",
    "    - timmermansheet; str to Timmerman.csv is a CSV sheet containing simplified constraints from the Timmerman tables\n",
    "    - verifiersheet; str to PTVInfoSheet, xlsx file with plan infos\n",
    "    - locsheet; str, xlsx file with DICOM plan locations\n",
    "    - interval; int array [start, end], only read specified row from loc sheet\n",
    "    - debug; bool, show some extra info\n",
    "    - calc; bool, recalculate DVH statistics from dose image instead of only reading them from stored DICOM DVHs\n",
    "\n",
    "    Out:\n",
    "    - Dose.pickle ; Pandas dataframe, contains dose statistics from DICOM dose file. Columns: 'PlanSetupSer','Name', 'Search Term', 'Original Name', 'Timmerman Max Point Dose [cGy]', 'Timmerman Volume Max Dose [cGy]', 'Timmerman Volume Max [cc]', 'Number Fractions','Timmerman Matched Number Fractions', 'Center [mm]', 'Dose at Timmerman Volume Max [cGy]','Volume Reference Isodose [cc]', 'Target Volume Reference Isodose [cc]', 'Target Volume [cc]', 'Min Dose [cGy]', 'Max Dose [cGy]', 'Mean Dose [cGy]', 'Calc Volume Reference Isodose [cc]', 'Calc Target Volume Reference Isodose [cc]', 'Calc Target Volume [cc]', 'Calc Min Dose [cGy]', 'Calc Max Dose [cGy]', 'Calc Mean Dose [cGy]', 'Calc Total Dose [cGy]', 'Calc Overlap PTV [cc]', 'Calc Overlap with Dilated Shell of PTV All', 'DICOM DVH'\n",
    "    - DistsPTVOAR.pickle ; Pandas dataframe, contains distances between PTV and OARs. Columns: 'PlanSetupSer', 'PTV Name', 'OAR Name', 'Min Distance Between Contours [mm]'\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {'PlanSetupSer' : [],'Name' : [], 'Search Term':[], 'Original Name' : [], 'Timmerman Max Point Dose [cGy]':[], 'Timmerman Volume Max Dose [cGy]':[], 'Timmerman Volume Max [cc]':[], 'Number Fractions' :[],'Timmerman Matched Number Fractions':[], 'Center [mm]': [], 'Dose at Timmerman Volume Max [cGy]':[],'Volume Reference Isodose [cc]' : [], 'Target Volume Reference Isodose [cc]' : [], 'Target Volume [cc]' : [], 'Min Dose [cGy]' : [], 'Max Dose [cGy]' : [], 'Mean Dose [cGy]' : [], 'Calc Volume Reference Isodose [cc]' : [], 'Calc Target Volume Reference Isodose [cc]' : [], 'Calc Target Volume [cc]' : [], 'Calc Min Dose [cGy]' : [], 'Calc Max Dose [cGy]' : [], 'Calc Mean Dose [cGy]' : [], 'Calc Total Dose [cGy]' : [], 'Calc Overlap PTV [cc]' : [], 'Calc Overlap with Dilated Shell of PTV All' : [], 'DICOM DVH' : []}\n",
    "    distresults = {'PlanSetupSer' : [], 'PTV Name' : [], 'OAR Name' : [], 'Min Distance Between Contours [mm]' : []}\n",
    "    if interval:\n",
    "        locs=updateLocs(locsheet,debug)[interval[0]:interval[1]]\n",
    "    else:    \n",
    "        locs=updateLocs(locsheet,debug)\n",
    "    try:\n",
    "        structs=pd.read_pickle(f\"{save}/PTV{interval if interval else ''}.pickle\")\n",
    "    except:\n",
    "        structs=pd.read_pickle(f\"{save}/PTV.pickle\")\n",
    "    structs=structs[['PTV' in r for r in structs['TV Type']]]\n",
    "    ver=pd.read_excel(verifiersheet).rename(columns={'PTV name' : 'Name'})[[\"PlanSetupSer\",\"Name\", \"Prescribed Dose [cGy]\", \"Num Fractions\"]]\n",
    "    structs=pd.merge(ver,structs,on=['PlanSetupSer','Name'])\n",
    "    logging.basicConfig(filename=f'{save}/LogsDose.log', encoding='utf-8', level=logging.WARNING)\n",
    "    TimFracs=np.array([1,2,3,4,5,8,10,15,20,30])\n",
    "    Timmerman=pd.read_csv(timmermansheet)\n",
    "    for i,RDfileloc in enumerate(locs[\"Dose_Path\"]):\n",
    "        try:\n",
    "            PSS=locs[\"PlanSetupSer\"].iloc[i]\n",
    "            try:\n",
    "                with gzip.open(RDfileloc, 'rb') as f:\n",
    "                    RDfile = pydicom.dcmread(f)\n",
    "            except:\n",
    "                continue\n",
    "            if RDfile.ReferencedRTPlanSequence[0].ReferencedSOPInstanceUID == locs[\"RTPlan_UID\"].iloc[i] and RDfile.ReferencedStructureSetSequence[0].ReferencedSOPInstanceUID == locs[\"StructureSet_UID\"].iloc[i]:\n",
    "                Strucfile = pydicom.dcmread(locs['StructureSet_Path'].iloc[i])\n",
    "                targets=structs[structs['PlanSetupSer']==PSS]\n",
    "                if len(targets)==0:\n",
    "                    continue\n",
    "                targetfractions=int(targets['Num Fractions'].iloc[0])\n",
    "                matchedtimfractions=TimFracs[np.argmin(np.abs(TimFracs-targetfractions))]\n",
    "                TimmermanFrac=Timmerman[Timmerman['Number Fractions']==matchedtimfractions]\n",
    "                NamesN,Vri,tVri,V,mind,maxd,meand,timvoldose,coverlapborder,cVri,ctVri,cV,cmind,cmaxd,cmeand,ctotald,coverlap,Centers,MinDists,NamesPTVDists,NamesOARDists,SearchtermsN,OrigNamesN,dvhdict=analyzeDose(RDfile,targets['XYZ'].to_list(),targets['Prescribed Dose [cGy]'].to_list(),Strucfile,targets['Name'].tolist(),TimmermanFrac,debug,calc) \n",
    "                print(i)\n",
    "                for ji,j in enumerate(OrigNamesN):\n",
    "                    mask=[tf in NamesN[ji] for tf in TimmermanFrac['OAR']]\n",
    "                    if np.any(mask):\n",
    "                        tmpd=TimmermanFrac['Max point dose [Gy]'][mask].iloc[0]*100\n",
    "                        tvmd=TimmermanFrac['Volume max [Gy]'][mask].iloc[0]*100\n",
    "                        tvm=TimmermanFrac['Volume [cc]'][mask].iloc[0]\n",
    "                    else:\n",
    "                        tmpd=np.nan\n",
    "                        tvmd=np.nan\n",
    "                        tvm=np.nan\n",
    "                    results['PlanSetupSer'].append(PSS)\n",
    "                    results['Name'].append(NamesN[ji])\n",
    "                    results['Search Term'].append(SearchtermsN[ji])\n",
    "                    results['Original Name'].append(j)\n",
    "                    results['Timmerman Max Point Dose [cGy]'].append(tmpd)\n",
    "                    results['Timmerman Volume Max Dose [cGy]'].append(tvmd)\n",
    "                    results['Timmerman Volume Max [cc]'].append(tvm)\n",
    "                    results['Number Fractions'].append(targetfractions)\n",
    "                    results['Timmerman Matched Number Fractions'].append(matchedtimfractions)\n",
    "                    results['Center [mm]'].append(Centers[OrigNamesN[ji]])\n",
    "                    results['Dose at Timmerman Volume Max [cGy]'].append(timvoldose[j])\n",
    "                    results['Volume Reference Isodose [cc]'].append(Vri)\n",
    "                    results['Target Volume Reference Isodose [cc]'].append(tVri[j])\n",
    "                    results['Target Volume [cc]'].append(V[j])\n",
    "                    results['Min Dose [cGy]'].append(mind[j])\n",
    "                    results['Max Dose [cGy]'].append(maxd[j])\n",
    "                    results['Mean Dose [cGy]'].append(meand[j])\n",
    "                    results['Calc Volume Reference Isodose [cc]'].append(cVri)\n",
    "                    results['Calc Target Volume Reference Isodose [cc]'].append(ctVri[j])\n",
    "                    results['Calc Target Volume [cc]'].append(cV[j])\n",
    "                    results['Calc Min Dose [cGy]'].append(cmind[j])\n",
    "                    results['Calc Max Dose [cGy]'].append(cmaxd[j])\n",
    "                    results['Calc Mean Dose [cGy]'].append(cmeand[j])\n",
    "                    results['Calc Total Dose [cGy]'].append(ctotald[j])\n",
    "                    results['Calc Overlap PTV [cc]'].append(coverlap[j])\n",
    "                    results['Calc Overlap with Dilated Shell of PTV All'].append(coverlapborder[j])\n",
    "                    results['DICOM DVH'].append(dvhdict[j])\n",
    "                distresults[\"PlanSetupSer\"].extend(np.repeat(PSS,len(MinDists)))\n",
    "                distresults['PTV Name'].extend(NamesPTVDists)\n",
    "                distresults['OAR Name'].extend(NamesOARDists)\n",
    "                distresults['Min Distance Between Contours [mm]'].extend(MinDists)\n",
    "                if debug:\n",
    "                    for key in distresults.keys():\n",
    "                        print(key,len(distresults[key]))\n",
    "                    for key in results.keys():\n",
    "                        print(key,len(results[key]))\n",
    "        except KeyboardInterrupt:\n",
    "            print ('KeyboardInterrupt exception is caught')\n",
    "            raise\n",
    "        except:\n",
    "            print(f\"Failed {RDfileloc}\")\n",
    "            logging.exception(f\"Failed {RDfileloc}\")\n",
    "    df = pd.DataFrame.from_dict(results)\n",
    "    df=calcDoseMetrics(df)\n",
    "    dfd = pd.DataFrame.from_dict(distresults)\n",
    "    if interval:\n",
    "        df.to_pickle(f\"{save}/Dose{interval}.pickle\")\n",
    "        dfd.to_pickle(f\"{save}/DistsPTVOAR{interval}.pickle\")\n",
    "    else:    \n",
    "        try:\n",
    "            shutil.copyfile(f\"{save}/Dose.pickle\",f\"{save}/Dosepickle.old\")\n",
    "            Dose=pd.read_pickle(f\"{save}/Dose.pickle\")\n",
    "            df=pd.concat([Dose,df])\n",
    "            df.to_pickle(f\"{save}/Dose.pickle\")\n",
    "            shutil.copyfile(f\"{save}/DistsPTVOAR.pickle\",f\"{save}/DistsPTVOARpickle.old\")\n",
    "            Dists=pd.read_pickle(f\"{save}/DistsPTVOAR.pickle\")\n",
    "            dfd=pd.concat([Dists,dfd])\n",
    "            dfd.to_pickle(f\"{save}/DistsPTVOAR.pickle\")\n",
    "        except:\n",
    "            if not os.path.exists(f\"{save}/Dose.pickle\"):\n",
    "                df.to_pickle(f\"{save}/Dose.pickle\")\n",
    "                dfd.to_pickle(f\"{save}/DistsPTVOAR.pickle\")\n",
    "            else:\n",
    "                logging.exception(f\"Dose.pickle exists, but appending failed. Create Dosenew.\")\n",
    "                df.to_pickle(f\"{save}/Dosenew.pickle\")\n",
    "                dfd.to_pickle(f\"{save}/DistsPTVOARnew.pickle\")\n",
    "\n",
    "    return df,dfd\n",
    "\n",
    "#Calc Dose Metrics\n",
    "def calcDoseMetrics(dose):\n",
    "    verifier= pd.read_excel(PTVInfoSheet).rename(columns={'PTV name' : 'Name', 'PTV volume (cc)' :'Volume Planning System [cm]'})\n",
    "    temp=dose.loc[dose['Name']=='body'][['PlanSetupSer','Calc Target Volume Reference Isodose [cc]','Calc Total Dose [cGy]']]\n",
    "    temp.rename(columns={'Calc Target Volume Reference Isodose [cc]':'Calc body Volume Reference Isodose [cc]', 'Calc Total Dose [cGy]' : 'Calc body Total Dose [cGy]'},inplace=True)\n",
    "    # temp=dose.loc[dose['Name']=='body'][['PlanSetupSer','Calc Target Volume Reference Isodose [cc]']]#,'Calc Total Dose [cGy]']]\n",
    "    # temp.rename(columns={'Calc Target Volume Reference Isodose [cc]':'Calc body Volume Reference Isodose [cc]'},inplace=True)#, 'Calc Total Dose [cGy]' : 'Calc body Total Dose [cGy]'\n",
    "    dose=pd.merge(dose,temp,on=['PlanSetupSer'],how='left')\n",
    "    dose=pd.merge(dose,verifier[['PlanSetupSer','Prescribed Dose [cGy]','Name']],on=['PlanSetupSer','Name'],how='left')\n",
    "    dose['Conformity Index 1']=dose['Volume Reference Isodose [cc]']/dose['Target Volume [cc]']\n",
    "    dose['Calc Conformity Index 1']=(dose['Calc body Volume Reference Isodose [cc]']+dose['Calc Target Volume Reference Isodose [cc]'])/dose['Calc Target Volume [cc]']\n",
    "    dose['Conformity Index 2']=dose['Target Volume Reference Isodose [cc]']/dose['Volume Reference Isodose [cc]']\n",
    "    dose['Calc Conformity Index 2']=dose['Calc Target Volume Reference Isodose [cc]']/(dose['Calc body Volume Reference Isodose [cc]']+dose['Calc Target Volume Reference Isodose [cc]'])\n",
    "    dose['Conformity Index 3']=dose['Target Volume Reference Isodose [cc]']**2/dose['Target Volume [cc]']/dose['Volume Reference Isodose [cc]']\n",
    "    dose['Calc Conformity Index 3']=dose['Calc Target Volume Reference Isodose [cc]']**2/dose['Calc Target Volume [cc]']/(dose['Calc body Volume Reference Isodose [cc]']+dose['Calc Target Volume Reference Isodose [cc]'])\n",
    "    dose['Calc Conformity Index 4']=dose['Calc Total Dose [cGy]']/(dose['Calc Total Dose [cGy]']+dose['Calc body Total Dose [cGy]'])\n",
    "    dose['Homogeneity Index 1']=dose['Max Dose [cGy]']/dose['Prescribed Dose [cGy]']\n",
    "    dose['Homogeneity Index 2']=(dose['Target Volume [cc]']-dose['Target Volume Reference Isodose [cc]'])/dose['Target Volume [cc]']\n",
    "    dose['Calc Homogeneity Index 2']=(dose['Calc Target Volume [cc]']-dose['Calc Target Volume Reference Isodose [cc]'])/dose['Calc Target Volume [cc]']\n",
    "    dose['Max Dose/Timmerman Max Point Dose']=dose['Max Dose [cGy]']/dose['Timmerman Max Point Dose [cGy]']\n",
    "    dose['Calc Max Dose/Timmerman Max Point Dose']=dose['Calc Max Dose [cGy]']/dose['Timmerman Max Point Dose [cGy]']\n",
    "    dose['Target Volume Reference Isodose/Timmerman Volume Max']=dose['Target Volume Reference Isodose [cc]']/dose['Timmerman Volume Max [cc]']\n",
    "    dose['Calc Target Volume Reference Isodose/Timmerman Volume Max']=dose['Calc Target Volume Reference Isodose [cc]']/dose['Timmerman Volume Max [cc]']\n",
    "    dose['Dose at Timmerman Volume Max/Timmerman Volume Max Dose']=dose['Dose at Timmerman Volume Max [cGy]']/dose['Timmerman Volume Max Dose [cGy]']\n",
    "    return dose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch run, have to change batchAnalyzeDose by commenting out loading of this and passing them as parameters instead\n",
    "# structs=pd.read_pickle(f\"{SaveFolder}/PTV.pickle\") #{interval if interval else ''}\n",
    "# structs=structs[['PTV' in r for r in structs['TV Type']]]\n",
    "# ver=pd.read_excel(PTVInfoSheet).rename(columns={'PTV name' : 'Name'})[[\"PlanSetupSer\",\"Name\", \"Prescribed Dose [cGy]\", \"Num Fractions\"]]\n",
    "# structs=pd.merge(ver,structs,on=['PlanSetupSer','Name'])\n",
    "# bigint=[0,2000]\n",
    "# locs=updateLocs(locsheet=DICOMLocations,debug=False)[bigint[0]:bigint[1]]\n",
    "# done=np.loadtxt('//cifshd/homedir$/Evaluation/EllaOAR/DonePSS.txt')\n",
    "# with open(f\"{SaveFolder}/LogsDose.log\", 'r') as file:\n",
    "#     logs = file.read()\n",
    "# maskstr=[type(s)==str for s in np.array(locs['Dose_Path'])]\n",
    "# maskdone=[s in done for s in np.array(locs['PlanSetupSer'])]\n",
    "# maskerr=[s in logs for s in np.array(locs['Dose_Path'])[maskstr]]\n",
    "# locs['Dose_Path'].iloc[np.arange(len(locs))[maskstr][maskerr]]=np.nan\n",
    "# locs['Dose_Path'][maskdone]=np.nan\n",
    "# print(np.sum([type(s)==str for s in np.array(locs['Dose_Path'])]), len(locs), np.sum(maskstr), np.sum(maskerr), np.sum(maskdone))\n",
    "# structs=structs[[s in np.array(locs['PlanSetupSer']) for s in np.array(structs['PlanSetupSer'])]]\n",
    "# interval=np.array([bigint[0],bigint[0]+50],dtype=int)\n",
    "# while interval[1]<=bigint[1]:\n",
    "#     dfDose3,dfd=batchAnalyzeDose(SaveFolder,structs,locs[interval[0]-bigint[0]:interval[1]-bigint[0]],locsheet=DICOMLocations,debug=False,calc=True,interval=[interval[0],interval[1]])\n",
    "#     interval+=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge and Anonymize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeAllResults(PTV_sheet=PTVInfoSheet,CGTV_sheet=CGTVInfoSheet,save=SaveFolder):\n",
    "    \"\"\"\n",
    "    Merges all different pickle dataframes generated by the different functions into one big dataframe.\n",
    "\n",
    "    In:\n",
    "    - PTV_sheet; str to PTVInfoSheet Excel file\n",
    "    - CGTV_sheet; str to CGTVInfoSheet Excel File\n",
    "    - save; str to folder where all the pickles are stored\n",
    "    \n",
    "    Out:\n",
    "    - MergedResults.pickle; Pandas dataframe, contains all the final results of the analysis\n",
    "    \"\"\"\n",
    "    dfPCA_sheet = pd.read_pickle(f\"{save}/PCAMesh.pickle\")\n",
    "    if PTV_sheet:\n",
    "        dfPTV_sheet= pd.read_excel(PTV_sheet).rename(columns={'PTV name' : 'Name', 'PTV volume (cc)' :'Volume Planning System [cm]'})\n",
    "        dfPTV_sheet2=dfPTV_sheet[['PlanSetupSer','Rx site','Rx technique','Prescription']].groupby('PlanSetupSer').first()\n",
    "        dfmerged = pd.merge(dfPCA_sheet, dfPTV_sheet2, on=['PlanSetupSer'])\n",
    "        dfPTV_sheet3=dfPTV_sheet[['PlanSetupSer', 'Name', 'Mean dose (cGy)', 'Max dose (cGy)', 'Min dose (cGy)','Volume Planning System [cm]', 'Prescribed Dose [cGy]']]\n",
    "        dfCGTV_sheet= pd.read_excel(CGTV_sheet).rename(columns={'Target name' : 'Name', 'Target volume (cc)' : 'Volume Planning System [cm]'})\n",
    "        dfCGTV_sheet3=dfCGTV_sheet[['PlanSetupSer', 'Name', 'Mean dose (cGy)', 'Max dose (cGy)', 'Min dose (cGy)','Volume Planning System [cm]']]\n",
    "        dfdose=pd.concat([dfPTV_sheet3,dfCGTV_sheet3])\n",
    "        dfmerged= pd.merge(dfmerged, dfdose, on=['PlanSetupSer', 'Name'],how='left')\n",
    "    else:\n",
    "        dfmerged=dfPCA_sheet\n",
    "    \n",
    "    # sites,counts = np.unique(dfmerged['Rx site'],return_counts=True)\n",
    "    # for site in sites[counts==1]:\n",
    "    #     dfmerged=dfmerged.drop(dfmerged.loc[dfmerged['Rx site']==site])\n",
    "    dfmergedAll = dfmerged[['All' in tv for tv in dfmerged['TV Type']]]\n",
    "    dftreat_sheet = pd.read_pickle(f\"{save}/Treatable.pickle\")\n",
    "    dftreat_sheet.drop(['Treatable q0_attLung', 'Treatable q0','Treatable q0_att'],axis=1,inplace=True)\n",
    "    dfmerged4 = pd.merge(dftreat_sheet,dfmerged, on=['PlanSetupSer','Name'])\n",
    "    dfmergedSolo = dfmerged4.copy()\n",
    "    dfmerged4['TV Type']=[f\"{tv} All\" for tv in dfmerged4['TV Type']]\n",
    "    grouped=dfmerged4.groupby(['PlanSetupSer','TV Type'])\n",
    "    dang1=grouped['Mean Depth q0_att per Angle'].mean()\n",
    "    dang1.name='Mean Depth q0_att per Angle'\n",
    "    dfmergedAll = pd.merge(dang1,dfmergedAll, on=['PlanSetupSer','TV Type'])\n",
    "    dang3=grouped['Mean Depth q0 per Angle'].mean()\n",
    "    dang3.name='Mean Depth q0 per Angle'\n",
    "    dfmergedAll = pd.merge(dang3,dfmergedAll, on=['PlanSetupSer','TV Type'])\n",
    "    dang2=grouped['Mean Depth q0_attLung per Angle'].mean()\n",
    "    dang2.name='Mean Depth q0_attLung per Angle'\n",
    "    dfmergedAll = pd.merge(dang2,dfmergedAll, on=['PlanSetupSer','TV Type'])\n",
    "    d1=grouped['Max Depth q0_att'].max()\n",
    "    d1.name='Max Depth q0_att'\n",
    "    dfmergedAll = pd.merge(d1,dfmergedAll, on=['PlanSetupSer','TV Type'])\n",
    "    d3=grouped['Max Depth q0'].max()\n",
    "    d3.name='Max Depth q0'\n",
    "    dfmergedAll = pd.merge(d3,dfmergedAll, on=['PlanSetupSer','TV Type'])\n",
    "    d2=grouped['Max Depth q0_attLung'].max()\n",
    "    d2.name='Max Depth q0_attLung'\n",
    "    dfmergedAll = pd.merge(d2,dfmergedAll, on=['PlanSetupSer','TV Type'])\n",
    "    d1=grouped['Min Depth q0_att'].min()\n",
    "    d1.name='Min Depth q0_att'\n",
    "    dfmergedAll = pd.merge(d1,dfmergedAll, on=['PlanSetupSer','TV Type'])\n",
    "    d3=grouped['Min Depth q0'].min()\n",
    "    d3.name='Min Depth q0'\n",
    "    dfmergedAll = pd.merge(d3,dfmergedAll, on=['PlanSetupSer','TV Type'])\n",
    "    d2=grouped['Min Depth q0_attLung'].min()\n",
    "    d2.name='Min Depth q0_attLung'\n",
    "    dfmergedAll = pd.merge(d2,dfmergedAll, on=['PlanSetupSer','TV Type'])\n",
    "    # treatmeanlung=grouped['Treatable q0_attLung'].mean()\n",
    "    # treatmeanlung.name='Treatable q0_attLung'\n",
    "    # dfmergedAll = pd.merge(treatmeanlung,dfmergedAll, on=['PlanSetupSer','TV Type'])\n",
    "    # treatmean0=grouped['Treatable q0'].mean()\n",
    "    # treatmean0.name='Treatable q0'\n",
    "    # dfmergedAll = pd.merge(treatmean0,dfmergedAll, on=['PlanSetupSer','TV Type'])\n",
    "    # treatmean=grouped['Treatable q0_att'].mean()\n",
    "    # treatmean.name='Treatable q0_att'\n",
    "    # dfmergedAll = pd.merge(treatmean,dfmergedAll, on=['PlanSetupSer','TV Type'])\n",
    "    dfmerged9 = pd.concat([dfmergedAll,dfmergedSolo])\n",
    "    print(len(dfPCA_sheet),len(dfmerged),len(dftreat_sheet),len(dfmerged9))\n",
    "\n",
    "    dfMU_sheet = pd.read_pickle(f\"{save}/MU.pickle\")\n",
    "    # dfMU_sheet = dfMU_sheet.drop('Beam Limiting Device Position Sequence',axis=1)\n",
    "    dfMU_sheet=pd.merge(dfmerged9[[(('All' in tv or 'Solo' in tv) and 'PTV' in tv) for tv in dfmerged9['TV Type']]][['PlanSetupSer','Name']],dfMU_sheet,on=['PlanSetupSer'],how='left')\n",
    "    if PTV_sheet:\n",
    "        dfPTV_sheet4=dfPTV_sheet[['PlanSetupSer','Num Fractions', 'Dose/Frc (cGy)']].groupby('PlanSetupSer').first()\n",
    "        dfMU_sheet = pd.merge(dfMU_sheet,dfPTV_sheet4,on=['PlanSetupSer'])\n",
    "    dfmerged9 = pd.merge(dfmerged9,dfMU_sheet,on=['PlanSetupSer','Name'],how='left')\n",
    "    print(len(dfmerged9))\n",
    "\n",
    "    \n",
    "    dose = pd.read_pickle(f\"{save}/Dose.pickle\")\n",
    "    dose.rename(columns={'Name':'New Name', 'Original Name' : 'Name'},inplace=True)\n",
    "    dose.drop('Prescribed Dose [cGy]',axis=1,inplace=True)\n",
    "    dfmerged9=pd.merge(dfmerged9,dose,on=['PlanSetupSer','Name'],how='outer')\n",
    "   \n",
    "    \n",
    "    \n",
    "    dfHU = pd.read_pickle(f\"{save}/HU.pickle\") #Fix: Lung TV Type column clashes with other TV Type column. Lung HU different entry from Lung Timmerman Dose\n",
    "    dfHU.drop('TV Type',axis=1,inplace=True)\n",
    "    dfmerged9=pd.merge(dfHU,dfmerged9,on=['PlanSetupSer','Name'],how='outer')\n",
    "    print(dfmerged9.columns)\n",
    "    a=np.array(['Mets' if 'met' in str(p).casefold() else '' for p in dfmerged9['Prescription']],dtype=str)\n",
    "    b=np.array(['WBRT' if 'wbrt' in str(p).casefold() else '' for p in dfmerged9['Prescription']],dtype=str)\n",
    "    c=np.array(['Eye' if 'eye' in str(p).casefold() else '' for p in dfmerged9['Prescription']],dtype=str)\n",
    "    d=np.array(['Prostate' if 'prostate' in str(p).casefold() and dfmerged9['Volume'].iloc[i]<400 and 'PTV Solo' in dfmerged9['TV Type'].iloc[i] else '' for i,p in enumerate(dfmerged9['Prescription'])],dtype=str)\n",
    "    dfmerged9['Specific Site Info']=[a[i]+b[i]+c[i]+d[i] for i in range(len(a))]\n",
    "    \n",
    "    # dfmerged9.to_csv(f\"{save}/Merged.csv\", index=False)\n",
    "    dfmerged9.to_pickle(f\"{save}/MergedResults.pickle\")\n",
    "    return dfmerged9#, dfanon\n",
    "\n",
    "def anonymizeResults(dfmerged9,save=SaveFolder,savename='DataAnonymized',loaddict=None):\n",
    "    \"\"\"\n",
    "    Remove and rename columns to fit wanted data. Code plan ID and structure name using a dictionary to make it anonymous.\n",
    "    If you already have a dictionary of anonymized indices, use loaddict to load it and add to it.\n",
    "\n",
    "    In:\n",
    "    - dfmerged9; Pandas dataframe generated by mergeAllResults, stored as MergedResults.pickle\n",
    "    - save; str to folder where all the pickles are stored\n",
    "    - savename; str, Name of the anonymized pickle file\n",
    "    - loaddict; str, location of anonymization dictionary. If none, a new dictionary will be created.\n",
    "\n",
    "    Out:\n",
    "    - savename.pickle; Pandas dataframe, anonymized results\n",
    "    - loaddict; Json dictionary with the code to the anonymized indices\n",
    "    \"\"\"\n",
    "    dfanon=dfmerged9.copy()\n",
    "    if loaddict:\n",
    "        with open(loaddict, 'r') as fp:\n",
    "            scramblePSSf = json.load(fp)\n",
    "        scramblePSS = {v: k for k, v in scramblePSSf.items()}\n",
    "        scramblePSSonly = {v.split(',')[0]: k.split(',')[0] for k, v in scramblePSSf.items()}\n",
    "        newpsss=[]\n",
    "        newname=[]\n",
    "        if \"OAR Name\" in dfanon.columns:\n",
    "            newOARname=[]\n",
    "            dfanon.rename(columns={'PTV Name' : 'Name'},inplace=True)\n",
    "        for i,psss in enumerate(dfanon['PlanSetupSer']):\n",
    "            name=dfanon['Name'].iloc[i]\n",
    "            temp=scramblePSS.get(f\"{int(psss)},{name}\",False)\n",
    "            if temp:\n",
    "                p,s=temp.split(',')\n",
    "                newpsss.append(p)\n",
    "                newname.append(s)\n",
    "            else:\n",
    "                temp=scramblePSSonly.get(str(int(psss)),False)\n",
    "                if temp:\n",
    "                    newpsss.append(temp)\n",
    "                else:\n",
    "                    npss=np.random.randint(1e9)\n",
    "                    while str(int(npss)) in scramblePSSonly.keys():\n",
    "                        npss=np.random.randint(1e9)\n",
    "                    newpsss.append(npss)\n",
    "                nn=np.random.randint(1e5)\n",
    "                # print(f\"{newpsss[-1]},{int(nn)}\")\n",
    "                while f\"{newpsss[-1]},{int(nn)}\" in scramblePSS.values():\n",
    "                    print( f\"{newpsss[-1]},{int(nn)}\")\n",
    "                    nn=np.random.randint(1e5)\n",
    "                newname.append(nn)\n",
    "                scramblePSSonly[str(int(psss))]=str(newpsss[-1])\n",
    "                scramblePSS[f\"{int(psss)},{name}\"]=f\"{newpsss[-1]},{newname[-1]}\"\n",
    "            if \"OAR Name\" in dfanon.columns:\n",
    "                name=dfanon['OAR Name'].iloc[i]\n",
    "                temp=scramblePSS.get(f\"{int(psss)},{name}\",False)\n",
    "                if temp:\n",
    "                    p,s=temp.split(',')\n",
    "                    newOARname.append(s)\n",
    "                else:\n",
    "                    nn=np.random.randint(1e3)\n",
    "                    while f\"{int(nn)},{name}\" in scramblePSS.keys():\n",
    "                        nn=np.random.randint(1e3)\n",
    "                    newOARname.append(nn)\n",
    "                    scramblePSS[f\"{int(psss)},{name}\"]=f\"{newpsss[-1]},{newname[-1]}\"\n",
    "        dfanon['PlanSetupSer']=newpsss\n",
    "        dfanon['Name']=newname\n",
    "        if \"OAR Name\" in dfanon.columns:\n",
    "            dfanon['OAR Name']=newOARname\n",
    "            dfanon.rename(columns={'Name' : 'PTV Name'},inplace=True)\n",
    "        dfanon=dfanon[dfanon['PlanSetupSer']!=-1]\n",
    "    else:\n",
    "        if save:\n",
    "            loaddict=f\"{save}/AnonymizationDictPlan,Struc.json\"\n",
    "        PSSunique=dfmerged9['PlanSetupSer'].unique()\n",
    "        newPSS=np.arange(len(PSSunique))\n",
    "        np.random.shuffle(newPSS)\n",
    "        scramblePSS=dict(zip(PSSunique,newPSS))\n",
    "        newName=np.arange(len(dfanon['Name']))\n",
    "        np.random.shuffle(newName)\n",
    "        newPSSS=np.array([scramblePSS[int(psss)] for psss in dfanon['PlanSetupSer']])\n",
    "        scramblePSS=dict(zip((newPSSS.astype(str)+np.repeat(',',len(newPSSS))+newName.astype(str)),np.sum(np.array([dfanon['PlanSetupSer'].astype(int).astype(str), np.repeat(',',len(dfanon)),dfanon['Name']]).T,axis=1)))  \n",
    "        dfanon['PlanSetupSer']=newPSSS\n",
    "        dfanon['Name']=newName \n",
    "    if 'Prescription' in dfanon.columns:\n",
    "        dfanon.drop(['Original Name','PMRN','Rx Name','StructureUID','Prescription', \"In Lung\", 'In Left Lung', 'In Right Lung','Mean Depth q0_attLung per Angle', 'Mean Depth q0 per Angle', 'Mean Depth q0_att per Angle', 'CTOriginSpacingSize', 'PTVOriginSpacingSize'],axis=1,inplace=True,errors='ignore')\n",
    "        dfanon.rename(columns={'PC_min':'PC_Min [mm]','PC_max':'PC_Max [mm]','PC_Center':'PC_Center [mm]','min':'Min [mm]','max':'Max [mm]','Center':'Center [mm]','Size':'Size [mm]', 'Volume' : 'Volume [cc]', 'Min Depth q0': 'Min Depth q0 [cm]', 'Min Depth q0_attLung': 'Min Depth q0_attLung [cm]', 'Min Depth q0_att': 'Min Depth q0_att [cm]', 'Max Depth q0': 'Max Depth q0 [cm]', 'Max Depth q0_attLung': 'Max Depth q0_attLung [cm]', 'Max Depth q0_att': 'Max Depth q0_att [cm]'}, inplace=True)\n",
    "    dfanon.rename(columns={'PlanSetupSer': 'Plan Index (Anonymized)'}, inplace=True)\n",
    "    dfanon['Plan Index (Anonymized)'] =pd.to_numeric(dfanon['Plan Index (Anonymized)'])\n",
    "    if \"OAR Name\" not in dfanon.columns:   \n",
    "        dfanon.rename(columns={'Name': 'Structure Index (Anonymized)'}, inplace=True)\n",
    "        dfanon.set_index('Structure Index (Anonymized)',drop=False,inplace=True)\n",
    "        temp=dfanon.pop('Structure Index (Anonymized)')\n",
    "        dfanon.insert(1,'Structure Index (Anonymized)',temp)\n",
    "        dfanon['Structure Index (Anonymized)'] =pd.to_numeric(dfanon['Structure Index (Anonymized)'])\n",
    "    else:\n",
    "        dfanon.rename(columns={'PTV Name': 'PTV Structure Index (Anonymized)','OAR Name': 'OAR Structure Index (Anonymized)'}, inplace=True)\n",
    "        dfanon['PTV Structure Index (Anonymized)'] =pd.to_numeric(dfanon['PTV Structure Index (Anonymized)'])\n",
    "        dfanon['OAR Structure Index (Anonymized)'] =pd.to_numeric(dfanon['OAR Structure Index (Anonymized)'])\n",
    "\n",
    "        \n",
    "    if save:\n",
    "        if not os.path.exists(save):\n",
    "            os.makedirs(save)\n",
    "        dfanon.to_csv(f\"{save}/{savename}.csv\", index=False)\n",
    "        dfanon.to_pickle(f\"{save}/{savename}.pickle\")\n",
    "        with open(loaddict, 'w') as fp:\n",
    "            json.dump({v: k for k, v in scramblePSS.items()}, fp, sort_keys=True, indent=4)\n",
    "    return dfanon\n",
    "\n",
    "\n",
    "def verInfo(identifiers,dictLocation,PTV_sheet=PTVInfoSheet):\n",
    "    \"\"\"Get original indices for anonymized plan identifiers\"\"\"\n",
    "    verifier= pd.read_excel(PTV_sheet)\n",
    "    with open(dictLocation, 'r') as fp:\n",
    "        scramblePSSf = json.load(fp)\n",
    "    scramblePSSonly = {v.split(',')[0]: k.split(',')[0] for v, k in scramblePSSf.items()}\n",
    "    idPSS=np.array([scramblePSSonly.get(str(id)) for id in identifiers],dtype=int)\n",
    "    sortorder=np.argsort(np.argsort(idPSS))\n",
    "    found=verifier[[vID in idPSS for vID in verifier['PlanSetupSer']]]\n",
    "    first=np.unique(found['PlanSetupSer'], return_index=True)[1]\n",
    "    found=found.iloc[first]\n",
    "    found=found.iloc[sortorder]\n",
    "    found.to_csv(f\"{PTVInfoSheet}Searched.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis/Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['rgb(204,81,81)',\n",
    " 'rgb(127,51,51)',\n",
    " 'rgb(81,204,204)',\n",
    " 'rgb(51,127,127)',\n",
    " 'rgb(142,204,81)',\n",
    " 'rgb(89,127,51)',\n",
    " 'rgb(142,81,204)',\n",
    " 'rgb(89,51,127)',\n",
    " 'rgb(204,173,81)',\n",
    " 'rgb(127,108,51)',\n",
    " 'rgb(81,204,112)',\n",
    " 'rgb(51,127,70)',\n",
    " 'rgb(81,112,204)',\n",
    " 'rgb(51,70,127)',\n",
    " 'rgb(204,81,173)',\n",
    " 'rgb(127,51,108)',\n",
    " 'rgb(204,127,81)',\n",
    " 'rgb(127,79,51)',\n",
    " 'rgb(188,204,81)',\n",
    " 'rgb(117,127,51)',\n",
    " 'rgb(96,204,81)',\n",
    " 'rgb(60,127,51)',\n",
    " 'rgb(81,204,158)',\n",
    " 'rgb(51,127,98)',\n",
    " 'rgb(81,158,204)',\n",
    " 'rgb(51,98,127)',\n",
    " 'rgb(96,81,204)',\n",
    " 'rgb(60,51,127)',\n",
    " 'rgb(188,81,204)',\n",
    " 'rgb(117,51,127)',\n",
    " 'rgb(204,81,127)',\n",
    " 'rgb(127,51,79)',\n",
    " 'rgb(204,104,81)',\n",
    " 'rgb(127,65,51)',\n",
    " 'rgb(204,150,81)',\n",
    " 'rgb(127,94,51)',\n",
    " 'rgb(204,196,81)',\n",
    " 'rgb(127,122,51)',\n",
    " 'rgb(165,204,81)',\n",
    " 'rgb(103,127,51)',\n",
    " 'rgb(119,204,81)',\n",
    " 'rgb(74,127,51)',\n",
    " 'rgb(81,204,89)',\n",
    " 'rgb(51,127,55)',\n",
    " 'rgb(81,204,135)',\n",
    " 'rgb(51,127,84)',\n",
    " 'rgb(81,204,181)',\n",
    " 'rgb(51,127,113)',\n",
    " 'rgb(81,181,204)',\n",
    " 'rgb(51,113,127)',\n",
    " 'rgb(81,135,204)',\n",
    " 'rgb(51,84,127)',\n",
    " 'rgb(81,89,204)',\n",
    " 'rgb(51,55,127)',\n",
    " 'rgb(119,81,204)',\n",
    " 'rgb(74,51,127)',\n",
    " 'rgb(165,81,204)',\n",
    " 'rgb(103,51,127)',\n",
    " 'rgb(204,81,196)',\n",
    " 'rgb(127,51,122)',\n",
    " 'rgb(204,81,150)',\n",
    " 'rgb(127,51,94)',\n",
    " 'rgb(204,81,104)',\n",
    " 'rgb(127,51,65)',\n",
    " 'rgb(204,93,81)',\n",
    " 'rgb(127,58,51)',\n",
    " 'rgb(204,116,81)',\n",
    " 'rgb(127,72,51)',\n",
    " 'rgb(204,138,81)',\n",
    " 'rgb(127,86,51)',\n",
    " 'rgb(204,161,81)',\n",
    " 'rgb(127,101,51)',\n",
    " 'rgb(204,184,81)',\n",
    " 'rgb(127,115,51)',\n",
    " 'rgb(200,204,81)',\n",
    " 'rgb(125,127,51)',\n",
    " 'rgb(177,204,81)',\n",
    " 'rgb(110,127,51)',\n",
    " 'rgb(154,204,81)',\n",
    " 'rgb(96,127,51)',\n",
    " 'rgb(131,204,81)',\n",
    " 'rgb(82,127,51)',\n",
    " 'rgb(108,204,81)',\n",
    " 'rgb(67,127,51)',\n",
    " 'rgb(85,204,81)',\n",
    " 'rgb(53,127,51)',\n",
    " 'rgb(81,204,100)',\n",
    " 'rgb(51,127,62)',\n",
    " 'rgb(81,204,123)',\n",
    " 'rgb(51,127,77)',\n",
    " 'rgb(81,204,146)',\n",
    " 'rgb(51,127,91)',\n",
    " 'rgb(81,204,169)',\n",
    " 'rgb(51,127,105)',\n",
    " 'rgb(81,204,192)',\n",
    " 'rgb(51,127,120)',\n",
    " 'rgb(81,192,204)',\n",
    " 'rgb(51,120,127)',\n",
    " 'rgb(81,169,204)',\n",
    " 'rgb(51,105,127)']\n",
    "\n",
    "def cartesian_to_spherical(x,y,z):\n",
    "    theta = np.arctan2(np.sqrt(x ** 2 + y ** 2), z)\n",
    "    phi = np.arctan2(y, x) #if x >= 0 else np.arctan2(y, x) + np.pi\n",
    "    return theta, phi\n",
    "\n",
    "def plotPC(dfpca,save=None,show=True,pP=True,TVType='PTV',site=''):\n",
    "    PC=np.concatenate(dfpca['PC'].to_numpy()).reshape(-1,3,3)\n",
    "    if len(PC)<1000:\n",
    "        size=4\n",
    "    else:\n",
    "        size=2\n",
    "    if save:\n",
    "        folder = f\"{save}\"\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "    f11=go.Scatter3d(x=PC[:,0,0],y=PC[:,0,1],z=PC[:,0,2],mode='markers',name='Primary',marker=dict(size=size,color='blue'))\n",
    "    f12=go.Scatter3d(x=PC[:,1,0],y=PC[:,1,1],z=PC[:,1,2],mode='markers',name='Seconary',marker=dict(size=size,color='red'))\n",
    "    f13=go.Scatter3d(x=PC[:,2,0],y=PC[:,2,1],z=PC[:,2,2],mode='markers',name='Tertiary',marker=dict(size=size,color='green'))\n",
    "    traces1=[f11,f12,f13]\n",
    "    for i,t in enumerate(traces1):\n",
    "        fig1=go.Figure(t)\n",
    "        title1i=f\"Directions Principal Component {i+1} in 3D <br>for {TVType}{' Plan' if pP else ''}s {site}\"\n",
    "        fig1.update_layout(      title=title1i,\n",
    "                                    margin=dict(r=0, b=0, l=0, t=60),\n",
    "                                    height=600,\n",
    "                                    width=600,\n",
    "                                    scene_camera=dict(eye=dict(x=1.5, y=1.5, z=1.5))\n",
    "                                    )\n",
    "        if show: fig1.show()\n",
    "        if save: \n",
    "            fig1.write_image(f\"{folder}/{title1i.replace('<br>','')}.pdf\")\n",
    "            fig1.write_image(f\"{folder}/{title1i.replace('<br>','')}.png\")\n",
    "    fig1=go.Figure(traces1)\n",
    "    title1=f\"Directions of Principal Components in 3D <br>for {TVType}{' Plan' if pP else ''}s {site}\"\n",
    "    fig1.update_layout(      title=title1,\n",
    "                                margin=dict(r=0, b=0, l=0, t=60),\n",
    "                                height=600,\n",
    "                                width=600,\n",
    "                                scene_camera=dict(eye=dict(x=1.5, y=1.5, z=1.5))\n",
    "                                )\n",
    "    if show: fig1.show()\n",
    "    t1,p1 = cartesian_to_spherical(PC[:,0,0],y=PC[:,0,1],z=PC[:,0,2])\n",
    "    t2,p2 = cartesian_to_spherical(PC[:,1,0],y=PC[:,1,1],z=PC[:,1,2])\n",
    "    t3,p3 = cartesian_to_spherical(PC[:,2,0],y=PC[:,2,1],z=PC[:,2,2])\n",
    "    f1=go.Scatter(y=t1,x=p1,mode='markers',name='Primary',marker=dict(size=size,color='blue'))\n",
    "    f2=go.Scatter(y=t2,x=p2,mode='markers',name='Seconary',marker=dict(size=size,color='red'))\n",
    "    f3=go.Scatter(y=t3,x=p3,mode='markers',name='Tertiary',marker=dict(size=size,color='green'))\n",
    "    traces=[f1,f2,f3]\n",
    "    for i,t in enumerate(traces):\n",
    "        fig=go.Figure(t)\n",
    "        titlei=f\"Directions of Principal Component {i+1} in Spherical Coordinates <br>for {TVType}{' Plan' if pP else ''}s {site}\"\n",
    "        fig.update_layout(      title=titlei,\n",
    "                                    yaxis_title='Polar Angle θ [rad]',\n",
    "                                    xaxis_title='Azimuth φ [rad]',\n",
    "                                    xaxis = dict(\n",
    "                                        tickmode = 'array',\n",
    "                                        tickvals = [-np.pi, -np.pi/2, 0, np.pi/2, np.pi],\n",
    "                                        ticktext = ['-π', '-π/2', '0', 'π/2', 'π']\n",
    "                                    ),\n",
    "                                    yaxis = dict(\n",
    "                                        tickmode = 'array',\n",
    "                                        tickvals = [0, np.pi/2, np.pi],\n",
    "                                        ticktext = ['0', 'π/2', 'π']\n",
    "                                    ),\n",
    "                                    margin=dict(r=0, b=0, l=0, t=60),\n",
    "                                    yaxis_range=[0,np.pi], xaxis_range=[-np.pi,np.pi], height=400, width=800\n",
    "                                    )\n",
    "        if show: fig.show()\n",
    "        if save: \n",
    "            fig.write_image(f\"{folder}/{titlei.replace('<br>','')}.pdf\")\n",
    "            fig.write_image(f\"{folder}/{titlei.replace('<br>','')}.png\")\n",
    "    fig=go.Figure(traces)\n",
    "    title=f\"Directions of Principal Components in Spherical Coordinates <br>for {TVType}{' Plan' if pP else ''}s {site}\"\n",
    "    fig.update_layout(      title=title,\n",
    "                                yaxis_title='Polar Angle θ [rad]',\n",
    "                                xaxis_title='Azimuth φ [rad]',\n",
    "                                xaxis = dict(\n",
    "                                    tickmode = 'array',\n",
    "                                    tickvals = [-np.pi, -np.pi/2, 0, np.pi/2, np.pi],\n",
    "                                    ticktext = ['-π', '-π/2', '0', 'π/2', 'π']\n",
    "                                ),\n",
    "                                yaxis = dict(\n",
    "                                    tickmode = 'array',\n",
    "                                    tickvals = [0, np.pi/2, np.pi],\n",
    "                                    ticktext = ['0', 'π/2', 'π']\n",
    "                                ),\n",
    "                                margin=dict(r=0, b=0, l=0, t=60),\n",
    "                                yaxis_range=[0,np.pi], xaxis_range=[-np.pi,np.pi], height=400, width=800\n",
    "                                )\n",
    "    if show: fig.show()\n",
    "    if save:\n",
    "        fig.write_image(f\"{folder}/{title.replace('<br>','')}.pdf\")\n",
    "        fig1.write_image(f\"{folder}/{title1.replace('<br>','')}.pdf\")\n",
    "        fig.write_image(f\"{folder}/{title.replace('<br>','')}.png\")\n",
    "        fig1.write_image(f\"{folder}/{title1.replace('<br>','')}.png\")\n",
    "\n",
    "def plotPCVar(dfpca,save=None,show=True,pP=True,TVType='PTV',site=''):\n",
    "    PC_Var=np.concatenate(dfpca['PC_Var'].to_numpy()).reshape(-1,3)\n",
    "    if save:\n",
    "        save=f\"{save}/Var\"\n",
    "    plotTertHist(PC_Var,save, f\"Ternary Plot of Variance Ratio of Principal Components <br>for {TVType}{' Plan' if pP else ''}s {site}\",show,pP,TVType,site)\n",
    "\n",
    "def plotPCStd(dfpca,save=None,show=True,pP=True,TVType='PTV',site=''):\n",
    "    PC_Std=np.sqrt(np.concatenate(dfpca['PC_Var'].to_numpy()).reshape(-1,3))\n",
    "    if save:\n",
    "        save=f\"{save}/Std\"\n",
    "    plotTertHist(PC_Std,save,f\"Ternary Plot of Standard Deviation Ratio of Principal Components <br>for {TVType}{' Plan' if pP else ''}s {site}\",show,pP,TVType,site)\n",
    "\n",
    "def plotPCEfSize(dfpca,save=None,show=True,pP=True,TVType='PTV',site=''):\n",
    "    length=np.concatenate(dfpca['Size'].to_numpy()).reshape(-1,3)/10\n",
    "    # print(\"Eff Size\",np.mean(length), np.std(length), np.median(length))\n",
    "    if len(length)<1000:\n",
    "        size=4\n",
    "    else:\n",
    "        size=2\n",
    "    rmin = np.min(length[:,:2])\n",
    "    rmax = np.max(length[:,:2])\n",
    "    fig=go.Figure(go.Scatter(x=length[:,0],y=length[:,1],marker=dict(color=length[:,2],size=size,showscale=True,colorbar=dict(title=\"Tertiary [cm]\")),mode='markers'), layout={\"height\":600, \"width\":600})\n",
    "    title=f\"Effective Size along Principal Components <br>for {TVType}{' Plan' if pP else ''}s {site}\"\n",
    "    fig.update_layout(title=title,xaxis_title='Primary [cm]', yaxis_title='Secondary [cm]',xaxis_range=[rmin,rmax], yaxis_range=[rmin,rmax])\n",
    "    dtick=rmax/10\n",
    "    if dtick > 1: dtick=np.round(dtick)\n",
    "    else: dtick = np.round(dtick,1)\n",
    "    fig.update_yaxes(dtick=dtick)\n",
    "    fig.update_xaxes(dtick=dtick)\n",
    "    if show: fig.show()\n",
    "    if save:\n",
    "        folder = f\"{save}\"\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        fig.write_image(f\"{folder}/{title.replace('<br>','')}.pdf\")\n",
    "        fig.write_image(f\"{folder}/{title.replace('<br>','')}.png\")\n",
    "    plotTertHist(length,save,f\"Ternary Plot of Effective Size Ratio of Principal Components <br>for {TVType}{' Plan' if pP else ''}s {site}\",show,pP,TVType,site)\n",
    "\n",
    "def plotMinMax(dfpca,save=None,show=True,pP=True,TVType='PTV',site=''):\n",
    "    try:\n",
    "        PC_min=np.concatenate(dfpca['min'].to_numpy()).reshape(-1,3)\n",
    "        PC_max=np.concatenate(dfpca['max'].to_numpy()).reshape(-1,3)\n",
    "    except:\n",
    "        PC_min=dfpca['min'].to_numpy().reshape(-1,3)\n",
    "        PC_max=dfpca['max'].to_numpy().reshape(-1,3)\n",
    "    length=(PC_max-PC_min)/10\n",
    "    # print(\"Max Size Couch\",np.mean(length), np.std(length), np.median(length))\n",
    "    if len(length)<1000:\n",
    "        size=4\n",
    "    else:\n",
    "        size=2\n",
    "    fig=go.Figure(go.Scatter(x=length[:,0],y=length[:,1],marker=dict(color=length[:,2],size=size,showscale=True,colorbar=dict(title=\"z [cm]\")),mode='markers',), layout={\"height\":600, \"width\":600})\n",
    "    rmin = np.min(length[:,:2])\n",
    "    rmax = np.max(length[:,:2])\n",
    "    title=f\"Max Size along X, Y, Z (Couch System) <br>for {TVType}{' Plan' if pP else ''}s {site}\"\n",
    "    fig.update_layout(title=title,xaxis_title='X [cm]', yaxis_title='Y [cm]',xaxis_range=[rmin,rmax], yaxis_range=[rmin,rmax])\n",
    "    dtick=rmax/10\n",
    "    if dtick > 1: dtick=np.round(dtick)\n",
    "    else: dtick = np.round(dtick,1)\n",
    "    fig.update_yaxes(dtick=dtick)\n",
    "    fig.update_xaxes(dtick=dtick)\n",
    "    if show: fig.show()\n",
    "    if save:\n",
    "        folder = f\"{save}\"\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        fig.write_image(f\"{folder}/{title.replace('<br>','')}.pdf\")\n",
    "        fig.write_image(f\"{folder}/{title.replace('<br>','')}.png\")\n",
    "\n",
    "def plotPCL(dfpca,save=None,show=True,pP=True,TVType='PTV',site=''):\n",
    "    try:\n",
    "        PC_min=np.concatenate(dfpca['PC_min'].to_numpy()).reshape(-1,3)\n",
    "        PC_max=np.concatenate(dfpca['PC_max'].to_numpy()).reshape(-1,3)\n",
    "    except:\n",
    "        PC_min=dfpca['PC_min'].to_numpy().reshape(-1,3)\n",
    "        PC_max=dfpca['PC_max'].to_numpy().reshape(-1,3)\n",
    "\n",
    "    length=(PC_max-PC_min)/10\n",
    "    # print(\"Max Size\",np.mean(length), np.std(length), np.median(length))\n",
    "    # plot 3d instead of 2d\n",
    "    # fig=go.Figure(go.Scatter3d(x=length[:,0],y=length[:,1],z=length[:,2],mode='markers'))\n",
    "    # fig.update_layout(      title=\"Principal Components Measures\",\n",
    "    #                             scene = dict(xaxis_title='Primary',\n",
    "    #                             yaxis_title='Secondary',\n",
    "    #                             zaxis_title='Tertiary'),\n",
    "    #                             # legend=dict(yanchor=\"top\",\n",
    "    #                             #             y=0,\n",
    "    #                             #             xanchor=\"left\",\n",
    "    #                             #             x=1),\n",
    "    #                             # #width=700,\n",
    "    #                             margin=dict(r=0, b=0, l=0, t=60),\n",
    "    #                             # scene_camera=dict(eye=dict(x=2.0, y=2.0, z=0.75))\n",
    "    #                             )\n",
    "    \n",
    "    if len(length)<1000:\n",
    "        size=4\n",
    "    else:\n",
    "        size=2\n",
    "    rmin = np.min(length[:,:2])\n",
    "    rmax = np.max(length[:,:2])\n",
    "    fig=go.Figure(go.Scatter(x=length[:,0],y=length[:,1],marker=dict(color=length[:,2],size=size,showscale=True,colorbar=dict(title=\"Tertiary [cm]\")),mode='markers'), layout={\"height\":600, \"width\":600})\n",
    "    title=f\"Max Size along Principal Components <br>for {TVType}{' Plan' if pP else ''}s {site}\"\n",
    "    fig.update_layout(title=title,xaxis_title='Primary [cm]', yaxis_title='Secondary [cm]',xaxis_range=[rmin,rmax], yaxis_range=[rmin,rmax])#margin=dict(r=0, b=0, l=0, t=60))\n",
    "    dtick=rmax/10\n",
    "    if dtick > 1: dtick=np.round(dtick)\n",
    "    else: dtick = np.round(dtick,1)\n",
    "    fig.update_yaxes(dtick=dtick)\n",
    "    fig.update_xaxes(dtick=dtick)\n",
    "    if show: fig.show()\n",
    "    if save:\n",
    "        folder = f\"{save}\"\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        fig.write_image(f\"{folder}/{title.replace('<br>','')}.pdf\")\n",
    "        fig.write_image(f\"{folder}/{title.replace('<br>','')}.png\")\n",
    "        save=f\"{save}/Max Size\"\n",
    "    # plotTertHist(length,save,f\"Ternary Plot of Max Size Ratio of Principal Components <br>for {TVType}{' Plan' if pP else ''}s {site}\",show,pP,TVType,site)\n",
    "\n",
    "def plothistdf(df,x,save=None,show=True,pP=True,TVType='PTV',site=''):\n",
    "    fig1 = px.histogram(df,x=x)\n",
    "    title=f\"{x} <br>for {TVType}{' Plan' if pP else ''}s {site}\"\n",
    "    fig1.update_layout(      title=title,\n",
    "                                xaxis_title=x,\n",
    "                                yaxis_title=f\"Number of {TVType}{' Plan' if pP else ''}s\",\n",
    "                                height=300,\n",
    "                                width=600,\n",
    "                                margin=dict(r=0, b=0, l=0, t=60)\n",
    "                                )\n",
    "    if x=='Volume':\n",
    "        fig1.update_xaxes(range=[0,2000])\n",
    "\n",
    "    if show: fig1.show()\n",
    "    if save:\n",
    "        if not os.path.exists(save):\n",
    "            os.makedirs(save)\n",
    "        fig1.write_image(f\"{save}/{title.replace('<br>','').replace('/','_')}.pdf\")\n",
    "        fig1.write_image(f\"{save}/{title.replace('<br>','').replace('/','_')}.png\")\n",
    "\n",
    "def plothistdfs(dfs,cats,x,save=None,show=True,pP=True,TVType='PTV'):\n",
    "    try:\n",
    "        fig1=go.Figure()\n",
    "        hist_data=[]\n",
    "        names=[]\n",
    "        for i,df in enumerate(dfs):\n",
    "            h=df[x][~np.isnan(df[x])]\n",
    "            if len(h)>1:\n",
    "                if cats[i]!='Benign':\n",
    "                    hist_data.append(h)\n",
    "                    names.append(cats[i])\n",
    "\n",
    "        # for i,df in enumerate(dfs):\n",
    "        #     fig1.add_trace(go.Histogram(x=df[x],name=cats[i]))\n",
    "        fig1=ff.create_distplot(hist_data, names, show_hist=False, show_rug=False,colors=colors[:3*len(cats):3])\n",
    "        title=f\"Histogram of {x} <br>for {TVType}{' Plan' if pP else ''}s\"\n",
    "        fig1.update_layout(      title=title,\n",
    "                                    xaxis_title=x,\n",
    "                                    yaxis_title=f\"Kernel Density Estimation of Count\",\n",
    "                                    height=400,\n",
    "                                    width=600,\n",
    "                                    margin=dict(r=0, b=0, l=0, t=60)\n",
    "                                    )\n",
    "        # # Overlay both histograms\n",
    "        # fig1.update_layout(barmode='overlay')\n",
    "        # # Reduce opacity to see both histograms\n",
    "        # fig1.update_traces(opacity=0.75)\n",
    "        if x=='Volume':\n",
    "            fig1.update_xaxes(range=[0,2000])\n",
    "        elif x=='HU Median':\n",
    "            fig1.update_xaxes(range=[-800,200])\n",
    "\n",
    "        if show: fig1.show()\n",
    "        if save:\n",
    "            if not os.path.exists(save):\n",
    "                os.makedirs(save)\n",
    "            fig1.write_image(f\"{save}/{title.replace('<br>','').replace('/','_')}.pdf\")\n",
    "            fig1.write_image(f\"{save}/{title.replace('<br>','').replace('/','_')}.png\")\n",
    "    except:\n",
    "        print(f\"Failed Histogram of {x} <br>for {TVType}{' Plan' if pP else ''}s, too little data?\")\n",
    "\n",
    "def plotattvsattlung(df,save=None,show=True,pP=True,TVType='PTV',site=''):\n",
    "    fig1 = px.histogram(x=df[\"Max Depth q0_attLung\"]-df[\"Max Depth q0_att\"])\n",
    "    title=f\"Difference Depth q0_attLung - q0_att <br>for {TVType}{' Plan' if pP else ''}s {site}\"\n",
    "    fig1.update_layout(      title=title,\n",
    "                                xaxis_title=\"Max Depth q0_attLung - q0_att [cm]\",\n",
    "                                yaxis_title=f\"Number of {TVType}{' Plan' if pP else ''}s\",\n",
    "                                height=300,\n",
    "                                width=600,\n",
    "                                margin=dict(r=0, b=0, l=0, t=60)\n",
    "                                )\n",
    "    if show: fig1.show()\n",
    "    if save:\n",
    "        if not os.path.exists(save):\n",
    "            os.makedirs(save)\n",
    "        fig1.write_image(f\"{save}/{title.replace('<br>','')}.pdf\")\n",
    "        fig1.write_image(f\"{save}/{title.replace('<br>','')}.png\")\n",
    "\n",
    "def plotTechnique(df,save=None,show=True,pP=True,TVType='PTV',site=''):\n",
    "    fig1 = px.histogram(x=df[\"Rx technique\"], text_auto=True)\n",
    "    title=f\"Technique <br>for {TVType}{' Plan' if pP else ''}s {site}\"\n",
    "    fig1.update_layout(      title=title,\n",
    "                                xaxis_title=\"Technique\",\n",
    "                                yaxis_title=f\"Number of {TVType}{' Plan' if pP else ''}s\",\n",
    "                                height=300,\n",
    "                                width=600,\n",
    "                                margin=dict(r=0, b=0, l=0, t=60)\n",
    "                                )\n",
    "    if show: fig1.show()\n",
    "    if save:\n",
    "        if not os.path.exists(save):\n",
    "            os.makedirs(save)\n",
    "        fig1.write_image(f\"{save}/{title.replace('<br>','')}.pdf\")\n",
    "        fig1.write_image(f\"{save}/{title.replace('<br>','')}.png\")\n",
    "\n",
    "def plotMUAnglehist(df,save=None,show=True,pP=True,TVType='PTV',site=''):\n",
    "    y=[]\n",
    "    for entry in df[\"MU per Angle\"]:\n",
    "        try:\n",
    "            for e in entry:\n",
    "                for ee in e:\n",
    "                    y.append(ee)\n",
    "        except:\n",
    "            pass\n",
    "    x=[]\n",
    "    for entry in df[\"Gantry Angle\"]:\n",
    "        try:\n",
    "            for e in entry:\n",
    "                for ee in e:\n",
    "                    x.append(ee)\n",
    "        except:\n",
    "            pass\n",
    "    fig1 = px.histogram(x=x,y=y)\n",
    "    title=f\"MU per Gantry Angle Histogram <br>for {TVType}{' Plan' if pP else ''}s {site}\"\n",
    "    fig1.update_layout(      title=title,\n",
    "                                xaxis_title=\"Gantry Angle\",\n",
    "                                yaxis_title='MU',\n",
    "                                height=600,\n",
    "                                width=1200,\n",
    "                                margin=dict(r=0, b=0, l=0, t=60)\n",
    "                                )\n",
    "    if show: fig1.show()\n",
    "    if save:\n",
    "        if not os.path.exists(save):\n",
    "            os.makedirs(save)\n",
    "        fig1.write_image(f\"{save}/{title.replace('<br>','')}.pdf\")\n",
    "        fig1.write_image(f\"{save}/{title.replace('<br>','')}.png\")\n",
    "\n",
    "def plotskew(df,save=None,show=True,pP=True,TVType='PTV',site=''):\n",
    "    try:\n",
    "        skew=np.concatenate(df['PC_Skew'].to_numpy()).reshape(-1,3)\n",
    "    except:\n",
    "        skew=df['PC_Skew'].to_numpy().reshape(-1,3)\n",
    "    # plot 3d instead of 2d\n",
    "    norm=np.linalg.norm(skew,axis=1)\n",
    "    fig1 = px.histogram(x=norm)\n",
    "    title1=f\"L2 norm of Skewness <br>for {TVType}{' Plan' if pP else ''}s {site}\"\n",
    "    fig1.update_layout(      title=title1, #does that actually make sense? Why not L1?\n",
    "                                xaxis_title='L2 norm of Skewness',\n",
    "                                yaxis_title=f\"Number of {TVType}{' Plan' if pP else ''}s\",\n",
    "                                height=300,\n",
    "                                width=600,\n",
    "                                margin=dict(r=0, b=0, l=0, t=60),\n",
    "                                )\n",
    "    if show: fig1.show()\n",
    "    fig2=go.Figure(go.Scatter3d(x=skew[:,0],y=skew[:,1],z=skew[:,2],mode='markers',marker=dict(color=norm,size=3)))\n",
    "    title2=f\"Skewness along Principal Components <br>for {TVType}{' Plan' if pP else ''}s {site}\"\n",
    "    fig2.update_layout(      title=title2,\n",
    "                                scene = dict(xaxis_title='Primary',\n",
    "                                yaxis_title='Secondary',\n",
    "                                zaxis_title='Tertiary'),\n",
    "                                height=300,\n",
    "                                width=600,\n",
    "                                margin=dict(r=0, b=0, l=0, t=60),\n",
    "                                scene_camera=dict(eye=dict(x=1.5, y=1.5, z=1.5))\n",
    "                                )\n",
    "    if show: fig2.show()\n",
    "    rmin = np.min(skew[:,:2])\n",
    "    rmax = np.max(skew[:,:2])\n",
    "    fig3=go.Figure(go.Scatter(x=skew[:,0],y=skew[:,1],marker=dict(color=skew[:,2],showscale=True,colorbar=dict(title=\"Tertiary\")),mode='markers'), layout={\"height\":600, \"width\":600})\n",
    "    title3=f\"Skewness along Principal Components <br>for {TVType}{' Plan' if pP else ''}s {site}\"\n",
    "    fig3.update_layout(title=title3,xaxis_title='Primary', yaxis_title='Secondary',xaxis_range=[rmin,rmax], yaxis_range=[rmin,rmax])#,margin=dict(r=0, b=0, l=0, t=60))\n",
    "    dtick=rmax/10\n",
    "    if dtick > 1: dtick=np.round(dtick)\n",
    "    else: dtick = np.round(dtick,1)\n",
    "    fig3.update_yaxes(dtick=dtick)\n",
    "    fig3.update_xaxes(dtick=dtick)\n",
    "    if show: fig3.show()\n",
    "    \n",
    "    if save:\n",
    "        folder = f\"{save}\"\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        fig1.write_image(f\"{folder}/{title1.replace('<br>','')}.pdf\")#, height=600, width=1800)\n",
    "        fig2.write_image(f\"{folder}/{title2.replace('<br>','')}.pdf\")#, height=600, width=1800)\n",
    "        fig3.write_image(f\"{folder}/{title3.replace('<br>','')}.pdf\")#, height=600, width=1800)\n",
    "        fig1.write_image(f\"{folder}/{title1.replace('<br>','')}.png\")#, height=600, width=1800)\n",
    "        fig2.write_image(f\"{folder}/{title2.replace('<br>','')}.png\")#, height=600, width=1800)\n",
    "        fig3.write_image(f\"{folder}/{title3.replace('<br>','')}.png\")#, height=600, width=1800)\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def plotTertHist(PC_Var,save=None,title=\"Ternary Plot of Ratio of Principal Components\",show=True,pP=True,TVType='PTV',site=''):\n",
    "    \n",
    "    spherical=(PC_Var[:,2]/PC_Var[:,0])>0.66\n",
    "    discoid=(PC_Var[:,1][~spherical]/PC_Var[:,0][~spherical])>0.8\n",
    "    ellipsoid=(PC_Var[:,2][~spherical][~discoid]/PC_Var[:,1][~spherical][~discoid])<0.8\n",
    "    cylindrical=~ellipsoid\n",
    "    fig1=px.histogram(x=['Cylindrical','Ellipsoid', 'Discoid', 'Spherical'],y=[np.sum(cylindrical),np.sum(ellipsoid),np.sum(discoid),np.sum(spherical)])\n",
    "    title1=f\"Histogram of number of {TVType}{' Plan' if pP else ''}s <br>classified per Shape {site}\"\n",
    "    fig1.update_layout(      title=title1,\n",
    "                                xaxis_title='Classified Shape',\n",
    "                                yaxis_title=f\"Number of {TVType}{' Plan' if pP else ''}s\",\n",
    "                                height=300,\n",
    "                                width=600,\n",
    "                                margin=dict(r=0, b=0, l=0, t=60)\n",
    "                                )\n",
    "    if show: fig1.show()\n",
    "    norm=normalize(PC_Var,norm='l1')\n",
    "    sphere=norm[spherical]\n",
    "    \n",
    "    if len(norm)<1000:\n",
    "        size=4\n",
    "    else:\n",
    "        size=2\n",
    "    f1=go.Scatterternary(a=sphere[:,0],b=sphere[:,1],c=sphere[:,2],mode='markers',name=\"Spherical\",marker=dict(size=size))\n",
    "    disc=norm[~spherical][discoid]\n",
    "    f2=go.Scatterternary(a=disc[:,0],b=disc[:,1],c=disc[:,2],mode='markers',name='Discoid',marker=dict(size=size))\n",
    "    ellipse=norm[~spherical][~discoid][ellipsoid]\n",
    "    f3=go.Scatterternary(a=ellipse[:,0],b=ellipse[:,1],c=ellipse[:,2],mode='markers',name='Ellipsoid',marker=dict(size=size))\n",
    "    cylinder=norm[~spherical][~discoid][~ellipsoid]\n",
    "    f4=go.Scatterternary(a=cylinder[:,0],b=cylinder[:,1],c=cylinder[:,2],mode='markers',name='Cylindrical',marker=dict(size=size))\n",
    "    fig=go.Figure([f1,f2,f3,f4], layout={\"height\":600, \"width\":600})\n",
    "    fig.update_layout(title=title)\n",
    "    if show: fig.show()\n",
    "    \n",
    "    if save:\n",
    "        folder = f\"{save}\"\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        fig1.write_image(f\"{folder}/{title1.replace('<br>','')}.pdf\")#, height=600, width=1800)\n",
    "        fig.write_image(f\"{folder}/{title.replace('<br>','')}.pdf\")#, height=600, width=1800)\n",
    "        fig1.write_image(f\"{folder}/{title1.replace('<br>','')}.png\")#, height=600, width=1800)\n",
    "        fig.write_image(f\"{folder}/{title.replace('<br>','')}.png\")#, height=600, width=1800)\n",
    "\n",
    "    # plot all pointclouds per category\n",
    "    \n",
    "    # sphere = np.concatenate(dfpca['XYZ'].to_numpy()[spherical]).reshape(-1,3)\n",
    "    # fig2= go.Figure(go.Scatter3d(x=sphere[:,0],y=sphere[:,1],z=sphere[:,2],mode='markers',marker=dict(size=2)))\n",
    "    # fig2.show()\n",
    "    # disc = np.concatenate(dfpca['XYZ'].to_numpy()[~spherical][discoid]).reshape(-1,3)\n",
    "    # fig3= go.Figure(go.Scatter3d(x=disc[:,0],y=disc[:,1],z=disc[:,2],mode='markers',marker=dict(size=2)))\n",
    "    # fig3.show()\n",
    "    # ellipse = np.concatenate(dfpca['XYZ'].to_numpy()[~spherical][~discoid][ellipsoid]).reshape(-1,3)\n",
    "    # fig4= go.Figure(go.Scatter3d(x=ellipse[:,0],y=ellipse[:,1],z=ellipse[:,2],mode='markers',marker=dict(size=2)))\n",
    "    # fig4.show()\n",
    "    # cylinder = np.concatenate(dfpca['XYZ'].to_numpy()[~spherical][~discoid][~ellipsoid]).reshape(-1,3)\n",
    "    # fig5= go.Figure(go.Scatter3d(x=cylinder[:,0],y=cylinder[:,1],z=cylinder[:,2],mode='markers',marker=dict(size=2)))\n",
    "    # fig5.show()\n",
    "\n",
    "def plotTreatableDepthVol(df,save,show,depth,vol,dcolumns,vcolumn,pP,TVType,site):\n",
    "    for dcolumn in dcolumns:\n",
    "        dft=df.loc[~np.isnan(df[dcolumn])]\n",
    "        total=lenu(dft['PlanSetupSer'],pP)\n",
    "        data=[]\n",
    "        for v in vol:\n",
    "            # total=np.array([lenu(df['PlanSetupSer'],pP)-lenu(df['PlanSetupSer'][np.any([df[vcolumn]>v,df[dcolumn]>d],0)],pP) for d in depth])\n",
    "            totaltr=np.array([lenu(dft['PlanSetupSer'][np.all([dft[vcolumn]<=v,dft[dcolumn]<=d],0)],pP) for d in depth])\n",
    "            y=totaltr/total*100\n",
    "            std=np.sqrt(totaltr)/total*100\n",
    "            data.append(go.Scatter(x=depth,y=y,error_y=dict(array=std),name=f\"{v}\"),)\n",
    "        title=f\"Treatable {TVType}{' Plan' if pP else ''}s <br>for {dcolumn}, {vcolumn}, {site}\"\n",
    "        fig=go.Figure(data)\n",
    "        fig.update_layout(      title=title,\n",
    "                                    xaxis_title=f\"Max. Treatable {dcolumn} [cm]\",\n",
    "                                    yaxis_title='Percent Treatable of Total',\n",
    "                                    height=600,\n",
    "                                    width=600,\n",
    "                                    margin=dict(r=0, b=0, l=0, t=60),\n",
    "                                    legend={\"title\":f\"Max. Treatable <br> {vcolumn} [cc]\"}\n",
    "                                    )\n",
    "        if show: fig.show()\n",
    "        if save:\n",
    "            if not os.path.exists(save):\n",
    "                os.makedirs(save)\n",
    "            fig.write_image(f\"{save}/{title.replace('<br>','')}.pdf\")\n",
    "            fig.write_image(f\"{save}/{title.replace('<br>','')}.png\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plotbardfcat(dfs,cats,x,plot,pP,TVType,width,show,func=None,log=False): \n",
    "    if x==\"Length\" or x==\"LengthXYZ\" or x==\"Std\" or x==\"Skew\" or x=='Effective Size':     \n",
    "        if x==\"Length\":\n",
    "            length=[(np.concatenate(d['PC_max'].to_numpy()).reshape(-1,3)-np.concatenate(d['PC_min'].to_numpy()).reshape(-1,3))/10 for d in dfs]\n",
    "            yaxis_title=f\"Max Size along Principal Axes [cm]\"# with 95% Confidence Interval [cm]\"   \n",
    "            title=f\"Max Size along Principal Axes per Site\"\n",
    "        elif x==\"LengthXYZ\":\n",
    "            length=[(np.concatenate(d['max'].to_numpy()).reshape(-1,3)-np.concatenate(d['min'].to_numpy()).reshape(-1,3))/10 for d in dfs]\n",
    "            yaxis_title=f\"Max Size along XYZ Axes [cm]\"# with 95% Confidence Interval [cm]\"\n",
    "            title=f\"Max Size along XYZ Axes (Couch System) per Site\"\n",
    "        elif x==\"Std\":\n",
    "            length=[(np.sqrt(np.concatenate(d['PC_Var'].to_numpy()).reshape(-1,3)))/10 for d in dfs]\n",
    "            yaxis_title=f\"Standard Deviation along Principal Axes [cm]\"# with 95% Confidence Interval [cm]\"\n",
    "            title=f\"Standard Deviation along Principal Axes per Site\"\n",
    "        elif x==\"Effective Size\":\n",
    "            length=[np.concatenate(d['Size'].to_numpy()).reshape(-1,3)/10 for d in dfs]\n",
    "            yaxis_title=f\"Effective Size along Principal Axes [cm]\"# with 95% Confidence Interval [cm]\"\n",
    "            title=f\"Effective Size along Principal Axes per Site\"\n",
    "        elif x==\"Skew\":\n",
    "            length=[np.concatenate(d['PC_Skew'].to_numpy()).reshape(-1,3) for d in dfs]\n",
    "            yaxis_title=f\"Skewness along Principal Axes\"# with 95% Confidence Interval\"\n",
    "            title=f\"Skewness along Principal Axes per Site\"\n",
    "        else:\n",
    "            warnings.warn(f\"{x} not recognized\")\n",
    "            return\n",
    "        data=[]\n",
    "        for i in range(3):\n",
    "            y=np.concatenate([l[:,i] for l in length])\n",
    "            catss=[]\n",
    "            for c,cat in enumerate(cats):\n",
    "                for _ in range(len(length[c])):\n",
    "                    catss.append(cat)\n",
    "            data.append(go.Box(name=f\"Axis {i+1}\",y=y,x=catss,boxmean=True,notched=True,showlegend=True))\n",
    "        fig = go.Figure(data)#\n",
    "        fig.update_layout(boxmode='group')\n",
    "        if not x=='Skew':\n",
    "            fig.update_yaxes(range=[0,30])\n",
    "        else:\n",
    "            fig.update_yaxes(range=[-1,1])\n",
    "            # fig.update_yaxes(range=[0,None],autorange='max')#rangemode=\"tozero\")\n",
    "        # fig.update_layout(barmode='group')\n",
    "        xaxis_title='Site'\n",
    "    elif x==\"Dif. of Max Depth q0_attLung - q0_att\":\n",
    "        xaxis_title='Site'\n",
    "        title=f\"{x} per Site\"\n",
    "        dif=[d[\"Max Depth q0_attLung\"]-d[\"Max Depth q0_att\"] for d in dfs]\n",
    "        catss=[]\n",
    "        for c,cat in enumerate(cats):\n",
    "            for _ in range(len(dif[c])):\n",
    "                catss.append(cat)\n",
    "        yaxis_title=f\"{x} [cm] with SD\"\n",
    "        boxmean='sd'\n",
    "        fig=go.Figure(go.Box(y=np.concatenate(dif),x=catss,boxmean=boxmean,notched=True,showlegend=False))\n",
    "        fig.update_yaxes(range=[-5,5])\n",
    "    elif x==\"Dif. of Max Depth q0 - q0_att\":\n",
    "        xaxis_title='Site'\n",
    "        title=f\"{x} per Site\"\n",
    "        dif=[d[\"Max Depth q0\"]-d[\"Max Depth q0_att\"] for d in dfs]\n",
    "        catss=[]\n",
    "        for c,cat in enumerate(cats):\n",
    "            for _ in range(len(dif[c])):\n",
    "                catss.append(cat)\n",
    "        yaxis_title=f\"{x} [cm] with SD\"\n",
    "        boxmean='sd'\n",
    "        fig=go.Figure(go.Box(y=np.concatenate(dif),x=catss,boxmean=boxmean,notched=True,showlegend=False))\n",
    "        fig.update_yaxes(range=[-5,5])\n",
    "    elif x==\"Dif. of Max Depth q0 - q0_attLung\":\n",
    "        xaxis_title='Site'\n",
    "        title=f\"{x} per Site\"\n",
    "        dif=[d[\"Max Depth q0\"]-d[\"Max Depth q0_attLung\"] for d in dfs]\n",
    "        catss=[]\n",
    "        for c,cat in enumerate(cats):\n",
    "            for _ in range(len(dif[c])):\n",
    "                catss.append(cat)\n",
    "        yaxis_title=f\"{x} [cm] with SD\"\n",
    "        boxmean='sd'\n",
    "        fig=go.Figure(go.Box(y=np.concatenate(dif),x=catss,boxmean=boxmean,notched=True,showlegend=False))\n",
    "        fig.update_yaxes(range=[0,5])\n",
    "    elif x==\"Count TV\":\n",
    "        xaxis_title='Site'\n",
    "        title=f\"Number of {TVType}s per Plan per Site\"\n",
    "        dif=[np.unique(d['PlanSetupSer'],return_counts=True)[1] for d in dfs]\n",
    "        catss=[]\n",
    "        for c,cat in enumerate(cats):\n",
    "            for _ in range(len(dif[c])):\n",
    "                catss.append(cat)\n",
    "        yaxis_title=f\"Number of {TVType}s per Plan with SD\"\n",
    "        boxmean='sd'\n",
    "        fig=go.Figure(go.Box(y=np.concatenate(dif),x=catss,boxmean=boxmean,notched=True,showlegend=False))\n",
    "    elif x=='Prescribed Dose (cGy)':\n",
    "        xaxis_title='Site'\n",
    "        title=f\"{x} per Site\"\n",
    "        y=[d['Num Fractions']*d['Dose/Frc (cGy)'] for d in dfs]\n",
    "        catss=[]\n",
    "        for c,cat in enumerate(cats):\n",
    "            for _ in range(len(y[c])):\n",
    "                catss.append(cat)\n",
    "        fig=go.Figure()\n",
    "        boxmean=True\n",
    "        yaxis_title=f\"{x}\"\n",
    "        fig.add_box(y=np.concatenate(y),x=catss,boxmean=boxmean,notched=True,showlegend=False)\n",
    "        fig.update_yaxes(range=[0,None],autorange='max')\n",
    "    else:\n",
    "        xaxis_title='Site'\n",
    "        title=f\"{x} per Site\"\n",
    "        if func:\n",
    "            y=[[func(a) for a in d[x]] for d in dfs]\n",
    "        else:    \n",
    "            y=[d[x] for d in dfs]\n",
    "        catss=[]\n",
    "        for c,cat in enumerate(cats):\n",
    "            for _ in range(len(y[c])):\n",
    "                catss.append(cat)\n",
    "        fig=go.Figure()\n",
    "        if x==\"Max Depth q0_attLung\" or x==\"Max Depth q0_att\" or x==\"Max Depth q0\" or x==\"Min Depth q0_attLung\" or x==\"Min Depth q0_att\" or x==\"Min Depth q0\":\n",
    "            boxmean='sd'\n",
    "            yaxis_title=f\"{x} [cm]\"\n",
    "            fig.add_box(y=np.concatenate(y),x=catss,boxmean=boxmean,notched=True,showlegend=False)\n",
    "            # fig.layout.xaxis2 = go.layout.XAxis(overlaying='x', range=[0, 2], showticklabels=False)\n",
    "            # fig.add_scatter(name='8 cm',x = [0, 2], y = [8, 8], mode='lines', xaxis='x2',line=dict(dash='dash', color = \"firebrick\", width = 2))\n",
    "            # fig.update_layout(yaxis_range=[0,15])\n",
    "            # fig.update_yaxes(range=[0,None],autorange='max')\n",
    "        else:\n",
    "            boxmean=True\n",
    "            yaxis_title=f\"{x}\"\n",
    "            fig.add_box(y=np.concatenate(y),x=catss,boxmean=boxmean,notched=True,showlegend=False)\n",
    "    \n",
    "    if 'olume' in x:\n",
    "        fig.update_layout(yaxis_range=[0,3000])\n",
    "        yaxis_title=f\"{yaxis_title} [cc]\"\n",
    "        ## if 'Plan' in x:\n",
    "        # fig.layout.xaxis2 = go.layout.XAxis(overlaying='x', range=[0, 2], showticklabels=False)\n",
    "        # fig.add_scatter(name='125 cc',x = [0, 2], y = [125, 125], mode='lines', xaxis='x2',line=dict(dash='dash', color = \"firebrick\", width = 2))\n",
    "    elif \"SSD\" in x:\n",
    "        fig.update_layout(yaxis_range=[80,100])\n",
    "    elif \"Beam Delivery Duration Limit [s]\"==x:\n",
    "        fig.update_layout(yaxis_range=[0,1000])\n",
    "    elif \"HU Median\"==x:\n",
    "        fig.update_layout(yaxis_range=[-500,500])\n",
    "    # elif \"Index\" in x:\n",
    "    #     fig.update_layout(yaxis_range=[0,100])\n",
    "    if log:\n",
    "        fig.update_yaxes(dict(type='log'))\n",
    "    if width>1000:\n",
    "        height=600\n",
    "    else:\n",
    "        height=300\n",
    "    fig.update_layout(      title=f\"{title}<br> {TVType if not TVType in title else ''}{' Plans' if pP else ''}\",\n",
    "                                    yaxis_title=yaxis_title,\n",
    "                                    xaxis_title=xaxis_title\n",
    "                                    )\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(plot):\n",
    "        os.makedirs(plot)\n",
    "    fig.write_html(f\"{plot}/{title.replace('/','_')} {TVType}{' Plans' if pP else ''}.html\")\n",
    "    fig.update_layout(  height=height,\n",
    "                        width=width,\n",
    "                         margin=dict(r=0, b=0, l=0, t=60))\n",
    "    if show: fig.show()\n",
    "    fig.write_image(f\"{plot}/{title.replace('/','_')} {TVType}{' Plans' if pP else ''}.pdf\")\n",
    "    fig.write_image(f\"{plot}/{title.replace('/','_')} {TVType}{' Plans' if pP else ''}.png\")\n",
    "\n",
    "def lenu(x,do):\n",
    "    if do:\n",
    "        return len(x.unique())\n",
    "    else:\n",
    "        return len(x)\n",
    "\n",
    "def plottreatable(df,dfs,cats,x,plot,pP,TVType,width,vol=125,depth=8,show=True):\n",
    "    if x=='Treatable_q0_att_VolumeVerifier' or x=='Treatable_q0_attLung_VolumeVerifier' or x=='Treatable_q0_att_VolumeCalculated' or x=='Treatable_q0_attLung_VolumeCalculated':\n",
    "        se1=[np.sqrt((lenu(d[\"PlanSetupSer\"],pP)-lenu(d[\"PlanSetupSer\"][d[x]==False],pP)))/lenu(d[\"PlanSetupSer\"],pP)*100 for d in dfs]\n",
    "        fig1=go.Bar(name=\"per Site\",y=[(lenu(d[\"PlanSetupSer\"],pP)-lenu(d[\"PlanSetupSer\"][d[x]==False],pP))/lenu(d[\"PlanSetupSer\"],pP)*100 for d in dfs],x=cats,error_y=dict(array=se1))\n",
    "        y=[(lenu(d[\"PlanSetupSer\"],pP)-lenu(d[\"PlanSetupSer\"][d[x]==False],pP))/lenu(df[\"PlanSetupSer\"],pP)*100 for d in dfs]\n",
    "        se2=[np.sqrt((lenu(d[\"PlanSetupSer\"],pP)-lenu(d[\"PlanSetupSer\"][d[x]==False],pP)))/lenu(df[\"PlanSetupSer\"],pP)*100 for d in dfs]\n",
    "        total=lenu(df['PlanSetupSer'],pP)-lenu(df['PlanSetupSer'][df[x]==False],pP)\n",
    "        fig2=go.Bar(name=f\"Total {int(np.round(total/lenu(df['PlanSetupSer'],pP)*100))} ± {int(np.round(np.sqrt(total)/lenu(df['PlanSetupSer'],pP)*100))}%\",y=y,x=cats,error_y=dict(array=se2))\n",
    "        title=f\"Treatable {TVType}{' Plans' if pP else ''} (Depth {'_'.join(x.split('_')[1:3])} <8cm, Volume<125cc)\"\n",
    "    elif x=='Treatable q0_att' or x=='Treatable q0_attLung':\n",
    "        se1=[np.sqrt((lenu(d[\"PlanSetupSer\"],pP)-lenu(d[\"PlanSetupSer\"][d[x]<1],pP)))/lenu(d[\"PlanSetupSer\"],pP)*100 for d in dfs]\n",
    "        fig1=go.Bar(name=\"per Site\",y=[(lenu(d[\"PlanSetupSer\"],pP)-lenu(d[\"PlanSetupSer\"][d[x]<1],pP))/lenu(d[\"PlanSetupSer\"],pP)*100 for d in dfs],x=cats,error_y=dict(array=se1))\n",
    "        y=[(lenu(d[\"PlanSetupSer\"],pP)-lenu(d[\"PlanSetupSer\"][d[x]<1],pP))/lenu(df[\"PlanSetupSer\"],pP)*100 for d in dfs]\n",
    "        se2=[np.sqrt((lenu(d[\"PlanSetupSer\"],pP)-lenu(d[\"PlanSetupSer\"][d[x]<1],pP)))/lenu(df[\"PlanSetupSer\"],pP)*100 for d in dfs]\n",
    "        total=lenu(df['PlanSetupSer'],pP)-lenu(df['PlanSetupSer'][df[x]<1],pP)\n",
    "        fig2=go.Bar(name=f\"Total {int(np.round(total/lenu(df['PlanSetupSer'],pP)*100))} ± {int(np.round(np.sqrt(total)/lenu(df['PlanSetupSer'],pP)*100))}%\",y=y,x=cats,error_y=dict(array=se2))\n",
    "        title=f\"Treatable {TVType}{' Plans' if pP else ''} <br>(Depth {'_'.join(x.split('_')[0:2])} <8cm)\"\n",
    "    elif x=='Plan PTV Volume Verifier' or x=='Volume' or x=='Max Depth q0_att' or x=='Max Depth q0_attLung' or x=='Max Depth q0':\n",
    "        if 'Volume' in x:\n",
    "            lim=vol\n",
    "        else:\n",
    "            lim=depth\n",
    "        se1=[np.sqrt((lenu(d[\"PlanSetupSer\"][d[x]<=lim],pP)))/lenu(d[\"PlanSetupSer\"],pP)*100 for d in dfs]\n",
    "        fig1=go.Bar(name=\"per Site\",y=[(lenu(d[\"PlanSetupSer\"][d[x]<=lim],pP))/lenu(d[\"PlanSetupSer\"],pP)*100 for d in dfs],x=cats,error_y=dict(array=se1))\n",
    "        y=[(lenu(d[\"PlanSetupSer\"][d[x]<=lim],pP))/lenu(df[\"PlanSetupSer\"],pP)*100 for d in dfs]\n",
    "        se2=[np.sqrt((lenu(d[\"PlanSetupSer\"][d[x]<=lim],pP)))/lenu(df[\"PlanSetupSer\"],pP)*100 for d in dfs]\n",
    "        total=lenu(df['PlanSetupSer'][df[x]<=lim],pP)\n",
    "        fig2=go.Bar(name=f\"Total {int(np.round(total/lenu(df['PlanSetupSer'],pP)*100))} ± {int(np.round(np.sqrt(total)/lenu(df['PlanSetupSer'],pP)*100))}%\",y=y,x=cats,error_y=dict(array=se2))\n",
    "        title=f\"Treatable {TVType}{' Plans' if pP else ''} <br>({x} <{lim}{'cc' if 'Volume' in x else 'cm'})\"\n",
    "    elif x=='Volume Max Depth q0_att' or x=='Volume Max Depth q0_attLung' or x=='Volume Max Depth q0':\n",
    "        x1='Volume'\n",
    "        x2=\" \".join(x.split(\" \")[1:])\n",
    "        se1=[np.sqrt((lenu(d[\"PlanSetupSer\"][np.all([d[x1]<=vol,d[x2]<=depth],0)],pP)))/lenu(d[\"PlanSetupSer\"],pP)*100 for d in dfs]\n",
    "        fig1=go.Bar(name=\"per Site\",y=[(lenu(d[\"PlanSetupSer\"][np.all([d[x1]<=vol,d[x2]<=depth],0)],pP))/lenu(d[\"PlanSetupSer\"],pP)*100 for d in dfs],x=cats,error_y=dict(array=se1))\n",
    "        y=[(lenu(d[\"PlanSetupSer\"][np.all([d[x1]<=vol,d[x2]<=depth],0)],pP))/lenu(df[\"PlanSetupSer\"],pP)*100 for d in dfs]\n",
    "        se2=[np.sqrt((lenu(d[\"PlanSetupSer\"][np.all([d[x1]<=vol,d[x2]<=depth],0)],pP)))/lenu(df[\"PlanSetupSer\"],pP)*100 for d in dfs]\n",
    "        total=lenu(df['PlanSetupSer'][np.all([df[x1]<=vol,df[x2]<=depth],0)],pP)\n",
    "        fig2=go.Bar(name=f\"Total {int(np.round(total/lenu(df['PlanSetupSer'],pP)*100))} ± {int(np.round(np.sqrt(total)/lenu(df['PlanSetupSer'],pP)*100))}%\",y=y,x=cats,error_y=dict(array=se2))\n",
    "        title=f\"Treatable {TVType}{' Plans' if pP else ''} <br>({x2} <{depth}cm, {x1} <{vol}cc)\"\n",
    "    else:\n",
    "        warnings.warn(f\"{x} not recognized\")\n",
    "        return\n",
    "    fig=go.Figure([fig1,fig2])\n",
    "    \n",
    "    if width>1000:\n",
    "        height=600\n",
    "    else:\n",
    "        height=300\n",
    "    fig.update_layout(      title=title,\n",
    "                                xaxis_title='Site',\n",
    "                                yaxis_title=f\"Percentage of Treatable {TVType}{' Plans' if pP else 's'} with Estimated Standard Error (\\u221An)\",\n",
    "                                height=height,\n",
    "                                width=width,\n",
    "                                margin=dict(r=0, b=0, l=0, t=60),\n",
    "                                barmode='group'\n",
    "                                )\n",
    "    fig.update_yaxes(range=[0, 100])\n",
    "    if show: fig.show()\n",
    "    if plot:\n",
    "        plot=f\"{plot}/Treatable {x}\"\n",
    "        if not os.path.exists(plot):\n",
    "            os.makedirs(plot)\n",
    "        fig.write_image(f\"{plot}/{title.replace('<br>','').replace('<', '')}.pdf\")\n",
    "        fig.write_image(f\"{plot}/{title.replace('<br>','').replace('<', '')}.png\")\n",
    "\n",
    "def plotBeamType(dfs,cats,plot,width,show):\n",
    "    static=[]\n",
    "    dynamic=[]\n",
    "    for d in dfs:\n",
    "        staticd=[]\n",
    "        dynamicd=[]\n",
    "        for i,entry in enumerate(d[\"Beam Type\"]):\n",
    "            try:\n",
    "                if entry[0]=='STATIC':\n",
    "                    staticd.append(i)\n",
    "                elif entry[0]=='DYNAMIC':\n",
    "                    dynamicd.append(i)\n",
    "            except:\n",
    "                pass\n",
    "        static.append(staticd)\n",
    "        dynamic.append(dynamicd)\n",
    "    ys=[len(d) for d in static]\n",
    "    yd=[len(d) for d in dynamic]\n",
    "    fig1=go.Bar(name=f\"Static <br>(Total {np.sum(ys)})\",y=ys,x=cats)\n",
    "    fig2=go.Bar(name=f\"Dynamic <br>(Total {np.sum(yd)})\",y=yd,x=cats)\n",
    "    title=f\"Beam Type per Site Histogram\"\n",
    "    fig=go.Figure([fig1,fig2])\n",
    "    \n",
    "    if width>1000:\n",
    "        height=600\n",
    "    else:\n",
    "        height=300\n",
    "    fig.update_layout(      title=title,\n",
    "                                xaxis_title='Site',\n",
    "                                yaxis_title=f\"Number of Plans with Beam Type\",\n",
    "                                height=height,\n",
    "                                width=width,\n",
    "                                margin=dict(r=0, b=0, l=0, t=60),\n",
    "                                barmode='group'\n",
    "                                )\n",
    "    if show: fig.show()\n",
    "    if plot:\n",
    "        if not os.path.exists(plot):\n",
    "            os.makedirs(plot)\n",
    "        fig.write_image(f\"{plot}/{title.replace('<br>','').replace('<', '')}.pdf\")\n",
    "        fig.write_image(f\"{plot}/{title.replace('<br>','').replace('<', '')}.png\")\n",
    "\n",
    "\n",
    "def plotSRS(dfs,cats,plot,width,show):\n",
    "    static=[]\n",
    "    for d in dfs:\n",
    "        staticd=[]\n",
    "        for i,entry in enumerate(d[\"High Dose Technique Type\"]):\n",
    "            try:\n",
    "                if entry[0]=='SRS':\n",
    "                    staticd.append(i)\n",
    "            except:\n",
    "                pass\n",
    "        static.append(staticd)\n",
    "    ys=[len(d) for d in static]\n",
    "    fig1=go.Bar(y=ys,x=cats)\n",
    "    title=f\"SRS per Site Histogram (Total {np.sum(ys)})\"\n",
    "    fig=go.Figure([fig1])\n",
    "    \n",
    "    if width>1000:\n",
    "        height=600\n",
    "    else:\n",
    "        height=300\n",
    "    fig.update_layout(      title=title,\n",
    "                                xaxis_title='Site',\n",
    "                                yaxis_title=f\"Number of SRS Plans\",\n",
    "                                height=height,\n",
    "                                width=width,\n",
    "                                margin=dict(r=0, b=0, l=0, t=60),\n",
    "                                showlegend=False\n",
    "                                )\n",
    "    if show: fig.show()\n",
    "    if plot:\n",
    "        if not os.path.exists(plot):\n",
    "            os.makedirs(plot)\n",
    "        fig.write_image(f\"{plot}/{title.replace('<br>','').replace('<', '')}.pdf\")\n",
    "        fig.write_image(f\"{plot}/{title.replace('<br>','').replace('<', '')}.png\")\n",
    "\n",
    "def plotavailabledata(df,dfs,cats,plot,title,width,show):\n",
    "    if 'Plan' in title:\n",
    "        y=np.array([len(d[\"PlanSetupSer\"].unique()) for d in dfs])\n",
    "        total=len(df[\"PlanSetupSer\"].unique())\n",
    "    else:\n",
    "        y=np.array([len(d[\"PlanSetupSer\"]) for d in dfs])\n",
    "        total=len(df[\"PlanSetupSer\"])\n",
    "\n",
    "    fig1=go.Bar(name=\"Percent\",y=y/total*100,x=cats,opacity=0)\n",
    "    fig2=go.Bar(name=f\"Total\",y=y,x=cats,text=y,textposition='auto')\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    fig.add_trace(fig2,secondary_y=False)\n",
    "    fig.add_trace(fig1,secondary_y=True)\n",
    "    \n",
    "    if width>1000:\n",
    "        height=600\n",
    "    else:\n",
    "        height=300\n",
    "    fig.update_layout(      title=f\"{title} (Total: {total})\",\n",
    "                                xaxis_title='Site',\n",
    "                                #yaxis_title=[\"Percentage of Plans\",'# of Plans'],\n",
    "                                height=height,\n",
    "                                width=width,\n",
    "                                margin=dict(r=0, b=0, l=0, t=60),\n",
    "                                showlegend=False\n",
    "                                # barmode='group'\n",
    "                                )\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Percentage of Total\", secondary_y=True)\n",
    "    fig.update_yaxes(title_text=\"Count\", secondary_y=False)\n",
    "    # fig.layout.xaxis2 = go.layout.XAxis(overlaying='x', range=[0, 2], showticklabels=False)\n",
    "    # fig.add_scatter(name='100',x = [0, 2], y = [100, 100], mode='lines', xaxis='x2',line=dict(dash='dash', color = \"firebrick\", width = 2))\n",
    "    if show: fig.show()\n",
    "    if plot:\n",
    "        if not os.path.exists(plot):\n",
    "            os.makedirs(plot)\n",
    "    fig.write_image(f\"{plot}/{title}.pdf\")\n",
    "    fig.write_image(f\"{plot}/{title}.png\")\n",
    "\n",
    "def plotScatter(df,x,y,save,show,pP,TVType,site,func=None,log=False, line=False):\n",
    "    try:\n",
    "        xaxis_title=x\n",
    "        yaxis_title=y\n",
    "        title=f\"{xaxis_title} vs {yaxis_title}<br> for {TVType}{' Plan' if pP else ''}s {site}\"\n",
    "        if save:\n",
    "            save=f\"{save}/{x.replace('<br>','').replace('/','_')}\"\n",
    "        if func:\n",
    "            x=[func(a) for a in df[x]]\n",
    "            if line=='id':  \n",
    "                y=[func(a) for a in df[y]]\n",
    "            else:\n",
    "                y=df[y]\n",
    "        else:\n",
    "            x=df[x]\n",
    "            y=df[y]\n",
    "        \n",
    "        if 'Mean Dose/Prescribed Dose' in yaxis_title:\n",
    "            mask=y<1.3\n",
    "            x=x[mask]\n",
    "            y=y[mask]\n",
    "        # mask=y<(np.mean(y)+5*np.std(y))\n",
    "        # fig1=px.scatter(x=x[mask],y=y[mask])\n",
    "            \n",
    "        # if 'Conformity Index 1' in y or 'Conformity Index 4' in y:\n",
    "        #     log=True\n",
    "        # else:\n",
    "        #     log=False\n",
    "            \n",
    "        # fig1=px.scatter(df,x,y,log_y=log)\n",
    "        colorscale = ['#7A4579', '#D56073', 'rgb(236,158,105)', (1, 1, 0.2), (0.98,0.98,0.98)]\n",
    "        if len(x)>1000:\n",
    "            point_size=1\n",
    "        else:\n",
    "            point_size=3\n",
    "        # fig1=px.density_heatmap(x=x,y=y)\n",
    "        \n",
    "        fig1=ff.create_2d_density(\n",
    "        x, y, colorscale=colorscale, point_size=point_size)\n",
    "        if func==np.cbrt:\n",
    "            xaxis_title=f\"{xaxis_title}^(1/3)\"\n",
    "            if line:   \n",
    "                yaxis_title=f\"{yaxis_title}^(1/3)\"\n",
    "        \n",
    "        if 'Volume' in x:\n",
    "            fig1.update_xaxes(range=[0,2000])\n",
    "        if line=='id':\n",
    "            p1=np.min([np.max(np.array(x)[~np.isnan(x)] ),np.max(np.array(y)[~np.isnan(y)])])\n",
    "            fig1.add_scatter(x=[0,p1],y=[0,p1],name='1:1',mode='lines', \n",
    "                        opacity=0.5,line=dict(\n",
    "                        # color='#0061ff',\n",
    "                        width=1\n",
    "                    ))\n",
    "        elif line=='one':\n",
    "            xnan=np.array(x)[~np.isnan(x)]\n",
    "            fig1.add_scatter(x=[np.min(xnan),np.max(xnan)],y=[1,1],name='1',mode='lines', \n",
    "                        opacity=0.5,line=dict(\n",
    "                        # color='#0061ff',\n",
    "                        width=1))\n",
    "        if 'Total' in xaxis_title:\n",
    "            try:\n",
    "                result = sm.OLS(y, x,'drop').fit()\n",
    "                # printing the summary table\n",
    "                print(result.summary())\n",
    "                slope=result.params[0]\n",
    "                # print(slope)\n",
    "                # x=np.array(x)\n",
    "                # y=np.array(y)\n",
    "                # b1=x>100\n",
    "                # x=x[b1]\n",
    "                # y=y[b1]\n",
    "                # b2=y>100\n",
    "                # x=x[b2]\n",
    "                # y=y[b2]\n",
    "                # fig2=px.scatter(x=x, y=y, trendline=\"ols\")#,trendline_options=dict(add_constant=False,log_x=True))\n",
    "                # fig2.data = [t for t in fig2.data if t.mode == \"lines\"]\n",
    "                # fig2.show()\n",
    "                # p1=np.max(np.array(x)[~np.isnan(x)])\n",
    "                p2=np.max(np.array(y)[~np.isnan(y)])\n",
    "                fig1.add_scatter(x=[0,p2/slope],y=[0,p2],name=f'Fit Slope={np.round(slope,2)}',mode='lines', \n",
    "                            opacity=0.5,line=dict(\n",
    "                            # color='#0061ff',\n",
    "                            width=1\n",
    "                        ))\n",
    "                title=f\"{title} (Fit Slope={np.round(slope,2)})\"\n",
    "            except:\n",
    "                print('Fitting Failed.')\n",
    "    # if \"Index\" in y:\n",
    "    #     fig1.update_layout(yaxis_range=[0,100])\n",
    "    \n",
    "        fig1.update_layout(      title=title,\n",
    "                                    xaxis_title=xaxis_title,\n",
    "                                    yaxis_title=yaxis_title,\n",
    "                                    legend=dict(visible=True,\n",
    "                                                yanchor=\"top\",\n",
    "                                                y=0.99,\n",
    "                                                xanchor=\"left\",\n",
    "                                                x=0.01)\n",
    "                                    )\n",
    "        if show: fig1.show()\n",
    "        if save:\n",
    "            if not os.path.exists(save):\n",
    "                os.makedirs(save)\n",
    "                fig1.write_html(f\"{save}/{title.replace('<br>','').replace('/','_')}.html\")\n",
    "                fig1.update_layout(height=500,\n",
    "                                    width=600,\n",
    "                                    margin=dict(r=0, b=0, l=0, t=60),)\n",
    "                fig1.write_image(f\"{save}/{title.replace('<br>','').replace('/','_')}.pdf\")\n",
    "                fig1.write_image(f\"{save}/{title.replace('<br>','').replace('/','_')}.png\")\n",
    "    except:\n",
    "        print(f'{save} failed. Not enough data points?')\n",
    "\n",
    "def plotIndices(df,x2,save,show,pP,TVType,site,func=None,log=True):\n",
    "    plotScatter(df,x2,'Conformity Index 1',save,show,pP,TVType,site,func,False)\n",
    "    plotScatter(df,x2,'Conformity Index 2',save,show,pP,TVType,site,func,False)\n",
    "    plotScatter(df,x2,'Conformal Number',save,show,pP,TVType,site,func,False)\n",
    "    plotScatter(df,x2,'Calc Conformity Index 1',save,show,pP,TVType,site,func,False)\n",
    "    plotScatter(df,x2,'Calc Conformity Index 2',save,show,pP,TVType,site,func,False)\n",
    "    plotScatter(df,x2,'Calc Conformal Number',save,show,pP,TVType,site,func,False)\n",
    "    plotScatter(df,x2,'Dose Balance Index',save,show,pP,TVType,site,func,log)\n",
    "    plotScatter(df,x2,'Homogeneity Index 1',save,show,pP,TVType,site,func,False)\n",
    "    plotScatter(df,x2,'Lesion Underdosage Factor',save,show,pP,TVType,site,func,False)\n",
    "    plotScatter(df,x2,'Calc Lesion Underdosage Factor',save,show,pP,TVType,site,func,False)\n",
    "\n",
    "\n",
    "def plotall(df,depth,vol,dcolumn,vcolumn,pP,TVType,site,save=None,show=True):\n",
    "    \n",
    "    \n",
    "    plotScatter(df,'Compactness','Solidity',save,show,pP,TVType,site)\n",
    "    plotScatter(df,'Volume','Compactness',save,show,pP,TVType,site,np.cbrt)\n",
    "    plotScatter(df,'Volume','Solidity',save,show,pP,TVType,site,np.cbrt)\n",
    "    plotScatter(df,'Volume','Max Depth q0_att',save,show,pP,TVType,site,np.cbrt)\n",
    "    plotTreatableDepthVol(df,save,show,depth,vol,dcolumn,vcolumn,pP,TVType,site)\n",
    "    try:\n",
    "        # plotPC(df,save,show,pP,TVType,site)\n",
    "        plotMinMax(df,save,show,pP,TVType,site)\n",
    "        # plotPCStd(df,save,show,pP,TVType,site)\n",
    "        plotPCL(df,save,show,pP,TVType,site)\n",
    "        # plotPCVar(df,save,show,pP,TVType,site)\n",
    "        plotskew(df,save,show,pP,TVType,site)\n",
    "        # try: \n",
    "        #     plothistdf(df,'PTV volume (cc)',save,show)\n",
    "        # except:\n",
    "        #     print('PTV volume (cc) not found')\n",
    "        plotPCEfSize(df,save,show,pP,TVType,site)\n",
    "    except:\n",
    "        print(f'size stuff failed pP{pP}TV{TVType}{site}')\n",
    "    plothistdf(df,'Volume',save,show,pP,TVType,site)\n",
    "    plothistdf(df,'Compactness',save,show,pP,TVType,site)\n",
    "    plothistdf(df,'Solidity',save,show,pP,TVType,site)\n",
    "    plothistdf(df,'Max Depth q0_attLung',save,show,pP,TVType,site)\n",
    "    plothistdf(df,'Max Depth q0_att',save,show,pP,TVType,site)\n",
    "    plothistdf(df,'Max Depth q0',save,show,pP,TVType,site)\n",
    "    if TVType=='PTV':\n",
    "        plotIndices(df,'Volume',save,show,pP,TVType,site,np.cbrt)\n",
    "        plotIndices(df,'Max Depth q0_att',save,show,pP,TVType,site)\n",
    "        plotIndices(df,'Compactness',save,show,pP,TVType,site)\n",
    "        plotIndices(df,'Solidity',save,show,pP,TVType,site)\n",
    "        plotScatter(df,'Volume Reference Isodose [cc]', 'Target Volume Reference Isodose [cc]', save, show, pP, TVType,site, line='id', func=np.cbrt)\n",
    "        plotScatter(df,'Calc Target Volume [cc]', 'Calc Target Volume Reference Isodose [cc]', save, show, pP, TVType,site, line='id', func=np.cbrt)\n",
    "        plotScatter(df,'Target Volume [cc]', 'Target Volume Reference Isodose [cc]', save, show, pP, TVType,site, line='id', func=np.cbrt)\n",
    "        if pP:\n",
    "            plotTechnique(df,save,show,pP,TVType,site)\n",
    "            plotScatter(df,'Calc Volume Reference Isodose [cc]', 'Calc Target Volume Reference Isodose [cc]', save, show, pP, TVType,site, line='id', func=np.cbrt)\n",
    "            plotScatter(df,'Total Dose TV [cGy]', 'Total Dose body sin. TV [cGy]', save, show, pP, TVType,site,line='id', func=np.cbrt)\n",
    "            # plothistdf(df,'Total Meterset/Prescribed Dose [MU/cGy]',save,show,pP,TVType,site)\n",
    "            plothistdf(df,'Total Meterset/Dose per Fraction [MU/cGy]',save,show,pP,TVType,site)\n",
    "            # plothistdf(df,'Dose/Frc/Total Meterset [cGy/MU]',save,show,pP,TVType,site)\n",
    "            plotIndices(df,'Total Meterset [MU]',save,show,pP,TVType,site)\n",
    "            plotScatter(df,'Total Meterset/Dose per Fraction [MU/cGy]','Max Depth q0_att',save,show,pP,TVType,site)\n",
    "            plotScatter(df,'Total Meterset/Dose per Fraction [MU/cGy]','Compactness',save,show,pP,TVType,site)\n",
    "            plotScatter(df,'Total Meterset/Dose per Fraction [MU/cGy]','Solidity',save,show,pP,TVType,site)\n",
    "            plotScatter(df,'Total Meterset/Dose per Fraction [MU/cGy]','Volume',save,show,pP,TVType,site,log=True)\n",
    "            plotScatter(df,'Total Meterset [MU]','Max Depth q0_att',save,show,pP,TVType,site)\n",
    "            plotScatter(df,'Total Meterset [MU]','Volume',save,show,pP,TVType,site,log=True)\n",
    "            plotScatter(df,'Total Meterset [MU]','Dose/Frc (cGy)',save,show,pP,TVType,site)\n",
    "            # plotMUAnglehist(df,save,show,pP,TVType,site)\n",
    "            plothistdf(df,\"Num Fractions\",save,show,pP,TVType,site)\n",
    "            plothistdf(df,\"Dose/Frc (cGy)\",save,show,pP,TVType,site)\n",
    "\n",
    "        else:\n",
    "            plothistdf(df,'Mean Dose/Prescribed Dose',save,show,pP,TVType,site)\n",
    "            plotScatter(df,'Prescribed Dose [cGy]','Mean dose (cGy)',save,show,pP,TVType,site,line='id')\n",
    "            plotScatter(df,'Prescribed Dose [cGy]','Mean Dose/Prescribed Dose',save,show,pP,TVType,site,line='one')\n",
    "            plothistdf(df,\"Mean dose (cGy)\",save,show,pP,TVType,site)\n",
    "    # plotattvsattlung(df,save,show,pP,TVType,site)\n",
    "\n",
    "def analyzebycategory(dfmain,save,perPlan=True,fine=False,plotSiteDetails=False,TVTypes=['PrePTV','PTV','CTV','GTV'],show=True):\n",
    "    \"\"\"\n",
    "    Takes the final results dataframe and performs the statistical analysis in form of plots.\n",
    "\n",
    "    In:\n",
    "    - dfmain; Pandas dataframe, MergedResults.pickle generated by mergeAllResults\n",
    "    - save; str, folder in which plots will be stored\n",
    "    - perPlan; bool, determines whether the analysis should be perdormed per plan (structs merged), or per structure\n",
    "    - fine; bool, If true, will plot subsites, if false, will plot main sites\n",
    "    - plotSiteDetails; bool, If true, generates plots for each site, if false, only generates site comparison plots\n",
    "    - TVType; str array ['PrePTV','PTV','CTV','GTV'], determine which kinds of TV to perform the analysis on\n",
    "    - show; bool, If true, plots will be shown in notebook, if False, only stored. Set to false for many plots\n",
    "\n",
    "    Out:\n",
    "    - dfs; Pandas dataframe, filtered according to the conditions given as input\n",
    "    - Many, many plots in save folder.\n",
    "\n",
    "    \"\"\"\n",
    "    dfmain=dfmain.dropna(subset=['TV Type'])\n",
    "    dfmain.rename(columns={'Calc Conformity Index 4':'Dose Balance Index','Calc Homogeneity Index 2':'Calc Lesion Underdosage Factor', 'Homogeneity Index 2' : 'Lesion Underdosage Factor','Calc Conformity Index 3':'Calc Conformal Number','Conformity Index 3':'Conformal Number','Calc Total Dose [cGy]' : 'Total Dose TV [cGy]','Calc body Total Dose [cGy]':'Total Dose body sin. TV [cGy]'},inplace=True)\n",
    "    if fine: dfmain[\"Rx site\"]=[c.replace('/','_') for c in dfmain[\"Rx site\"]]\n",
    "    # dfmain['Prescribed Dose [cGy]']=dfmain['Num Fractions']*dfmain['Dose/Frc (cGy)']\n",
    "    dfmain['Mean Dose/Prescribed Dose']=dfmain['Mean dose (cGy)']/dfmain['Prescribed Dose [cGy]']\n",
    "    dfmain['Total Meterset [MU]']=[np.sum(a) for a in dfmain['Meterset [MU]']]\n",
    "    # dfmain['Dose/Frc/Total Meterset [cGy/MU]']=dfmain['Dose/Frc (cGy)']/dfmain['Total Meterset [MU]']\n",
    "    # dfmain['Total Meterset/Prescribed Dose [MU/cGy]']=dfmain['Total Meterset [MU]']/dfmain['Prescribed Dose [cGy]']\n",
    "    dfmain['Total Meterset/Dose per Fraction [MU/cGy]']=dfmain['Total Meterset [MU]']/dfmain['Dose/Frc (cGy)']\n",
    "\n",
    "    if perPlan:\n",
    "        dfmain=dfmain[[('All' in tv or 'Solo' in tv) for tv in dfmain['TV Type']]]\n",
    "    else:\n",
    "        dfmain=dfmain[['All' not in tv for tv in dfmain['TV Type']]]\n",
    "    for TVType in TVTypes:\n",
    "        if TVType=='PrePTV':\n",
    "            try:\n",
    "                dfmaing=dfmain.groupby('PlanSetupSer').filter(lambda x: not np.any([\"CTV\" in y for y in x['TV Type']]))\n",
    "            except:\n",
    "                dfmaing=dfmain.groupby('Plan Index (Anonymized)').filter(lambda x: not np.any([\"CTV\" in y for y in x['TV Type']]))\n",
    "            df=pd.concat([dfmain[['CTV' in tv for tv in dfmain['TV Type']]],dfmaing[['GTV' in tv for tv in dfmaing['TV Type']]]]).copy()\n",
    "        else:\n",
    "            df=dfmain[[TVType in tv for tv in dfmain['TV Type']]].copy()\n",
    "        try:\n",
    "            dfCT=df.loc[~np.isnan(df['Max Depth q0_att'])]\n",
    "        except:\n",
    "            dfCT=df.loc[~np.isnan(df['Max Depth q0_att [cm]'])]\n",
    "        if fine:\n",
    "            cats=df[\"Rx site\"].unique()\n",
    "            cats.sort()\n",
    "            sites=[[c] for c in cats]\n",
    "            catsCT=dfCT[\"Rx site\"].unique()\n",
    "            catsCT.sort()\n",
    "            sitesCT=[[c] for c in catsCT]\n",
    "            width=1800\n",
    "            plot=f\"{save}/Sub Sites/{'Per Plan' if perPlan else 'Per Structure'}/{TVType}\"\n",
    "        else:\n",
    "            cats=[\"Pediatric\", \"Breast\", \"H&N\", \"Skin\", \"GI\", \"GU\", \"Gyn\", \"CNS\", \"Heme\", \"Sarcoma\", \"Thoracic\", \"Secondary\", \"Benign\"]\n",
    "            sites=[[\"Peds\"],\n",
    "                    [\"Breast\"],\n",
    "                    [\"H&N\"],\n",
    "                    [\"Skin\"],\n",
    "                    [\"GI\"],\n",
    "                    [\"GU\"],\n",
    "                    [\"Gyn\"],\n",
    "                    [\"CNS\"],\n",
    "                    [\"Heme\"],\n",
    "                    [\"Sarcoma\"],\n",
    "                    [\"Thoracic\"],\n",
    "                    [\"Secondary\"],\n",
    "                    [\"Benign\"]]\n",
    "            catsCT=cats\n",
    "            sitesCT=sites\n",
    "            width=600\n",
    "            plot=f\"{save}/Major Sites/{'Per Plan' if perPlan else 'Per Structure'}/{TVType}\"\n",
    "        # df=df[~np.isnan(df['Max Depth q0'])]\n",
    "        dfs=[df[[np.any([s in p for s in sites[c]]) for p in df[\"Rx site\"]]] for c,_ in enumerate(cats)] \n",
    "        # dmax=df['Max Depth q0_att'][~np.isnan(df['Max Depth q0_att'])]\n",
    "        # dmin=df['Min Depth q0_att'][~np.isnan(df['Min Depth q0_att'])]\n",
    "        # print('dmax',np.median(dmax),np.mean(dmax),np.std(dmax))\n",
    "        # print('dmin',np.median(dmin),np.mean(dmin),np.std(dmin))\n",
    "        # print('vol',np.median(df['Volume']),np.mean(df['Volume']),np.std(df['Volume']))\n",
    "        # print('HU',np.median(df['HU Median'][~np.isnan(df['HU Median'])]),np.mean(df['HU Median']),np.std(df['HU Median']))\n",
    "        if plot:  \n",
    "            if 'Lung' not in TVType: \n",
    "                if plotSiteDetails:\n",
    "                    plotall(df,np.arange(4,15.1,0.5),np.round(np.arange(1,7,1)**3).astype(int),['Max Depth q0_attLung','Max Depth q0_att','Max Depth q0'],'Volume',perPlan,TVType,'All Sites',f'{plot}/AllSites',show=show)\n",
    "                    for i,dfci in enumerate(dfs):\n",
    "                        plotall(dfci,np.arange(4,15.1,0.5),np.round(np.arange(1,7,1)**3).astype(int),['Max Depth q0_attLung','Max Depth q0_att','Max Depth q0'],'Volume',perPlan,TVType,cats[i],f'{plot}/{cats[i]}',show=False)\n",
    "                plotavailabledata(df,dfs,cats,plot,f\"{TVType}{' Plans' if perPlan else ''} per Site\",width,show=show)\n",
    "                # for depth in [4,8,10,12,15]:\n",
    "                #     plottreatable(df,dfs,cats,'Volume Max Depth q0_attLung',plot,perPlan,TVType,width,depth=depth,show=show)\n",
    "                #     plottreatable(df,dfs,cats,'Max Depth q0_attLung',plot,perPlan,TVType,width,depth=depth,show=show)\n",
    "                #     plottreatable(df,dfs,cats,'Volume Max Depth q0',plot,perPlan,TVType,width,depth=depth,show=show)\n",
    "                #     plottreatable(df,dfs,cats,'Max Depth q0',plot,perPlan,TVType,width,depth=depth,show=show)\n",
    "                \n",
    "                for vol in np.arange(1,7,1)**3:\n",
    "                    plottreatable(df,dfs,cats,'Volume',plot,perPlan,TVType,width,vol=vol,show=show)\n",
    "                dfsCT=[dfCT[[np.any([s in p for s in sitesCT[c]]) for p in dfCT[\"Rx site\"]]] for c,_ in enumerate(catsCT)]\n",
    "                plotavailabledata(dfCT,dfsCT,catsCT,plot,f\"{TVType}{' Plans' if perPlan else ''} with CT per Site\",width,show=show)\n",
    "                # for depth in [4,8,10,12,15]:\n",
    "                #     plottreatable(dfCT,dfsCT,catsCT,'Volume Max Depth q0_att',plot,perPlan,TVType,width,depth=depth,show=show)\n",
    "                #     plottreatable(dfCT,dfsCT,catsCT,'Max Depth q0_att',plot,perPlan,TVType,width,depth=depth,show=show)\n",
    "                \n",
    "                # try:\n",
    "                #     plottreatable(df,dfs,cats,'PTV Volume Verifier',plot,perPlan,TVType,width)\n",
    "                #     plotbardfcat(dfs,cats,'PTV Volume Verifier',plot,'Mean',width)\n",
    "                # except:\n",
    "                #     print(\"Volume Verifier not found.\")\n",
    "\n",
    "                if not \"Pre\" in TVType and not perPlan:\n",
    "                    plotbardfcat(dfs,cats,'Mean Dose/Prescribed Dose',plot,perPlan,TVType,width,show)\n",
    "                    plothistdfs(dfs,cats,'Mean Dose/Prescribed Dose',plot,show=show,pP=perPlan,TVType=TVType)\n",
    "                    plotbardfcat(dfs,cats,\"Mean dose (cGy)\",plot,perPlan,TVType,width,show)\n",
    "                    plotbardfcat(dfs,cats,\"Min dose (cGy)\",plot,perPlan,TVType,width,show)\n",
    "                    plotbardfcat(dfs,cats,\"Max dose (cGy)\",plot,perPlan,TVType,width,show)\n",
    "                \n",
    "                if TVType=='PTV' and perPlan:\n",
    "                    plotbardfcat(dfs,cats,'Total Meterset/Dose per Fraction [MU/cGy]',plot,perPlan,TVType,width,show)\n",
    "                    plotbardfcat(dfs,cats,'Total Meterset [MU]',plot,perPlan,TVType,width,show)\n",
    "                    # plotbardfcat(dfs,cats,'Dose/Frc/Total Meterset [cGy/MU]',plot,perPlan,TVType,width,show)\n",
    "                    plotbardfcat(dfs,cats,\"Num Fractions\",plot,perPlan,TVType,width,show)\n",
    "                    plotbardfcat(dfs,cats,\"Dose/Frc (cGy)\",plot,perPlan,TVType,width,show)\n",
    "                    plotbardfcat(dfs,cats,\"Prescribed Dose (cGy)\",plot,perPlan,TVType,width,show)\n",
    "                    plotbardfcat(dfs,cats,\"SSD [cm]\",plot,perPlan,TVType,width,show,np.mean)      \n",
    "                    plotbardfcat(dfs,cats,\"Dose Rate Set [MU/min]\",plot,perPlan,TVType,width,show,np.mean)\n",
    "                    plotbardfcat(dfs,cats,\"Dose [Gy]\",plot,perPlan,TVType,width,show,np.sum)\n",
    "                    plotbardfcat(dfs,cats,\"Beam Delivery Duration Limit [s]\",plot,perPlan,TVType,width,show,np.sum)\n",
    "                    plotBeamType(dfs,cats,plot,width,show)\n",
    "                    plotSRS(dfs,cats,plot,width,show)\n",
    "                    plotbardfcat(dfs,cats,\"Conformity Index 1\",plot,perPlan,TVType,width,show,log=True)\n",
    "                    plotbardfcat(dfs,cats,\"Calc Conformity Index 1\",plot,perPlan,TVType,width,show,log=True)\n",
    "                    plotbardfcat(dfs,cats,\"Conformity Index 2\",plot,perPlan,TVType,width,show)\n",
    "                    plotbardfcat(dfs,cats,\"Calc Conformity Index 2\",plot,perPlan,TVType,width,show)\n",
    "                    plotbardfcat(dfs,cats,\"Conformal Number\",plot,perPlan,TVType,width,show)\n",
    "                    plotbardfcat(dfs,cats,\"Calc Conformal Number\",plot,perPlan,TVType,width,show)\n",
    "                    plothistdfs(dfs,cats,\"Calc Conformal Number\",plot,show=show,pP=perPlan,TVType=TVType)\n",
    "                    plotbardfcat(dfs,cats,\"Dose Balance Index\",plot,perPlan,TVType,width,show)\n",
    "                    plothistdfs(dfs,cats,\"Dose Balance Index\",plot,show=show,pP=perPlan,TVType=TVType)\n",
    "                plotbardfcat(dfs,cats,\"Max Depth q0\",plot,perPlan,TVType,width,show)\n",
    "                plotbardfcat(dfs,cats,\"Max Depth q0_attLung\",plot,perPlan,TVType,width,show)\n",
    "                plotbardfcat(dfs,cats,\"Max Depth q0_att\",plot,perPlan,TVType,width,show)\n",
    "                plotbardfcat(dfs,cats,\"Min Depth q0\",plot,perPlan,TVType,width,show)\n",
    "                plotbardfcat(dfs,cats,\"Min Depth q0_attLung\",plot,perPlan,TVType,width,show)\n",
    "                plotbardfcat(dfs,cats,\"Min Depth q0_att\",plot,perPlan,TVType,width,show)\n",
    "                plotbardfcat(dfs,cats,\"Dif. of Max Depth q0_attLung - q0_att\",plot,perPlan,TVType,width,show)\n",
    "                plotbardfcat(dfs,cats,\"Dif. of Max Depth q0 - q0_att\",plot,perPlan,TVType,width,show)\n",
    "                plotbardfcat(dfs,cats,\"Dif. of Max Depth q0 - q0_attLung\",plot,perPlan,TVType,width,show)\n",
    "                # plotbardfcat(dfs,cats,\"PTV volume (cc)\",plot,'Median')\n",
    "                plotbardfcat(dfs,cats,\"Volume\",plot,perPlan,TVType,width,show)\n",
    "                plothistdfs(dfs,cats,\"Volume\",plot,show=show,pP=perPlan,TVType=TVType)\n",
    "                plothistdfs(dfs,cats,\"Max Depth q0_att\",plot,show=show,pP=perPlan,TVType=TVType)\n",
    "                plotbardfcat(dfs,cats,\"Compactness\",plot,perPlan,TVType,width,show)\n",
    "                plotbardfcat(dfs,cats,\"Solidity\",plot,perPlan,TVType,width,show)\n",
    "                plotbardfcat(dfs,cats,\"Skew\",plot,perPlan,TVType,width,show)\n",
    "                plotbardfcat(dfs,cats,\"Length\",plot,perPlan,TVType,width,show)\n",
    "                plotbardfcat(dfs,cats,\"LengthXYZ\",plot,perPlan,TVType,width,show)\n",
    "                plotbardfcat(dfs,cats,\"Effective Size\",plot,perPlan,TVType,width,show)\n",
    "                # plotbardfcat(dfs,cats,\"Std\",plot,perPlan,TVType,width,show)\n",
    "                if not perPlan: plotbardfcat(dfs,cats,\"Count TV\",plot,perPlan,TVType,width,show)\n",
    "                plotbardfcat(dfs,cats,'Homogeneity Index 1',plot,perPlan,TVType,width,show)\n",
    "                plotbardfcat(dfs,cats,'Lesion Underdosage Factor',plot,perPlan,TVType,width,show)\n",
    "                plothistdfs(dfs,cats,'Lesion Underdosage Factor',plot,show=show,pP=perPlan,TVType=TVType)\n",
    "                plotbardfcat(dfs,cats,'Calc Lesion Underdosage Factor',plot,perPlan,TVType,width,show)\n",
    "            plotbardfcat(dfs,cats,'HU Mean',plot,perPlan,TVType,width,show)\n",
    "            plotbardfcat(dfs,cats,'HU Median',plot,perPlan,TVType,width,show)   \n",
    "            plothistdfs(dfs,cats,\"HU Median\",plot,show=show,pP=perPlan,TVType=TVType)\n",
    "            plotbardfcat(dfs,cats,'HU First Quartile',plot,perPlan,TVType,width,show)   \n",
    "            plotbardfcat(dfs,cats,'HU Third Quartile',plot,perPlan,TVType,width,show)    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different function calls which built partly on top of each other to run the analysis. Hover over the functions to see their respective in and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of doing the complete sheet, only do a specified interval. Can be used to split sheet up to work in parallel.\n",
    "interval=[0,3]\n",
    "#1: Extract TV point clouds, depth and HU data from Dicom structure and image files. Generates PTV.pickle, Files.pickle\n",
    "dfPointCloud,dfFolder=batchAnalyzeStructuresCT_sheet(loc_sheet=DICOMLocations,debug=0,interval=interval)\n",
    "#1.1: Take TV point clouds and calculate shape statistics. Generates PCAMesh.pickle\n",
    "dfShape = compileTVShapes(dfPointCloud, save=SaveFolder,interval=interval)\n",
    "#1.2: Take HU data and compile into one dataframe. Generates HU.pickle\n",
    "dfHU=compileHU(SaveFolder,interval=interval)\n",
    "#1.3: Take per slice depth data and calculate global max and min for each case, combine all plans into one dataframe. Generates Treatable.pickle\n",
    "dfDepth=batchDetermineMinMaxDepths(SaveFolder,plot=False,interval=interval)\n",
    "#2: Extract MU and plan info from DICOM plan files, directly compile into one dataframe. Generates MU.pickle\n",
    "dfMU=batchAnalyzePlan(SaveFolder,interval=interval)\n",
    "#3: Extract dose data from DICOM dose and structure files. Returns one dataframe with dose results and one with distances between OAR and TV. Generates Dose.pickle, DistsPTVOAR.pickle\n",
    "dfDose,dfOARDistance=batchAnalyzeDose(SaveFolder,locsheet=DICOMLocations,debug=False,calc=True,interval=interval)\n",
    "#4: If you chose intervals, merge them all into one dataframe. Generates all pickles without intervals\n",
    "mergeIntervalResults()\n",
    "#5: Merge all different kinds of analysis into one big overarching dataframe. Generates MergedAll.pickle\n",
    "dfAll=mergeAllResults()\n",
    "#6: If needed, anonymize dataframe. Generates DataAnonymized.pickle\n",
    "dfAnon=anonymizeResults(dfAll)\n",
    "#7: Generate statistics plots according to your wishes. Generates plots\n",
    "dfPlot=analyzebycategory(dfAll,f\"{SaveFolder}/Plots\",perPlan=True,fine=True,plotSiteDetails=True,show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have the MergedResults.pickle and only want to analyze it, you can load it using \n",
    "\n",
    "df=pd.read_pickle('MergedResults.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
